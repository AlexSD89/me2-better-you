#!/usr/bin/env python3
"""
LayerX 5é€šé“å¹¶å‘ç”Ÿäº§å¼•æ“
åŸºäºBMADæ··åˆæ™ºèƒ½æ¶æ„çš„è‡ªåŠ¨åŒ–ä¿¡æ¯å¤„ç†å’ŒçŸ¥è¯†ç”Ÿäº§ç³»ç»Ÿ
"""

import asyncio
import json
import time
import subprocess
from datetime import datetime
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    channel: str
    query: str
    results: List[Dict[str, Any]]
    timestamp: datetime
    quality_score: float
    source_count: int

@dataclass
class ChannelConfig:
    """é€šé“é…ç½®"""
    name: str
    description: str
    tools: List[str]
    priority: int
    concurrent_queries: int
    quality_threshold: float

class FiveChannelConcurrentEngine:
    """5é€šé“å¹¶å‘ç”Ÿäº§å¼•æ“"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.channels = self._initialize_channels()
        self.active_searches = {}
        self.results_cache = {}
        self.executor = ThreadPoolExecutor(max_workers=10)
        
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            "max_concurrent_searches": 5,
            "quality_threshold": 7.5,
            "timeout_seconds": 120,
            "cache_duration": 3600,
            "bmad_optimization": True,
            "learning_enabled": True
        }
        
        if config_path:
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                default_config.update(user_config)
            except FileNotFoundError:
                logger.warning(f"Config file {config_path} not found, using defaults")
        
        return default_config
    
    def _initialize_channels(self) -> Dict[str, ChannelConfig]:
        """åˆå§‹åŒ–5ä¸ªå¹¶å‘æœç´¢é€šé“"""
        return {
            "investment_discovery": ChannelConfig(
                name="æŠ•èµ„å‘ç°é€šé“",
                description="AIæŠ•èµ„æœºä¼šå‘ç°å’Œåˆ†æ",
                tools=["tavily-search", "python-sandbox", "workspace-filesystem"],
                priority=1,
                concurrent_queries=3,
                quality_threshold=8.0
            ),
            "tech_trends": ChannelConfig(
                name="æŠ€æœ¯è¶‹åŠ¿é€šé“", 
                description="å‰æ²¿æŠ€æœ¯å‘å±•è¶‹åŠ¿ç›‘æ§",
                tools=["tavily-search", "fetch", "jina-reader"],
                priority=2,
                concurrent_queries=2,
                quality_threshold=7.5
            ),
            "market_dynamics": ChannelConfig(
                name="å¸‚åœºåŠ¨æ€é€šé“",
                description="å¸‚åœºå˜åŒ–å’Œå•†ä¸šæœºä¼šåˆ†æ",
                tools=["tavily-search", "firecrawl", "hotnews"],
                priority=2,
                concurrent_queries=2,
                quality_threshold=7.0
            ),
            "enterprise_needs": ChannelConfig(
                name="ä¼ä¸šéœ€æ±‚é€šé“",
                description="ä¼ä¸šæœåŠ¡éœ€æ±‚å’Œè§£å†³æ–¹æ¡ˆæŒ–æ˜",
                tools=["tavily-search", "workspace-filesystem"],
                priority=3,
                concurrent_queries=2,
                quality_threshold=6.5
            ),
            "knowledge_synthesis": ChannelConfig(
                name="çŸ¥è¯†æ²‰æ·€é€šé“",
                description="æ–¹æ³•è®ºæå–å’Œç»éªŒæ€»ç»“",
                tools=["workspace-filesystem", "knowledge-base", "methodology-library"],
                priority=4,
                concurrent_queries=1,
                quality_threshold=7.5
            )
        }
    
    async def start_concurrent_search(self, 
                                    user_query: str, 
                                    domain: str = "general",
                                    priority_channels: Optional[List[str]] = None) -> Dict[str, SearchResult]:
        """å¯åŠ¨5é€šé“å¹¶å‘æœç´¢"""
        
        logger.info(f"ğŸš€ å¯åŠ¨5é€šé“å¹¶å‘æœç´¢å¼•æ“")
        logger.info(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢: {user_query}")
        logger.info(f"ğŸ¯ ä¸šåŠ¡åŸŸ: {domain}")
        
        # ç”Ÿæˆé’ˆå¯¹æ€§æœç´¢æŸ¥è¯¢
        channel_queries = self._generate_channel_queries(user_query, domain)
        
        # å¦‚æœæŒ‡å®šäº†ä¼˜å…ˆé€šé“ï¼Œè°ƒæ•´æ‰§è¡Œé¡ºåº
        if priority_channels:
            channel_queries = self._prioritize_channels(channel_queries, priority_channels)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰é€šé“æœç´¢
        search_tasks = []
        for channel_name, queries in channel_queries.items():
            if channel_name in self.channels:
                task = asyncio.create_task(
                    self._execute_channel_search(channel_name, queries)
                )
                search_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆ
        results = {}
        completed_tasks = await asyncio.gather(*search_tasks, return_exceptions=True)
        
        for i, result in enumerate(completed_tasks):
            if isinstance(result, Exception):
                logger.error(f"é€šé“æœç´¢å¤±è´¥: {result}")
            else:
                channel_name = list(channel_queries.keys())[i]
                results[channel_name] = result
        
        # è´¨é‡è¯„ä¼°å’Œä¼˜åŒ–
        optimized_results = self._optimize_results(results, user_query, domain)
        
        # è®°å½•å­¦ä¹ æ•°æ®
        if self.config.get("learning_enabled", True):
            await self._record_learning_data(user_query, domain, optimized_results)
        
        logger.info(f"âœ… 5é€šé“å¹¶å‘æœç´¢å®Œæˆï¼Œè·å¾— {len(optimized_results)} ä¸ªé«˜è´¨é‡ç»“æœ")
        
        return optimized_results
    
    def _generate_channel_queries(self, user_query: str, domain: str) -> Dict[str, List[str]]:
        """ç”Ÿæˆå„é€šé“ä¸“é—¨åŒ–æŸ¥è¯¢"""
        
        # åŸºç¡€æŸ¥è¯¢åˆ†æ
        base_keywords = user_query.lower().split()
        
        # æŠ•èµ„å‘ç°é€šé“æŸ¥è¯¢
        investment_queries = []
        if any(word in user_query.lower() for word in ['æŠ•èµ„', 'èèµ„', 'ä¼°å€¼', 'AI', 'åˆ›ä¸š', 'ç‹¬è§’å…½']):
            investment_queries = [
                f"{user_query} æŠ•èµ„æœºä¼š",
                f"{user_query} èèµ„æƒ…å†µ",
                f"{user_query} å¸‚åœºä¼°å€¼",
                f"AI {user_query} æŠ•èµ„åˆ†æ"
            ]
        else:
            investment_queries = [f"{user_query} å•†ä¸šä»·å€¼", f"{user_query} æŠ•èµ„æ½œåŠ›"]
        
        # æŠ€æœ¯è¶‹åŠ¿é€šé“æŸ¥è¯¢
        tech_queries = [
            f"{user_query} æŠ€æœ¯è¶‹åŠ¿ 2024 2025",
            f"{user_query} æœ€æ–°æŠ€æœ¯å‘å±•",
            f"{user_query} å‰æ²¿æŠ€æœ¯åº”ç”¨",
            f"{user_query} æŠ€æœ¯åˆ›æ–°çªç ´"
        ]
        
        # å¸‚åœºåŠ¨æ€é€šé“æŸ¥è¯¢
        market_queries = [
            f"{user_query} å¸‚åœºåˆ†ææŠ¥å‘Š",
            f"{user_query} è¡Œä¸šå‘å±•è¶‹åŠ¿",
            f"{user_query} ç«äº‰æ ¼å±€åˆ†æ",
            f"{user_query} å•†ä¸šæ¨¡å¼åˆ›æ–°"
        ]
        
        # ä¼ä¸šéœ€æ±‚é€šé“æŸ¥è¯¢
        enterprise_queries = [
            f"{user_query} ä¼ä¸šè§£å†³æ–¹æ¡ˆ",
            f"{user_query} ä¼ä¸šæœåŠ¡éœ€æ±‚",
            f"{user_query} ä¼ä¸šæ•°å­—åŒ–è½¬å‹",
            f"ä¼ä¸šå¦‚ä½•åº”ç”¨ {user_query}"
        ]
        
        # çŸ¥è¯†æ²‰æ·€é€šé“æŸ¥è¯¢ï¼ˆä¸»è¦æ˜¯å†…éƒ¨çŸ¥è¯†åº“æ£€ç´¢ï¼‰
        knowledge_queries = [
            f"{user_query} æ–¹æ³•è®º",
            f"{user_query} æœ€ä½³å®è·µ",
            f"{user_query} ç»éªŒæ€»ç»“",
            f"{user_query} åˆ†ææ¡†æ¶"
        ]
        
        return {
            "investment_discovery": investment_queries,
            "tech_trends": tech_queries,
            "market_dynamics": market_queries,
            "enterprise_needs": enterprise_queries,
            "knowledge_synthesis": knowledge_queries
        }
    
    def _prioritize_channels(self, channel_queries: Dict[str, List[str]], 
                           priority_channels: List[str]) -> Dict[str, List[str]]:
        """æ ¹æ®ä¼˜å…ˆçº§é‡æ–°æ’åºé€šé“"""
        prioritized = {}
        
        # å…ˆæ·»åŠ ä¼˜å…ˆé€šé“
        for channel in priority_channels:
            if channel in channel_queries:
                prioritized[channel] = channel_queries[channel]
        
        # æ·»åŠ å…¶ä½™é€šé“
        for channel, queries in channel_queries.items():
            if channel not in prioritized:
                prioritized[channel] = queries
        
        return prioritized
    
    async def _execute_channel_search(self, channel_name: str, queries: List[str]) -> SearchResult:
        """æ‰§è¡Œå•ä¸ªé€šé“çš„æœç´¢"""
        
        channel_config = self.channels[channel_name]
        logger.info(f"ğŸ” æ‰§è¡Œ{channel_config.name}æœç´¢...")
        
        all_results = []
        total_quality_score = 0.0
        source_count = 0
        
        # å¹¶å‘æ‰§è¡Œè¯¥é€šé“çš„å¤šä¸ªæŸ¥è¯¢
        search_futures = []
        
        for i, query in enumerate(queries[:channel_config.concurrent_queries]):
            future = self.executor.submit(self._execute_single_search, channel_name, query)
            search_futures.append(future)
        
        # æ”¶é›†æœç´¢ç»“æœ
        for future in as_completed(search_futures, timeout=self.config["timeout_seconds"]):
            try:
                result = future.result()
                if result:
                    all_results.extend(result.get("results", []))
                    source_count += result.get("source_count", 0)
                    total_quality_score += result.get("quality_score", 0.0)
            except Exception as e:
                logger.error(f"æœç´¢æŸ¥è¯¢å¤±è´¥: {e}")
        
        # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°
        avg_quality = total_quality_score / max(len(queries), 1)
        
        # å»é‡å’Œä¼˜åŒ–ç»“æœ
        unique_results = self._deduplicate_results(all_results)
        
        return SearchResult(
            channel=channel_name,
            query="; ".join(queries),
            results=unique_results,
            timestamp=datetime.now(),
            quality_score=avg_quality,
            source_count=source_count
        )
    
    def _execute_single_search(self, channel_name: str, query: str) -> Optional[Dict[str, Any]]:
        """æ‰§è¡Œå•æ¬¡æœç´¢"""
        try:
            channel_config = self.channels[channel_name]
            
            # æ ¹æ®é€šé“é€‰æ‹©æœ€ä½³å·¥å…·
            if "tavily-search" in channel_config.tools:
                return self._tavily_search(query)
            elif "workspace-filesystem" in channel_config.tools:
                return self._knowledge_base_search(query)
            elif "fetch" in channel_config.tools:
                return self._web_fetch_search(query)
            else:
                return self._generic_search(query)
                
        except Exception as e:
            logger.error(f"æœç´¢æ‰§è¡Œå¤±è´¥ - {channel_name}: {e}")
            return None
    
    def _tavily_search(self, query: str) -> Dict[str, Any]:
        """ä½¿ç”¨Tavilyè¿›è¡Œæœç´¢"""
        try:
            # æ¨¡æ‹ŸTavilyæœç´¢è°ƒç”¨
            # å®é™…ç¯å¢ƒä¸­è¿™é‡Œä¼šè°ƒç”¨MCPçš„tavily-searchæœåŠ¡
            result = {
                "query": query,
                "results": [
                    {
                        "title": f"å…³äº{query}çš„ç ”ç©¶æŠ¥å‘Š",
                        "url": "https://example.com/report1",
                        "content": f"åŸºäºæœ€æ–°æ•°æ®çš„{query}æ·±åº¦åˆ†æ...",
                        "relevance_score": 0.9,
                        "source_type": "research_report"
                    },
                    {
                        "title": f"{query}è¡Œä¸šæ´å¯Ÿ",
                        "url": "https://example.com/insight1", 
                        "content": f"{query}é¢†åŸŸçš„é‡è¦è¶‹åŠ¿å’Œæœºä¼š...",
                        "relevance_score": 0.85,
                        "source_type": "industry_analysis"
                    }
                ],
                "quality_score": 8.5,
                "source_count": 2
            }
            
            time.sleep(0.5)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            return result
            
        except Exception as e:
            logger.error(f"Tavilyæœç´¢å¤±è´¥: {e}")
            return {"results": [], "quality_score": 0.0, "source_count": 0}
    
    def _knowledge_base_search(self, query: str) -> Dict[str, Any]:
        """æœç´¢å†…éƒ¨çŸ¥è¯†åº“"""
        try:
            # æ¨¡æ‹ŸçŸ¥è¯†åº“æœç´¢
            result = {
                "query": query,
                "results": [
                    {
                        "title": f"{query}æ–¹æ³•è®ºæ–‡æ¡£",
                        "path": "/knowledge/methodology/" + query.replace(" ", "_") + ".md",
                        "content": f"å†…éƒ¨ç§¯ç´¯çš„{query}ç›¸å…³æ–¹æ³•è®ºå’Œæœ€ä½³å®è·µ...",
                        "relevance_score": 0.8,
                        "source_type": "internal_methodology"
                    }
                ],
                "quality_score": 7.5,
                "source_count": 1
            }
            
            time.sleep(0.2)  # æ¨¡æ‹Ÿæ–‡ä»¶è®¿é—®å»¶è¿Ÿ
            return result
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: {e}")
            return {"results": [], "quality_score": 0.0, "source_count": 0}
    
    def _web_fetch_search(self, query: str) -> Dict[str, Any]:
        """WebæŠ“å–æœç´¢"""
        try:
            # æ¨¡æ‹ŸWebæŠ“å–
            result = {
                "query": query,
                "results": [
                    {
                        "title": f"{query}æœ€æ–°åŠ¨æ€",
                        "url": f"https://news.example.com/{query}",
                        "content": f"æœ€æ–°çš„{query}ç›¸å…³æ–°é—»å’ŒåŠ¨æ€...",
                        "relevance_score": 0.75,
                        "source_type": "news_article"
                    }
                ],
                "quality_score": 7.0,
                "source_count": 1
            }
            
            time.sleep(0.8)  # æ¨¡æ‹Ÿç½‘é¡µæŠ“å–å»¶è¿Ÿ
            return result
            
        except Exception as e:
            logger.error(f"WebæŠ“å–å¤±è´¥: {e}")
            return {"results": [], "quality_score": 0.0, "source_count": 0}
    
    def _generic_search(self, query: str) -> Dict[str, Any]:
        """é€šç”¨æœç´¢æ–¹æ³•"""
        return {
            "query": query,
            "results": [
                {
                    "title": f"å…³äº{query}çš„ä¿¡æ¯",
                    "content": f"é€šç”¨æœç´¢æ‰¾åˆ°çš„{query}ç›¸å…³ä¿¡æ¯...",
                    "relevance_score": 0.6,
                    "source_type": "generic"
                }
            ],
            "quality_score": 6.0,
            "source_count": 1
        }
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æœç´¢ç»“æœ"""
        seen_titles = set()
        unique_results = []
        
        for result in results:
            title = result.get("title", "")
            if title not in seen_titles:
                seen_titles.add(title)
                unique_results.append(result)
        
        return unique_results
    
    def _optimize_results(self, 
                         results: Dict[str, SearchResult], 
                         user_query: str, 
                         domain: str) -> Dict[str, SearchResult]:
        """BMADè´¨é‡ä¼˜åŒ–å’Œç»“æœç­›é€‰"""
        
        logger.info("ğŸ“Š æ‰§è¡ŒBMADè´¨é‡ä¼˜åŒ–...")
        
        optimized_results = {}
        
        for channel_name, search_result in results.items():
            if search_result.quality_score >= self.channels[channel_name].quality_threshold:
                # å¯¹é«˜è´¨é‡ç»“æœè¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–
                optimized_result = self._apply_bmad_optimization(search_result, user_query, domain)
                optimized_results[channel_name] = optimized_result
            else:
                logger.warning(f"é€šé“ {channel_name} è´¨é‡ä¸è¾¾æ ‡: {search_result.quality_score}")
        
        return optimized_results
    
    def _apply_bmad_optimization(self, 
                               search_result: SearchResult, 
                               user_query: str, 
                               domain: str) -> SearchResult:
        """åº”ç”¨BMADæ‰¹è¯„ä¼˜åŒ–æœºåˆ¶"""
        
        # æ‰¹è¯„é˜¶æ®µ - è¯„ä¼°ç»“æœè´¨é‡
        quality_issues = []
        
        if search_result.source_count < 2:
            quality_issues.append("æ¥æºæ•°é‡ä¸è¶³")
        
        if len(search_result.results) < 1:
            quality_issues.append("ç»“æœæ•°é‡è¿‡å°‘")
        
        avg_relevance = sum(r.get("relevance_score", 0) for r in search_result.results) / max(len(search_result.results), 1)
        if avg_relevance < 0.7:
            quality_issues.append("ç›¸å…³æ€§åä½")
        
        # ä¼˜åŒ–é˜¶æ®µ - åº”ç”¨æ”¹è¿›æªæ–½
        if quality_issues:
            logger.info(f"ğŸ”§ å¯¹é€šé“ {search_result.channel} åº”ç”¨è´¨é‡ä¼˜åŒ–: {quality_issues}")
            
            # æå‡è´¨é‡åˆ†æ•°çš„ä¼˜åŒ–é€»è¾‘
            optimization_bonus = 0.0
            
            # å¦‚æœæ˜¯æŠ•èµ„åˆ†æåŸŸï¼Œç»™æŠ•èµ„ç›¸å…³ç»“æœåŠ åˆ†
            if domain == "investment_analysis" and search_result.channel == "investment_discovery":
                optimization_bonus += 0.5
            
            # æ ¹æ®ç»“æœå¤šæ ·æ€§åŠ åˆ†
            source_types = set(r.get("source_type", "") for r in search_result.results)
            if len(source_types) > 2:
                optimization_bonus += 0.3
            
            # åº”ç”¨ä¼˜åŒ–
            search_result.quality_score = min(10.0, search_result.quality_score + optimization_bonus)
        
        return search_result
    
    async def _record_learning_data(self, 
                                  user_query: str, 
                                  domain: str, 
                                  results: Dict[str, SearchResult]) -> None:
        """è®°å½•å­¦ä¹ æ•°æ®ç”¨äºæŒç»­ä¼˜åŒ–"""
        
        learning_data = {
            "timestamp": datetime.now().isoformat(),
            "user_query": user_query,
            "domain": domain,
            "channel_performance": {},
            "overall_quality": 0.0,
            "optimization_suggestions": []
        }
        
        total_quality = 0.0
        active_channels = 0
        
        for channel_name, search_result in results.items():
            learning_data["channel_performance"][channel_name] = {
                "quality_score": search_result.quality_score,
                "result_count": len(search_result.results),
                "source_count": search_result.source_count,
                "execution_time": "æ¨¡æ‹Ÿæ—¶é—´"  # å®é™…ç¯å¢ƒä¸­è®°å½•çœŸå®æ‰§è¡Œæ—¶é—´
            }
            
            total_quality += search_result.quality_score
            active_channels += 1
        
        learning_data["overall_quality"] = total_quality / max(active_channels, 1)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if learning_data["overall_quality"] < 7.0:
            learning_data["optimization_suggestions"].append("æ•´ä½“æœç´¢è´¨é‡åä½ï¼Œå»ºè®®è°ƒæ•´æŸ¥è¯¢ç­–ç•¥")
        
        if active_channels < 4:
            learning_data["optimization_suggestions"].append("æ´»è·ƒé€šé“æ•°é‡ä¸è¶³ï¼Œå»ºè®®ä¼˜åŒ–é€šé“é…ç½®")
        
        # ä¿å­˜å­¦ä¹ æ•°æ®
        try:
            learning_file = "/tmp/5_channel_learning_data.json"
            with open(learning_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(learning_data, ensure_ascii=False) + "\n")
            
            logger.info(f"ğŸ“š å­¦ä¹ æ•°æ®å·²è®°å½•: {learning_file}")
            
        except Exception as e:
            logger.error(f"å­¦ä¹ æ•°æ®è®°å½•å¤±è´¥: {e}")
    
    def generate_comprehensive_report(self, results: Dict[str, SearchResult], user_query: str) -> str:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        
        report = f"""# 5é€šé“å¹¶å‘æœç´¢ç»¼åˆæŠ¥å‘Š

## ğŸ“Š æœç´¢æ¦‚è¦
- **æŸ¥è¯¢å†…å®¹**: {user_query}
- **æ‰§è¡Œæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ´»è·ƒé€šé“**: {len(results)}ä¸ª
- **æ€»ç»“æœæ•°é‡**: {sum(len(r.results) for r in results.values())}æ¡

## ğŸ” å„é€šé“åˆ†æç»“æœ

"""
        
        for channel_name, search_result in results.items():
            channel_config = self.channels[channel_name]
            
            report += f"""### {channel_config.name}
- **è´¨é‡è¯„åˆ†**: {search_result.quality_score:.1f}/10.0
- **ç»“æœæ•°é‡**: {len(search_result.results)}æ¡
- **ä¿¡æ¯æºæ•°**: {search_result.source_count}ä¸ª
- **æŸ¥è¯¢ç­–ç•¥**: {search_result.query}

#### å…³é”®å‘ç°:
"""
            
            for i, result in enumerate(search_result.results[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
                report += f"{i}. **{result.get('title', 'æ— æ ‡é¢˜')}** (ç›¸å…³æ€§: {result.get('relevance_score', 0):.2f})\n"
                report += f"   {result.get('content', 'æ— å†…å®¹')[:100]}...\n\n"
        
        # ç»¼åˆæ´å¯Ÿ
        avg_quality = sum(r.quality_score for r in results.values()) / len(results)
        total_sources = sum(r.source_count for r in results.values())
        
        report += f"""## ğŸ’¡ ç»¼åˆæ´å¯Ÿ

### æœç´¢æ•ˆæœè¯„ä¼°
- **å¹³å‡è´¨é‡è¯„åˆ†**: {avg_quality:.1f}/10.0
- **ä¿¡æ¯æºè¦†ç›–**: {total_sources}ä¸ªç‹¬ç«‹æ¥æº
- **æ•°æ®å®Œæ•´æ€§**: {'ä¼˜ç§€' if avg_quality > 8.0 else 'è‰¯å¥½' if avg_quality > 7.0 else 'ä¸€èˆ¬'}

### ä¸»è¦å‘ç°
"""
        
        # æå–å…³é”®æ´å¯Ÿ
        if avg_quality > 8.5:
            report += "- âœ… æœç´¢è´¨é‡ä¼˜ç§€ï¼Œä¿¡æ¯è¦†ç›–å…¨é¢\n"
        elif avg_quality > 7.0:
            report += "- âœ… æœç´¢è´¨é‡è‰¯å¥½ï¼Œè·å¾—äº†æœ‰ä»·å€¼çš„ä¿¡æ¯\n"
        else:
            report += "- âš ï¸ æœç´¢è´¨é‡æœ‰å¾…æå‡ï¼Œå»ºè®®ä¼˜åŒ–æŸ¥è¯¢ç­–ç•¥\n"
        
        if total_sources > 10:
            report += "- âœ… ä¿¡æ¯æºä¸°å¯Œï¼Œç¡®ä¿äº†ä¿¡æ¯çš„å¤šæ ·æ€§å’Œå¯é æ€§\n"
        
        report += f"""
### å»ºè®®è¡ŒåŠ¨
1. é‡ç‚¹å…³æ³¨è´¨é‡è¯„åˆ†æœ€é«˜çš„ {max(results.items(), key=lambda x: x[1].quality_score)[0]} é€šé“ç»“æœ
2. å¯¹äºä½è´¨é‡é€šé“ï¼Œå»ºè®®è°ƒæ•´æœç´¢ç­–ç•¥æˆ–å¢åŠ ä¿¡æ¯æº
3. å°†é«˜ä»·å€¼å‘ç°æ•´åˆåˆ°çŸ¥è¯†åº“ä¸­ï¼Œä¾›æœªæ¥å‚è€ƒ

---
*æœ¬æŠ¥å‘Šç”±LayerX 5é€šé“å¹¶å‘æœç´¢å¼•æ“è‡ªåŠ¨ç”Ÿæˆ*
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}*
"""
        
        return report

# ä¸»ç¨‹åºå…¥å£
async def main():
    """ä¸»ç¨‹åºæ¼”ç¤º"""
    
    print("ğŸš€ LayerX 5é€šé“å¹¶å‘ç”Ÿäº§å¼•æ“å¯åŠ¨æµ‹è¯•")
    
    # åˆå§‹åŒ–å¼•æ“
    engine = FiveChannelConcurrentEngine()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        ("AIè§†é¢‘ç”ŸæˆæŠ€æœ¯çš„æŠ•èµ„æœºä¼š", "investment_analysis"),
        ("ä¼ä¸šæ•°å­—åŒ–è½¬å‹è§£å†³æ–¹æ¡ˆ", "enterprise_service"),
        ("2025å¹´æŠ€æœ¯è¶‹åŠ¿é¢„æµ‹", "knowledge_research")
    ]
    
    for query, domain in test_queries:
        print(f"\n" + "="*80)
        print(f"ğŸ” æµ‹è¯•æŸ¥è¯¢: {query}")
        print(f"ğŸ¯ ä¸šåŠ¡åŸŸ: {domain}")
        print("="*80)
        
        # æ‰§è¡Œ5é€šé“å¹¶å‘æœç´¢
        results = await engine.start_concurrent_search(query, domain)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = engine.generate_comprehensive_report(results, query)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ“Š æœç´¢ç»“æœæ¦‚è¦:")
        for channel_name, result in results.items():
            channel_desc = engine.channels[channel_name].name
            print(f"- {channel_desc}: {result.quality_score:.1f}/10.0 ({len(result.results)}æ¡ç»“æœ)")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = f"/tmp/5_channel_report_{int(time.time())}.md"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report)
        
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç»§ç»­
        await asyncio.sleep(2)
    
    print("\nâœ… 5é€šé“å¹¶å‘å¼•æ“æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(main())