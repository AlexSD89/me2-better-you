#!/usr/bin/env python3
"""
TradingAgents æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
Performance Monitor for TradingAgents Enhancement System

ç›‘æ§æŒ‡æ ‡ï¼š
1. APIå“åº”æ€§èƒ½ - FinnHub APIè°ƒç”¨å“åº”æ—¶é—´å’ŒæˆåŠŸç‡
2. ç­–ç•¥è®¡ç®—æ€§èƒ½ - å•è‚¡ç¥¨åˆ†æè€—æ—¶ã€å¤šç­–ç•¥å¹¶è¡Œå¤„ç†é€Ÿåº¦
3. ç³»ç»Ÿç¨³å®šæ€§ - é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§ã€é”™è¯¯æ¢å¤èƒ½åŠ›
4. æ•°æ®å‡†ç¡®æ€§éªŒè¯ - ä¸å®˜æ–¹æ•°æ®å¯¹æ¯”ã€è®¡ç®—ç»“æœéªŒè¯
"""

import asyncio
import time
import psutil
import json
import logging
import threading
import queue
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
import pandas as pd
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('performance_monitor.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    timestamp: str
    api_response_time: float
    api_success_rate: float
    memory_usage_mb: float
    cpu_usage_percent: float
    strategy_calculation_time: float
    concurrent_requests: int
    error_count: int
    data_accuracy_score: float

@dataclass
class APICallMetrics:
    """APIè°ƒç”¨æŒ‡æ ‡"""
    endpoint: str
    response_time: float
    status_code: int
    success: bool
    error_message: str = ""
    rate_limited: bool = False

class PerformanceMonitor:
    """TradingAgents æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, api_key: str, monitoring_interval: int = 60):
        self.api_key = api_key
        self.monitoring_interval = monitoring_interval
        self.base_url = "https://finnhub.io/api/v1"
        
        # æ€§èƒ½æ•°æ®å­˜å‚¨
        self.metrics_history: List[PerformanceMetrics] = []
        self.api_calls_history: List[APICallMetrics] = []
        
        # ç›‘æ§çŠ¶æ€
        self.is_monitoring = False
        self.start_time = None
        
        # é”™è¯¯è®¡æ•°å™¨
        self.error_counts = {
            'api_errors': 0,
            'calculation_errors': 0,
            'memory_errors': 0,
            'timeout_errors': 0
        }
        
        # åŸºå‡†è‚¡ç¥¨åˆ—è¡¨ (ç”¨äºæ€§èƒ½æµ‹è¯•)
        self.benchmark_stocks = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA']
        
        # æ€§èƒ½é˜ˆå€¼å®šä¹‰
        self.performance_thresholds = {
            'api_response_time_ms': 2000,  # 2ç§’
            'api_success_rate_percent': 95,
            'memory_usage_mb': 512,
            'cpu_usage_percent': 70,
            'strategy_calculation_time_ms': 5000,  # 5ç§’
            'concurrent_limit': 10
        }
        
        logger.info("æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def measure_api_performance(self, symbol: str, endpoint_type: str = 'quote') -> APICallMetrics:
        """æµ‹é‡APIæ€§èƒ½"""
        start_time = time.time()
        
        try:
            if endpoint_type == 'quote':
                url = f"{self.base_url}/quote"
                params = {"symbol": symbol, "token": self.api_key}
            elif endpoint_type == 'candle':
                end_time = int(datetime.now().timestamp())
                start_timestamp = int((datetime.now() - timedelta(days=30)).timestamp())
                url = f"{self.base_url}/stock/candle"
                params = {
                    'symbol': symbol,
                    'resolution': 'D',
                    'from': start_timestamp,
                    'to': end_time,
                    'token': self.api_key
                }
            elif endpoint_type == 'profile':
                url = f"{self.base_url}/stock/profile2"
                params = {"symbol": symbol, "token": self.api_key}
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç«¯ç‚¹ç±»å‹: {endpoint_type}")
            
            response = requests.get(url, params=params, timeout=10)
            response_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            # æ£€æŸ¥æ˜¯å¦è¢«é™é¢‘
            rate_limited = response.status_code == 429
            
            success = response.status_code == 200
            if success and endpoint_type == 'candle':
                data = response.json()
                success = data.get('s') == 'ok'
            
            return APICallMetrics(
                endpoint=f"{endpoint_type}_{symbol}",
                response_time=response_time,
                status_code=response.status_code,
                success=success,
                error_message="" if success else response.text[:200],
                rate_limited=rate_limited
            )
            
        except requests.Timeout:
            self.error_counts['timeout_errors'] += 1
            return APICallMetrics(
                endpoint=f"{endpoint_type}_{symbol}",
                response_time=(time.time() - start_time) * 1000,
                status_code=0,
                success=False,
                error_message="è¯·æ±‚è¶…æ—¶"
            )
        except Exception as e:
            self.error_counts['api_errors'] += 1
            return APICallMetrics(
                endpoint=f"{endpoint_type}_{symbol}",
                response_time=(time.time() - start_time) * 1000,
                status_code=0,
                success=False,
                error_message=str(e)[:200]
            )
    
    def measure_strategy_performance(self, symbol: str) -> Tuple[float, bool]:
        """æµ‹é‡ç­–ç•¥è®¡ç®—æ€§èƒ½"""
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿç­–ç•¥è®¡ç®—è¿‡ç¨‹
            # 1. è·å–å†å²æ•°æ®
            api_metric = self.measure_api_performance(symbol, 'candle')
            
            if not api_metric.success:
                return (time.time() - start_time) * 1000, False
            
            # 2. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®è¿›è¡ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—
            data_length = 100
            prices = np.random.randn(data_length).cumsum() + 100
            df = pd.DataFrame({
                'close': prices,
                'high': prices * 1.02,
                'low': prices * 0.98,
                'volume': np.random.randint(1000, 10000, data_length)
            })
            
            # 3. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (æ¨¡æ‹Ÿå¤šä¸ªç­–ç•¥)
            # MAè®¡ç®—
            df['MA5'] = df['close'].rolling(5).mean()
            df['MA20'] = df['close'].rolling(20).mean()
            
            # RSIè®¡ç®—
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
            rs = gain / loss
            df['RSI'] = 100 - (100 / (1 + rs))
            
            # MACDè®¡ç®—
            exp1 = df['close'].ewm(span=12).mean()
            exp2 = df['close'].ewm(span=26).mean()
            df['MACD'] = exp1 - exp2
            df['Signal'] = df['MACD'].ewm(span=9).mean()
            
            # 4. ç­–ç•¥ä¿¡å·ç”Ÿæˆ
            signals = []
            
            # è¶‹åŠ¿ç­–ç•¥
            if df['MA5'].iloc[-1] > df['MA20'].iloc[-1]:
                signals.append(1)
            else:
                signals.append(-1)
            
            # RSIç­–ç•¥
            latest_rsi = df['RSI'].iloc[-1]
            if latest_rsi < 30:
                signals.append(1)
            elif latest_rsi > 70:
                signals.append(-1)
            else:
                signals.append(0)
            
            # MACDç­–ç•¥
            if df['MACD'].iloc[-1] > df['Signal'].iloc[-1]:
                signals.append(1)
            else:
                signals.append(-1)
            
            calculation_time = (time.time() - start_time) * 1000
            return calculation_time, True
            
        except Exception as e:
            self.error_counts['calculation_errors'] += 1
            logger.error(f"ç­–ç•¥è®¡ç®—é”™è¯¯ {symbol}: {str(e)}")
            return (time.time() - start_time) * 1000, False
    
    def measure_concurrent_performance(self, num_concurrent: int = 5) -> Dict[str, Any]:
        """æµ‹é‡å¹¶å‘å¤„ç†æ€§èƒ½"""
        logger.info(f"å¼€å§‹å¹¶å‘æ€§èƒ½æµ‹è¯• - å¹¶å‘æ•°: {num_concurrent}")
        
        start_time = time.time()
        results = []
        
        with ThreadPoolExecutor(max_workers=num_concurrent) as executor:
            # æäº¤å¹¶å‘ä»»åŠ¡
            futures = []
            for i in range(num_concurrent):
                symbol = self.benchmark_stocks[i % len(self.benchmark_stocks)]
                future = executor.submit(self.measure_api_performance, symbol, 'quote')
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=15)
                    results.append(result)
                except Exception as e:
                    logger.error(f"å¹¶å‘ä»»åŠ¡å¤±è´¥: {str(e)}")
        
        total_time = (time.time() - start_time) * 1000
        success_count = sum(1 for r in results if r.success)
        
        return {
            'total_time_ms': total_time,
            'successful_requests': success_count,
            'failed_requests': len(results) - success_count,
            'success_rate': (success_count / len(results)) * 100 if results else 0,
            'average_response_time': statistics.mean([r.response_time for r in results]) if results else 0,
            'max_response_time': max([r.response_time for r in results]) if results else 0,
            'min_response_time': min([r.response_time for r in results]) if results else 0
        }
    
    def measure_system_resources(self) -> Dict[str, float]:
        """æµ‹é‡ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            # å†…å­˜ä½¿ç”¨
            memory_info = psutil.virtual_memory()
            process = psutil.Process()
            process_memory_mb = process.memory_info().rss / 1024 / 1024
            
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # ç½‘ç»œI/O
            network_io = psutil.net_io_counters()
            
            return {
                'memory_usage_mb': process_memory_mb,
                'system_memory_percent': memory_info.percent,
                'cpu_usage_percent': cpu_percent,
                'network_bytes_sent': network_io.bytes_sent,
                'network_bytes_recv': network_io.bytes_recv
            }
            
        except Exception as e:
            self.error_counts['memory_errors'] += 1
            logger.error(f"ç³»ç»Ÿèµ„æºç›‘æ§é”™è¯¯: {str(e)}")
            return {
                'memory_usage_mb': 0,
                'system_memory_percent': 0,
                'cpu_usage_percent': 0,
                'network_bytes_sent': 0,
                'network_bytes_recv': 0
            }
    
    def validate_data_accuracy(self, symbol: str = 'AAPL') -> float:
        """éªŒè¯æ•°æ®å‡†ç¡®æ€§"""
        try:
            # è·å–å®æ—¶æŠ¥ä»·
            quote_metric = self.measure_api_performance(symbol, 'quote')
            if not quote_metric.success:
                return 0.0
            
            # è·å–å†å²æ•°æ®
            candle_metric = self.measure_api_performance(symbol, 'candle')
            if not candle_metric.success:
                return 0.0
            
            # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
            accuracy_score = 100.0
            
            # ç®€å•çš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
            if quote_metric.response_time > 5000:  # è¶…è¿‡5ç§’è®¤ä¸ºå“åº”å¼‚å¸¸
                accuracy_score -= 20
            
            if candle_metric.response_time > 10000:  # å†å²æ•°æ®è¶…è¿‡10ç§’
                accuracy_score -= 20
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if quote_metric.error_message or candle_metric.error_message:
                accuracy_score -= 30
            
            return max(0, accuracy_score)
            
        except Exception as e:
            logger.error(f"æ•°æ®å‡†ç¡®æ€§éªŒè¯é”™è¯¯: {str(e)}")
            return 0.0
    
    def collect_metrics(self) -> PerformanceMetrics:
        """æ”¶é›†ä¸€è½®å®Œæ•´çš„æ€§èƒ½æŒ‡æ ‡"""
        logger.info("æ”¶é›†æ€§èƒ½æŒ‡æ ‡...")
        
        # 1. APIæ€§èƒ½æµ‹è¯•
        api_metrics = []
        for symbol in self.benchmark_stocks[:3]:  # æµ‹è¯•3åªè‚¡ç¥¨
            metric = self.measure_api_performance(symbol, 'quote')
            api_metrics.append(metric)
            self.api_calls_history.append(metric)
            time.sleep(0.2)  # é¿å…é¢‘ç‡é™åˆ¶
        
        # è®¡ç®—APIæ€§èƒ½ç»Ÿè®¡
        successful_apis = [m for m in api_metrics if m.success]
        api_success_rate = (len(successful_apis) / len(api_metrics)) * 100 if api_metrics else 0
        avg_api_response_time = statistics.mean([m.response_time for m in successful_apis]) if successful_apis else 0
        
        # 2. ç­–ç•¥è®¡ç®—æ€§èƒ½
        strategy_times = []
        for symbol in self.benchmark_stocks[:2]:  # æµ‹è¯•2åªè‚¡ç¥¨çš„ç­–ç•¥è®¡ç®—
            calc_time, success = self.measure_strategy_performance(symbol)
            if success:
                strategy_times.append(calc_time)
            time.sleep(0.3)
        
        avg_strategy_time = statistics.mean(strategy_times) if strategy_times else 0
        
        # 3. ç³»ç»Ÿèµ„æºç›‘æ§
        system_resources = self.measure_system_resources()
        
        # 4. æ•°æ®å‡†ç¡®æ€§éªŒè¯
        data_accuracy = self.validate_data_accuracy()
        
        # 5. å¹¶å‘æ€§èƒ½æµ‹è¯• (æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡)
        current_minute = datetime.now().minute
        concurrent_requests = 0
        if current_minute % 5 == 0:
            concurrent_result = self.measure_concurrent_performance(3)
            concurrent_requests = concurrent_result['successful_requests']
        
        # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡å¯¹è±¡
        metrics = PerformanceMetrics(
            timestamp=datetime.now().isoformat(),
            api_response_time=avg_api_response_time,
            api_success_rate=api_success_rate,
            memory_usage_mb=system_resources['memory_usage_mb'],
            cpu_usage_percent=system_resources['cpu_usage_percent'],
            strategy_calculation_time=avg_strategy_time,
            concurrent_requests=concurrent_requests,
            error_count=sum(self.error_counts.values()),
            data_accuracy_score=data_accuracy
        )
        
        self.metrics_history.append(metrics)
        return metrics
    
    def check_performance_alerts(self, metrics: PerformanceMetrics) -> List[str]:
        """æ£€æŸ¥æ€§èƒ½å‘Šè­¦"""
        alerts = []
        
        # APIå“åº”æ—¶é—´å‘Šè­¦
        if metrics.api_response_time > self.performance_thresholds['api_response_time_ms']:
            alerts.append(f"ğŸš¨ APIå“åº”æ—¶é—´è¿‡é•¿: {metrics.api_response_time:.1f}ms (é˜ˆå€¼: {self.performance_thresholds['api_response_time_ms']}ms)")
        
        # APIæˆåŠŸç‡å‘Šè­¦
        if metrics.api_success_rate < self.performance_thresholds['api_success_rate_percent']:
            alerts.append(f"ğŸš¨ APIæˆåŠŸç‡è¿‡ä½: {metrics.api_success_rate:.1f}% (é˜ˆå€¼: {self.performance_thresholds['api_success_rate_percent']}%)")
        
        # å†…å­˜ä½¿ç”¨å‘Šè­¦
        if metrics.memory_usage_mb > self.performance_thresholds['memory_usage_mb']:
            alerts.append(f"ğŸš¨ å†…å­˜ä½¿ç”¨è¿‡é«˜: {metrics.memory_usage_mb:.1f}MB (é˜ˆå€¼: {self.performance_thresholds['memory_usage_mb']}MB)")
        
        # CPUä½¿ç”¨å‘Šè­¦
        if metrics.cpu_usage_percent > self.performance_thresholds['cpu_usage_percent']:
            alerts.append(f"ğŸš¨ CPUä½¿ç”¨è¿‡é«˜: {metrics.cpu_usage_percent:.1f}% (é˜ˆå€¼: {self.performance_thresholds['cpu_usage_percent']}%)")
        
        # ç­–ç•¥è®¡ç®—æ—¶é—´å‘Šè­¦
        if metrics.strategy_calculation_time > self.performance_thresholds['strategy_calculation_time_ms']:
            alerts.append(f"ğŸš¨ ç­–ç•¥è®¡ç®—æ—¶é—´è¿‡é•¿: {metrics.strategy_calculation_time:.1f}ms (é˜ˆå€¼: {self.performance_thresholds['strategy_calculation_time_ms']}ms)")
        
        # æ•°æ®å‡†ç¡®æ€§å‘Šè­¦
        if metrics.data_accuracy_score < 80:
            alerts.append(f"ğŸš¨ æ•°æ®å‡†ç¡®æ€§ä¸ä½³: {metrics.data_accuracy_score:.1f}% (å»ºè®®>80%)")
        
        # é”™è¯¯æ•°é‡å‘Šè­¦
        if metrics.error_count > 10:
            alerts.append(f"ğŸš¨ é”™è¯¯æ•°é‡è¿‡å¤š: {metrics.error_count} (å»ºè®®<10)")
        
        return alerts
    
    def print_real_time_status(self, metrics: PerformanceMetrics):
        """æ‰“å°å®æ—¶çŠ¶æ€"""
        print("\n" + "="*80)
        print(f"ğŸ“Š TradingAgents æ€§èƒ½ç›‘æ§ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)
        
        # æ ¸å¿ƒæŒ‡æ ‡
        print(f"ğŸš€ APIæ€§èƒ½:")
        print(f"   å“åº”æ—¶é—´: {metrics.api_response_time:.1f}ms")
        print(f"   æˆåŠŸç‡: {metrics.api_success_rate:.1f}%")
        
        print(f"âš¡ ç­–ç•¥æ€§èƒ½:")
        print(f"   è®¡ç®—æ—¶é—´: {metrics.strategy_calculation_time:.1f}ms")
        
        print(f"ğŸ’» ç³»ç»Ÿèµ„æº:")
        print(f"   å†…å­˜ä½¿ç”¨: {metrics.memory_usage_mb:.1f}MB")
        print(f"   CPUä½¿ç”¨: {metrics.cpu_usage_percent:.1f}%")
        
        print(f"ğŸ“ˆ æ•°æ®è´¨é‡:")
        print(f"   å‡†ç¡®æ€§è¯„åˆ†: {metrics.data_accuracy_score:.1f}/100")
        print(f"   ç´¯è®¡é”™è¯¯: {metrics.error_count}")
        
        # æ£€æŸ¥å‘Šè­¦
        alerts = self.check_performance_alerts(metrics)
        if alerts:
            print(f"\nâš ï¸ æ€§èƒ½å‘Šè­¦:")
            for alert in alerts:
                print(f"   {alert}")
        else:
            print(f"\nâœ… æ‰€æœ‰æŒ‡æ ‡æ­£å¸¸")
        
        # è¿è¡Œæ—¶é—´ç»Ÿè®¡
        if self.start_time:
            uptime = datetime.now() - self.start_time
            print(f"\nâ±ï¸ è¿è¡Œæ—¶é—´: {uptime}")
            print(f"ğŸ“Š ç›‘æ§è½®æ¬¡: {len(self.metrics_history)}")
    
    def save_metrics_to_file(self, filename: str = None):
        """ä¿å­˜æ€§èƒ½æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_metrics_{timestamp}.json"
        
        try:
            data = {
                'monitoring_config': {
                    'start_time': self.start_time.isoformat() if self.start_time else None,
                    'monitoring_interval': self.monitoring_interval,
                    'performance_thresholds': self.performance_thresholds
                },
                'metrics_history': [asdict(m) for m in self.metrics_history],
                'api_calls_history': [asdict(m) for m in self.api_calls_history[-100:]],  # ä¿å­˜æœ€è¿‘100æ¬¡APIè°ƒç”¨
                'error_counts': self.error_counts,
                'summary_stats': self.generate_summary_stats()
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ°: {filename}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {str(e)}")
    
    def generate_summary_stats(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        if not self.metrics_history:
            return {}
        
        api_times = [m.api_response_time for m in self.metrics_history if m.api_response_time > 0]
        success_rates = [m.api_success_rate for m in self.metrics_history if m.api_success_rate > 0]
        memory_usage = [m.memory_usage_mb for m in self.metrics_history if m.memory_usage_mb > 0]
        cpu_usage = [m.cpu_usage_percent for m in self.metrics_history if m.cpu_usage_percent > 0]
        
        return {
            'total_monitoring_time_minutes': len(self.metrics_history) * (self.monitoring_interval / 60),
            'api_response_time': {
                'avg': statistics.mean(api_times) if api_times else 0,
                'min': min(api_times) if api_times else 0,
                'max': max(api_times) if api_times else 0,
                'p95': np.percentile(api_times, 95) if api_times else 0
            },
            'api_success_rate': {
                'avg': statistics.mean(success_rates) if success_rates else 0,
                'min': min(success_rates) if success_rates else 0
            },
            'memory_usage_mb': {
                'avg': statistics.mean(memory_usage) if memory_usage else 0,
                'max': max(memory_usage) if memory_usage else 0
            },
            'cpu_usage_percent': {
                'avg': statistics.mean(cpu_usage) if cpu_usage else 0,
                'max': max(cpu_usage) if cpu_usage else 0
            },
            'total_errors': sum(self.error_counts.values()),
            'error_breakdown': self.error_counts
        }
    
    def start_monitoring(self):
        """å¼€å§‹æ€§èƒ½ç›‘æ§"""
        logger.info(f"å¼€å§‹TradingAgentsæ€§èƒ½ç›‘æ§ - é—´éš”: {self.monitoring_interval}ç§’")
        self.is_monitoring = True
        self.start_time = datetime.now()
        
        try:
            while self.is_monitoring:
                # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
                metrics = self.collect_metrics()
                
                # æ˜¾ç¤ºå®æ—¶çŠ¶æ€
                self.print_real_time_status(metrics)
                
                # æ¯10åˆ†é’Ÿä¿å­˜ä¸€æ¬¡æ•°æ®
                if len(self.metrics_history) % 10 == 0:
                    self.save_metrics_to_file()
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªç›‘æ§å‘¨æœŸ
                time.sleep(self.monitoring_interval)
                
        except KeyboardInterrupt:
            logger.info("ç›‘æ§è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            logger.error(f"ç›‘æ§è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        finally:
            self.stop_monitoring()
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.is_monitoring = False
        logger.info("æ€§èƒ½ç›‘æ§å·²åœæ­¢")
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.save_metrics_to_file(f"final_performance_report_{timestamp}.json")
        
        # ç”Ÿæˆç›‘æ§ä»ªè¡¨æ¿
        self.generate_dashboard_report()
    
    def generate_dashboard_report(self):
        """ç”Ÿæˆç›‘æ§ä»ªè¡¨æ¿æŠ¥å‘Š"""
        try:
            from .monitor_dashboard_generator import DashboardGenerator
            generator = DashboardGenerator(self.metrics_history, self.api_calls_history, self.error_counts)
            dashboard_content = generator.generate()
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"monitor-dashboard_{timestamp}.md"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(dashboard_content)
            
            logger.info(f"ç›‘æ§ä»ªè¡¨æ¿æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
            
        except ImportError:
            logger.warning("ä»ªè¡¨æ¿ç”Ÿæˆå™¨æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†ç”Ÿæˆç®€å•æŠ¥å‘Š")
            self.generate_simple_dashboard()
    
    def generate_simple_dashboard(self):
        """ç”Ÿæˆç®€å•çš„ä»ªè¡¨æ¿æŠ¥å‘Š"""
        if not self.metrics_history:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary = self.generate_summary_stats()
        
        report = f"""# ğŸ“Š TradingAgents æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**ç›‘æ§æ—¶é•¿**: {summary.get('total_monitoring_time_minutes', 0):.1f} åˆ†é’Ÿ  
**ç›‘æ§è½®æ¬¡**: {len(self.metrics_history)} æ¬¡

## ğŸ¯ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡

### API æ€§èƒ½
- **å¹³å‡å“åº”æ—¶é—´**: {summary.get('api_response_time', {}).get('avg', 0):.1f}ms
- **P95å“åº”æ—¶é—´**: {summary.get('api_response_time', {}).get('p95', 0):.1f}ms
- **æœ€å¤§å“åº”æ—¶é—´**: {summary.get('api_response_time', {}).get('max', 0):.1f}ms
- **å¹³å‡æˆåŠŸç‡**: {summary.get('api_success_rate', {}).get('avg', 0):.1f}%

### ç³»ç»Ÿèµ„æº
- **å¹³å‡å†…å­˜ä½¿ç”¨**: {summary.get('memory_usage_mb', {}).get('avg', 0):.1f}MB
- **å³°å€¼å†…å­˜ä½¿ç”¨**: {summary.get('memory_usage_mb', {}).get('max', 0):.1f}MB
- **å¹³å‡CPUä½¿ç”¨**: {summary.get('cpu_usage_percent', {}).get('avg', 0):.1f}%
- **å³°å€¼CPUä½¿ç”¨**: {summary.get('cpu_usage_percent', {}).get('max', 0):.1f}%

### é”™è¯¯ç»Ÿè®¡
- **æ€»é”™è¯¯æ•°**: {summary.get('total_errors', 0)}
- **APIé”™è¯¯**: {self.error_counts.get('api_errors', 0)}
- **è®¡ç®—é”™è¯¯**: {self.error_counts.get('calculation_errors', 0)}
- **è¶…æ—¶é”™è¯¯**: {self.error_counts.get('timeout_errors', 0)}
- **å†…å­˜é”™è¯¯**: {self.error_counts.get('memory_errors', 0)}

## ğŸ“ˆ æ€§èƒ½è¯„ä¼°

### æ€»ä½“å¥åº·çŠ¶æ€
"""
        # æ ¹æ®æ€§èƒ½æŒ‡æ ‡è¯„ä¼°å¥åº·çŠ¶æ€
        avg_api_time = summary.get('api_response_time', {}).get('avg', 0)
        avg_success_rate = summary.get('api_success_rate', {}).get('avg', 0)
        total_errors = summary.get('total_errors', 0)
        
        if avg_api_time < 1000 and avg_success_rate > 95 and total_errors < 5:
            health_status = "ğŸŸ¢ ä¼˜ç§€"
            health_desc = "ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œæ‰€æœ‰æŒ‡æ ‡å‡åœ¨æœ€ä½³èŒƒå›´å†…"
        elif avg_api_time < 2000 and avg_success_rate > 90 and total_errors < 15:
            health_status = "ğŸŸ¡ è‰¯å¥½"
            health_desc = "ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œéƒ¨åˆ†æŒ‡æ ‡éœ€è¦å…³æ³¨"
        else:
            health_status = "ğŸ”´ éœ€è¦ä¼˜åŒ–"
            health_desc = "ç³»ç»Ÿå­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œå»ºè®®ä¼˜åŒ–"
        
        report += f"**çŠ¶æ€**: {health_status}  \n**è¯´æ˜**: {health_desc}\n\n"
        
        # ä¿å­˜ç®€å•ä»ªè¡¨æ¿
        filename = f"monitor-dashboard_{timestamp}.md"
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ç®€å•ç›‘æ§ä»ªè¡¨æ¿å·²ç”Ÿæˆ: {filename}")

def run_benchmark_test(api_key: str):
    """è¿è¡ŒåŸºå‡†æ€§èƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹TradingAgentsåŸºå‡†æ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    monitor = PerformanceMonitor(api_key, monitoring_interval=30)
    
    # è¿è¡Œ5åˆ†é’Ÿçš„åŸºå‡†æµ‹è¯•
    monitor.start_time = datetime.now()
    end_time = monitor.start_time + timedelta(minutes=5)
    
    test_round = 1
    while datetime.now() < end_time:
        print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•è½®æ¬¡ {test_round}")
        print("-"*40)
        
        metrics = monitor.collect_metrics()
        monitor.print_real_time_status(metrics)
        
        test_round += 1
        time.sleep(30)  # æ¯30ç§’æµ‹è¯•ä¸€æ¬¡
    
    monitor.stop_monitoring()
    print("\nâœ… åŸºå‡†æ€§èƒ½æµ‹è¯•å®Œæˆ!")

def main():
    """ä¸»å‡½æ•° - å¯åŠ¨æ€§èƒ½ç›‘æ§"""
    api_key = "d1vm82hr01qqgeemb9r0d1vm82hr01qqgeemb9rg"
    
    print("""
ğŸ¯ TradingAgents æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
================================

é€‰æ‹©è¿è¡Œæ¨¡å¼:
1. æŒç»­ç›‘æ§æ¨¡å¼ (æ¨è)
2. åŸºå‡†æµ‹è¯•æ¨¡å¼ (5åˆ†é’Ÿå¿«é€Ÿæµ‹è¯•)
3. å•æ¬¡æ€§èƒ½æ£€æŸ¥

è¯·è¾“å…¥é€‰æ‹© (1-3):
""")
    
    try:
        choice = input().strip()
        
        if choice == '1':
            print("å¯åŠ¨æŒç»­ç›‘æ§æ¨¡å¼...")
            monitor = PerformanceMonitor(api_key, monitoring_interval=60)
            monitor.start_monitoring()
        elif choice == '2':
            print("å¯åŠ¨åŸºå‡†æµ‹è¯•æ¨¡å¼...")
            run_benchmark_test(api_key)
        elif choice == '3':
            print("æ‰§è¡Œå•æ¬¡æ€§èƒ½æ£€æŸ¥...")
            monitor = PerformanceMonitor(api_key)
            metrics = monitor.collect_metrics()
            monitor.print_real_time_status(metrics)
            monitor.save_metrics_to_file()
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œé€€å‡ºç¨‹åº")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œé”™è¯¯: {str(e)}")

if __name__ == "__main__":
    main()