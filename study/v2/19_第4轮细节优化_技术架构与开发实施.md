# ç¬¬4è½®ç»†èŠ‚ä¼˜åŒ–ï¼šæ™ºé“¾å¹³å°v2æŠ€æœ¯æ¶æ„ä¸å¼€å‘å®æ–½

**ç‰ˆæœ¬**: v2.0 | **æ—¥æœŸ**: 2025-08-12 | **çŠ¶æ€**: ç¬¬4è½®ç»†èŠ‚ä¼˜åŒ–  
**ç›®æ ‡**: åŸºäº2025å¹´æœ€ä½³å®è·µæ„å»ºå¯æ‰©å±•ã€é«˜å¯ç”¨ã€å®‰å…¨çš„å¾®æœåŠ¡æ¶æ„ç³»ç»Ÿ

---

## ğŸ—ï¸ å¾®æœåŠ¡æ¶æ„è®¾è®¡

### æ ¸å¿ƒå¾®æœåŠ¡æ‹†åˆ†ç­–ç•¥
```yaml
å¾®æœåŠ¡æ¶æ„æ€»è§ˆ:
  
  # ç”¨æˆ·åŸŸæœåŠ¡
  user-service:
    èŒè´£: ç”¨æˆ·è®¤è¯ã€æˆæƒã€ç”»åƒç®¡ç†
    æŠ€æœ¯æ ˆ: Node.js + TypeScript + JWT + Redis
    æ•°æ®å­˜å‚¨: PostgreSQL (ç”¨æˆ·ä¿¡æ¯) + Redis (ä¼šè¯ç¼“å­˜)
    ç«¯å£: 3001
    å®¹å™¨èµ„æº: CPU 0.5æ ¸, å†…å­˜ 512MB
  
  # æ¨èå¼•æ“æœåŠ¡  
  recommendation-service:
    èŒè´£: AIæ¨èç®—æ³•ã€å…­è§’è‰²åä½œå¼•æ“
    æŠ€æœ¯æ ˆ: Python + FastAPI + TensorFlow + Celery
    æ•°æ®å­˜å‚¨: PostgreSQL + Elasticsearch + Redis
    ç«¯å£: 3002
    å®¹å™¨èµ„æº: CPU 2æ ¸, å†…å­˜ 4GB
    
  # ä¾›åº”å•†ç®¡ç†æœåŠ¡
  supplier-service:
    èŒè´£: ä¾›åº”å•†æ³¨å†Œã€èƒ½åŠ›ç®¡ç†ã€è´¨é‡è¯„ä¼°
    æŠ€æœ¯æ ˆ: Node.js + TypeScript + Prisma
    æ•°æ®å­˜å‚¨: PostgreSQL + MongoDB (éç»“æ„åŒ–æ•°æ®)
    ç«¯å£: 3003
    å®¹å™¨èµ„æº: CPU 1æ ¸, å†…å­˜ 1GB
    
  # äº¤æ˜“ç®¡ç†æœåŠ¡
  transaction-service:
    èŒè´£: è®¢å•å¤„ç†ã€æ”¯ä»˜ç®¡ç†ã€ç»“ç®—
    æŠ€æœ¯æ ˆ: Node.js + TypeScript + Stripe SDK
    æ•°æ®å­˜å‚¨: PostgreSQL + Redis
    ç«¯å£: 3004  
    å®¹å™¨èµ„æº: CPU 1æ ¸, å†…å­˜ 1GB
    
  # é€šçŸ¥æœåŠ¡
  notification-service:
    èŒè´£: é‚®ä»¶ã€çŸ­ä¿¡ã€ç«™å†…æ¶ˆæ¯æ¨é€
    æŠ€æœ¯æ ˆ: Node.js + TypeScript + RabbitMQ
    æ•°æ®å­˜å‚¨: PostgreSQL + Redis
    ç«¯å£: 3005
    å®¹å™¨èµ„æº: CPU 0.5æ ¸, å†…å­˜ 512MB
    
  # æ•°æ®åˆ†ææœåŠ¡
  analytics-service:
    èŒè´£: ä¸šåŠ¡æ•°æ®åˆ†æã€æŠ¥è¡¨ç”Ÿæˆã€ç›‘æ§æŒ‡æ ‡
    æŠ€æœ¯æ ˆ: Python + FastAPI + Pandas + ClickHouse
    æ•°æ®å­˜å‚¨: ClickHouse (æ—¶åºæ•°æ®) + PostgreSQL
    ç«¯å£: 3006
    å®¹å™¨èµ„æº: CPU 1æ ¸, å†…å­˜ 2GB
    
  # APIç½‘å…³
  api-gateway:
    èŒè´£: è·¯ç”±ã€è®¤è¯ã€é™æµã€è´Ÿè½½å‡è¡¡
    æŠ€æœ¯æ ˆ: Kong + Lua + Redis
    ç«¯å£: 3000
    å®¹å™¨èµ„æº: CPU 1æ ¸, å†…å­˜ 512MB
```

### DockeråŒ–éƒ¨ç½²é…ç½®
```dockerfile
# æ¨èæœåŠ¡ Dockerfile
FROM node:18-alpine AS builder

WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY package*.json ./
COPY tsconfig.json ./

# å®‰è£…ä¾èµ–
RUN npm ci --only=production

# å¤åˆ¶æºä»£ç 
COPY src/ ./src/

# æ„å»ºåº”ç”¨
RUN npm run build

# ç”Ÿäº§ç¯å¢ƒé•œåƒ
FROM node:18-alpine AS production

WORKDIR /app

# åˆ›å»ºérootç”¨æˆ·
RUN addgroup -g 1001 -S nodejs
RUN adduser -S nodeuser -u 1001

# å¤åˆ¶æ„å»ºäº§ç‰©
COPY --from=builder --chown=nodeuser:nodejs /app/dist ./dist
COPY --from=builder --chown=nodeuser:nodejs /app/node_modules ./node_modules
COPY --from=builder --chown=nodeuser:nodejs /app/package*.json ./

# è®¾ç½®ç”¨æˆ·
USER nodeuser

# æš´éœ²ç«¯å£
EXPOSE 3002

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:3002/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["node", "dist/index.js"]
```

### Kuberneteséƒ¨ç½²é…ç½®
```yaml
# recommendation-service-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recommendation-service
  labels:
    app: recommendation-service
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: recommendation-service
  template:
    metadata:
      labels:
        app: recommendation-service
        version: v1
    spec:
      containers:
      - name: recommendation-service
        image: zhilink/recommendation-service:latest
        ports:
        - containerPort: 3002
        env:
        - name: NODE_ENV
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-secrets
              key: recommendation-db-url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-secrets
              key: redis-url
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 3002
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 3002
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
      volumes:
      - name: config-volume
        configMap:
          name: recommendation-config

---
apiVersion: v1
kind: Service
metadata:
  name: recommendation-service
spec:
  selector:
    app: recommendation-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3002
  type: ClusterIP

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: recommendation-service-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
spec:
  tls:
  - hosts:
    - api.zhilink.com
    secretName: zhilink-tls
  rules:
  - host: api.zhilink.com
    http:
      paths:
      - path: /api/v1/recommendations
        pathType: Prefix
        backend:
          service:
            name: recommendation-service
            port:
              number: 80
```

---

## ğŸ’¾ æ•°æ®åº“è®¾è®¡ä¸ä¼˜åŒ–

### PostgreSQLæ•°æ®åº“æ¨¡å¼è®¾è®¡
```sql
-- ç”¨æˆ·è¡¨è®¾è®¡
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    company_name VARCHAR(255) NOT NULL,
    company_size company_size_enum NOT NULL,
    industry industry_enum NOT NULL,
    role user_role_enum DEFAULT 'client',
    profile JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_company_size ON users(company_size);
CREATE INDEX idx_users_industry ON users(industry);
CREATE INDEX idx_users_created_at ON users(created_at);
CREATE INDEX idx_users_profile_gin ON users USING gin(profile);

-- éœ€æ±‚è¡¨è®¾è®¡
CREATE TABLE requirements (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    budget_min INTEGER,
    budget_max INTEGER,
    timeline_days INTEGER,
    complexity_score DECIMAL(3,2) CHECK (complexity_score >= 0 AND complexity_score <= 1),
    industry industry_enum NOT NULL,
    urgency_level urgency_enum DEFAULT 'medium',
    status requirement_status_enum DEFAULT 'draft',
    tags TEXT[] DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- éœ€æ±‚è¡¨ç´¢å¼•
CREATE INDEX idx_requirements_user_id ON requirements(user_id);
CREATE INDEX idx_requirements_status ON requirements(status);
CREATE INDEX idx_requirements_industry ON requirements(industry);
CREATE INDEX idx_requirements_budget ON requirements(budget_min, budget_max);
CREATE INDEX idx_requirements_timeline ON requirements(timeline_days);
CREATE INDEX idx_requirements_complexity ON requirements(complexity_score);
CREATE INDEX idx_requirements_tags_gin ON requirements USING gin(tags);
CREATE INDEX idx_requirements_created_at ON requirements(created_at DESC);

-- ä¾›åº”å•†èƒ½åŠ›è¡¨è®¾è®¡
CREATE TABLE supplier_capabilities (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    supplier_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    category capability_category_enum NOT NULL,
    subcategory VARCHAR(100),
    skills TEXT[] NOT NULL DEFAULT '{}',
    pricing_model pricing_model_enum NOT NULL,
    base_price INTEGER,
    unit_price INTEGER,
    commission_rate DECIMAL(5,2),
    min_budget INTEGER,
    max_budget INTEGER,
    estimated_timeline INTEGER,
    success_rate DECIMAL(5,2) DEFAULT 0,
    rating DECIMAL(3,2) DEFAULT 0,
    review_count INTEGER DEFAULT 0,
    is_active BOOLEAN DEFAULT true,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- ä¾›åº”å•†èƒ½åŠ›è¡¨ç´¢å¼•
CREATE INDEX idx_capabilities_supplier_id ON supplier_capabilities(supplier_id);
CREATE INDEX idx_capabilities_category ON supplier_capabilities(category);
CREATE INDEX idx_capabilities_skills_gin ON supplier_capabilities USING gin(skills);
CREATE INDEX idx_capabilities_pricing ON supplier_capabilities(pricing_model, base_price);
CREATE INDEX idx_capabilities_rating ON supplier_capabilities(rating DESC);
CREATE INDEX idx_capabilities_active ON supplier_capabilities(is_active) WHERE is_active = true;

-- æ¨èè®°å½•è¡¨
CREATE TABLE recommendations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    requirement_id UUID NOT NULL REFERENCES requirements(id) ON DELETE CASCADE,
    capability_id UUID NOT NULL REFERENCES supplier_capabilities(id) ON DELETE CASCADE,
    algorithm_version VARCHAR(50) NOT NULL,
    confidence_score DECIMAL(5,4) NOT NULL,
    ranking INTEGER NOT NULL,
    reasoning JSONB DEFAULT '{}',
    user_action recommendation_action_enum,
    feedback_rating INTEGER CHECK (feedback_rating >= 1 AND feedback_rating <= 5),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- æ¨èè®°å½•è¡¨ç´¢å¼•
CREATE INDEX idx_recommendations_requirement ON recommendations(requirement_id);
CREATE INDEX idx_recommendations_capability ON recommendations(capability_id);
CREATE INDEX idx_recommendations_confidence ON recommendations(confidence_score DESC);
CREATE INDEX idx_recommendations_ranking ON recommendations(ranking);
CREATE INDEX idx_recommendations_created_at ON recommendations(created_at DESC);

-- æšä¸¾ç±»å‹å®šä¹‰
CREATE TYPE company_size_enum AS ENUM ('startup', 'small', 'medium', 'large');
CREATE TYPE industry_enum AS ENUM ('manufacturing', 'finance', 'retail', 'healthcare', 'education', 'other');
CREATE TYPE user_role_enum AS ENUM ('client', 'supplier', 'admin');
CREATE TYPE urgency_enum AS ENUM ('low', 'medium', 'high');
CREATE TYPE requirement_status_enum AS ENUM ('draft', 'published', 'in_progress', 'completed', 'cancelled');
CREATE TYPE capability_category_enum AS ENUM ('workforce', 'expert_module', 'market_report');
CREATE TYPE pricing_model_enum AS ENUM ('fixed', 'hourly', 'project', 'commission');
CREATE TYPE recommendation_action_enum AS ENUM ('viewed', 'clicked', 'contacted', 'hired', 'dismissed');
```

### Redisç¼“å­˜ç­–ç•¥è®¾è®¡
```typescript
// Redisç¼“å­˜ç®¡ç†ç±»
class CacheManager {
  private redis: Redis.Redis;
  
  constructor() {
    this.redis = new Redis({
      host: process.env.REDIS_HOST,
      port: parseInt(process.env.REDIS_PORT || '6379'),
      password: process.env.REDIS_PASSWORD,
      retryDelayOnFailover: 100,
      db: 0,
    });
  }
  
  // ç”¨æˆ·ä¼šè¯ç¼“å­˜
  async setUserSession(userId: string, sessionData: object, ttl: number = 3600): Promise<void> {
    const key = `session:${userId}`;
    await this.redis.setex(key, ttl, JSON.stringify(sessionData));
  }
  
  async getUserSession(userId: string): Promise<object | null> {
    const key = `session:${userId}`;
    const data = await this.redis.get(key);
    return data ? JSON.parse(data) : null;
  }
  
  // æ¨èç»“æœç¼“å­˜
  async cacheRecommendations(requirementId: string, recommendations: any[], ttl: number = 1800): Promise<void> {
    const key = `recommendations:${requirementId}`;
    await this.redis.setex(key, ttl, JSON.stringify(recommendations));
  }
  
  async getCachedRecommendations(requirementId: string): Promise<any[] | null> {
    const key = `recommendations:${requirementId}`;
    const data = await this.redis.get(key);
    return data ? JSON.parse(data) : null;
  }
  
  // ä¾›åº”å•†èƒ½åŠ›ç¼“å­˜
  async cacheSupplierCapabilities(supplierId: string, capabilities: any[], ttl: number = 3600): Promise<void> {
    const key = `supplier:capabilities:${supplierId}`;
    await this.redis.setex(key, ttl, JSON.stringify(capabilities));
  }
  
  // APIé™æµç¼“å­˜
  async checkRateLimit(identifier: string, limit: number, window: number): Promise<{ allowed: boolean; remaining: number }> {
    const key = `rate_limit:${identifier}`;
    
    const multi = this.redis.multi();
    multi.incr(key);
    multi.expire(key, window);
    
    const results = await multi.exec();
    const current = results?.[0]?.[1] as number || 0;
    
    const allowed = current <= limit;
    const remaining = Math.max(0, limit - current);
    
    return { allowed, remaining };
  }
  
  // çƒ­é—¨æœç´¢ç¼“å­˜
  async cachePopularSearches(searches: string[], ttl: number = 7200): Promise<void> {
    const key = 'popular_searches';
    await this.redis.setex(key, ttl, JSON.stringify(searches));
  }
  
  // ç¼“å­˜å¤±æ•ˆç­–ç•¥
  async invalidateUserCache(userId: string): Promise<void> {
    const patterns = [
      `session:${userId}`,
      `user:profile:${userId}`,
      `user:preferences:${userId}`
    ];
    
    for (const pattern of patterns) {
      await this.redis.del(pattern);
    }
  }
}
```

---

## ğŸ”„ æ¶ˆæ¯é˜Ÿåˆ—ä¸å¼‚æ­¥å¤„ç†

### RabbitMQæ¶ˆæ¯é˜Ÿåˆ—é…ç½®
```typescript
// æ¶ˆæ¯é˜Ÿåˆ—ç®¡ç†å™¨
import amqp from 'amqplib';

export class MessageQueueManager {
  private connection: amqp.Connection | null = null;
  private channel: amqp.Channel | null = null;
  
  async connect(): Promise<void> {
    try {
      this.connection = await amqp.connect(process.env.RABBITMQ_URL!);
      this.channel = await this.connection.createChannel();
      
      // å®šä¹‰äº¤æ¢å™¨
      await this.channel.assertExchange('zhilink.events', 'topic', { durable: true });
      await this.channel.assertExchange('zhilink.dlx', 'topic', { durable: true });
      
      // å®šä¹‰é˜Ÿåˆ—
      await this.setupQueues();
      
      console.log('RabbitMQè¿æ¥æˆåŠŸ');
    } catch (error) {
      console.error('RabbitMQè¿æ¥å¤±è´¥:', error);
      throw error;
    }
  }
  
  private async setupQueues(): Promise<void> {
    const queues = [
      {
        name: 'recommendation.generate',
        routingKey: 'recommendation.generate',
        options: {
          durable: true,
          arguments: {
            'x-dead-letter-exchange': 'zhilink.dlx',
            'x-dead-letter-routing-key': 'recommendation.generate.failed',
            'x-message-ttl': 300000  // 5åˆ†é’ŸTTL
          }
        }
      },
      {
        name: 'notification.send',
        routingKey: 'notification.*',
        options: {
          durable: true,
          arguments: {
            'x-dead-letter-exchange': 'zhilink.dlx',
            'x-dead-letter-routing-key': 'notification.failed'
          }
        }
      },
      {
        name: 'analytics.track',
        routingKey: 'analytics.*',
        options: { durable: true }
      }
    ];
    
    for (const queue of queues) {
      await this.channel!.assertQueue(queue.name, queue.options);
      await this.channel!.bindQueue(queue.name, 'zhilink.events', queue.routingKey);
    }
  }
  
  // å‘å¸ƒæ¶ˆæ¯
  async publish(routingKey: string, message: object, options: amqp.Options.Publish = {}): Promise<boolean> {
    if (!this.channel) {
      throw new Error('MQé¢‘é“æœªåˆå§‹åŒ–');
    }
    
    const messageBuffer = Buffer.from(JSON.stringify({
      ...message,
      timestamp: new Date().toISOString(),
      messageId: generateUUID()
    }));
    
    return this.channel.publish('zhilink.events', routingKey, messageBuffer, {
      persistent: true,
      ...options
    });
  }
  
  // æ¶ˆè´¹æ¶ˆæ¯
  async consume(queueName: string, processor: (message: any) => Promise<void>): Promise<void> {
    if (!this.channel) {
      throw new Error('MQé¢‘é“æœªåˆå§‹åŒ–');
    }
    
    await this.channel.consume(queueName, async (msg) => {
      if (!msg) return;
      
      try {
        const message = JSON.parse(msg.content.toString());
        await processor(message);
        this.channel!.ack(msg);
      } catch (error) {
        console.error(`å¤„ç†æ¶ˆæ¯å¤±è´¥ [${queueName}]:`, error);
        
        // é‡è¯•é€»è¾‘
        const retryCount = (msg.properties.headers?.['x-retry-count'] || 0) + 1;
        if (retryCount <= 3) {
          // å»¶è¿Ÿé‡è¯•
          setTimeout(() => {
            this.channel!.publish('zhilink.events', msg.fields.routingKey, msg.content, {
              headers: { 'x-retry-count': retryCount }
            });
          }, retryCount * 1000);
        }
        
        this.channel!.nack(msg, false, false); // ä¸¢å¼ƒåˆ°æ­»ä¿¡é˜Ÿåˆ—
      }
    });
  }
}

// æ¨èç”Ÿæˆå¼‚æ­¥ä»»åŠ¡å¤„ç†å™¨
export class RecommendationProcessor {
  constructor(private mqManager: MessageQueueManager) {}
  
  async startProcessing(): Promise<void> {
    await this.mqManager.consume('recommendation.generate', async (message) => {
      const { requirementId, userId } = message;
      
      console.log(`å¼€å§‹ç”Ÿæˆæ¨è: éœ€æ±‚${requirementId}, ç”¨æˆ·${userId}`);
      
      try {
        // è°ƒç”¨æ¨èç®—æ³•
        const recommendations = await this.generateRecommendations(requirementId);
        
        // ç¼“å­˜æ¨èç»“æœ
        await this.cacheRecommendations(requirementId, recommendations);
        
        // é€šçŸ¥ç”¨æˆ·æ¨èå·²ç”Ÿæˆ
        await this.mqManager.publish('notification.recommendation.ready', {
          userId,
          requirementId,
          recommendationCount: recommendations.length
        });
        
        console.log(`æ¨èç”Ÿæˆå®Œæˆ: éœ€æ±‚${requirementId}, å…±${recommendations.length}ä¸ªæ¨è`);
      } catch (error) {
        console.error(`æ¨èç”Ÿæˆå¤±è´¥: éœ€æ±‚${requirementId}`, error);
        throw error;
      }
    });
  }
  
  private async generateRecommendations(requirementId: string): Promise<any[]> {
    // å®é™…çš„æ¨èç”Ÿæˆé€»è¾‘
    // è¿™é‡Œä¼šè°ƒç”¨å…­è§’è‰²åä½œæ¨èå¼•æ“
    return [];
  }
  
  private async cacheRecommendations(requirementId: string, recommendations: any[]): Promise<void> {
    // ç¼“å­˜æ¨èç»“æœ
  }
}
```

---

## ğŸ” å®‰å…¨ä¸è®¤è¯ç³»ç»Ÿ

### JWTè®¤è¯ä¸æˆæƒå®ç°
```typescript
import jwt from 'jsonwebtoken';
import bcrypt from 'bcrypt';
import { Request, Response, NextFunction } from 'express';

export class AuthService {
  private jwtSecret: string;
  private jwtRefreshSecret: string;
  
  constructor() {
    this.jwtSecret = process.env.JWT_SECRET!;
    this.jwtRefreshSecret = process.env.JWT_REFRESH_SECRET!;
  }
  
  // ç”Ÿæˆè®¿é—®ä»¤ç‰Œ
  generateAccessToken(payload: { userId: string; email: string; role: string }): string {
    return jwt.sign(payload, this.jwtSecret, {
      expiresIn: '15m',
      issuer: 'zhilink-platform',
      audience: 'zhilink-client'
    });
  }
  
  // ç”Ÿæˆåˆ·æ–°ä»¤ç‰Œ
  generateRefreshToken(payload: { userId: string }): string {
    return jwt.sign(payload, this.jwtRefreshSecret, {
      expiresIn: '7d',
      issuer: 'zhilink-platform',
      audience: 'zhilink-client'
    });
  }
  
  // éªŒè¯è®¿é—®ä»¤ç‰Œ
  verifyAccessToken(token: string): { userId: string; email: string; role: string } | null {
    try {
      const decoded = jwt.verify(token, this.jwtSecret, {
        issuer: 'zhilink-platform',
        audience: 'zhilink-client'
      }) as any;
      
      return {
        userId: decoded.userId,
        email: decoded.email,
        role: decoded.role
      };
    } catch (error) {
      return null;
    }
  }
  
  // å¯†ç åŠ å¯†
  async hashPassword(password: string): Promise<string> {
    const saltRounds = 12;
    return bcrypt.hash(password, saltRounds);
  }
  
  // å¯†ç éªŒè¯
  async verifyPassword(password: string, hash: string): Promise<boolean> {
    return bcrypt.compare(password, hash);
  }
}

// è®¤è¯ä¸­é—´ä»¶
export const authMiddleware = (requiredRoles: string[] = []) => {
  return async (req: Request, res: Response, next: NextFunction) => {
    try {
      const authHeader = req.headers.authorization;
      
      if (!authHeader || !authHeader.startsWith('Bearer ')) {
        return res.status(401).json({ error: 'ç¼ºå°‘æœ‰æ•ˆçš„è®¤è¯ä»¤ç‰Œ' });
      }
      
      const token = authHeader.substring(7);
      const authService = new AuthService();
      const payload = authService.verifyAccessToken(token);
      
      if (!payload) {
        return res.status(401).json({ error: 'ä»¤ç‰Œæ— æ•ˆæˆ–å·²è¿‡æœŸ' });
      }
      
      // è§’è‰²æƒé™æ£€æŸ¥
      if (requiredRoles.length > 0 && !requiredRoles.includes(payload.role)) {
        return res.status(403).json({ error: 'æƒé™ä¸è¶³' });
      }
      
      // å°†ç”¨æˆ·ä¿¡æ¯é™„åŠ åˆ°è¯·æ±‚å¯¹è±¡
      req.user = payload;
      next();
    } catch (error) {
      console.error('è®¤è¯ä¸­é—´ä»¶é”™è¯¯:', error);
      res.status(500).json({ error: 'è®¤è¯æœåŠ¡å†…éƒ¨é”™è¯¯' });
    }
  };
};

// APIé™æµä¸­é—´ä»¶
export const rateLimitMiddleware = (limit: number, windowMs: number) => {
  return async (req: Request, res: Response, next: NextFunction) => {
    const identifier = req.ip + ':' + (req.user?.userId || 'anonymous');
    
    const cacheManager = new CacheManager();
    const { allowed, remaining } = await cacheManager.checkRateLimit(
      identifier, 
      limit, 
      Math.floor(windowMs / 1000)
    );
    
    // è®¾ç½®å“åº”å¤´
    res.set({
      'X-RateLimit-Limit': limit.toString(),
      'X-RateLimit-Remaining': remaining.toString(),
      'X-RateLimit-Reset': new Date(Date.now() + windowMs).toISOString()
    });
    
    if (!allowed) {
      return res.status(429).json({ 
        error: 'è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•',
        retryAfter: Math.ceil(windowMs / 1000)
      });
    }
    
    next();
  };
};

// CORSé…ç½®
export const corsOptions = {
  origin: function (origin: string | undefined, callback: Function) {
    const allowedOrigins = process.env.ALLOWED_ORIGINS?.split(',') || ['http://localhost:3000'];
    
    if (!origin || allowedOrigins.includes(origin)) {
      callback(null, true);
    } else {
      callback(new Error('ä¸è¢«CORSç­–ç•¥å…è®¸çš„æ¥æº'));
    }
  },
  credentials: true,
  methods: ['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS'],
  allowedHeaders: ['Content-Type', 'Authorization', 'X-Requested-With'],
  maxAge: 86400 // 24å°æ—¶
};
```

---

## ğŸ“Š ç›‘æ§ä¸æ—¥å¿—ç³»ç»Ÿ

### åº”ç”¨æ€§èƒ½ç›‘æ§é…ç½®
```typescript
// ç›‘æ§æœåŠ¡
import prometheus from 'prom-client';
import winston from 'winston';

export class MonitoringService {
  private static instance: MonitoringService;
  private registry: prometheus.Registry;
  private logger: winston.Logger;
  
  // PrometheusæŒ‡æ ‡
  private httpRequestDuration: prometheus.Histogram<string>;
  private httpRequestTotal: prometheus.Counter<string>;
  private activeConnections: prometheus.Gauge<string>;
  private dbConnectionPool: prometheus.Gauge<string>;
  private recommendationLatency: prometheus.Histogram<string>;
  
  constructor() {
    this.registry = new prometheus.Registry();
    this.setupMetrics();
    this.setupLogger();
  }
  
  static getInstance(): MonitoringService {
    if (!MonitoringService.instance) {
      MonitoringService.instance = new MonitoringService();
    }
    return MonitoringService.instance;
  }
  
  private setupMetrics(): void {
    // HTTPè¯·æ±‚æŒç»­æ—¶é—´
    this.httpRequestDuration = new prometheus.Histogram({
      name: 'http_request_duration_seconds',
      help: 'HTTPè¯·æ±‚æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰',
      labelNames: ['method', 'route', 'status_code'],
      buckets: [0.1, 0.5, 1, 2, 5, 10]
    });
    
    // HTTPè¯·æ±‚æ€»æ•°
    this.httpRequestTotal = new prometheus.Counter({
      name: 'http_requests_total',
      help: 'HTTPè¯·æ±‚æ€»æ•°',
      labelNames: ['method', 'route', 'status_code']
    });
    
    // æ´»è·ƒè¿æ¥æ•°
    this.activeConnections = new prometheus.Gauge({
      name: 'active_connections',
      help: 'å½“å‰æ´»è·ƒè¿æ¥æ•°'
    });
    
    // æ•°æ®åº“è¿æ¥æ± 
    this.dbConnectionPool = new prometheus.Gauge({
      name: 'db_connection_pool',
      help: 'æ•°æ®åº“è¿æ¥æ± çŠ¶æ€',
      labelNames: ['state'] // idle, active, waiting
    });
    
    // æ¨èç”Ÿæˆå»¶è¿Ÿ
    this.recommendationLatency = new prometheus.Histogram({
      name: 'recommendation_generation_seconds',
      help: 'æ¨èç”Ÿæˆè€—æ—¶ï¼ˆç§’ï¼‰',
      labelNames: ['algorithm_type'],
      buckets: [1, 5, 10, 30, 60, 120]
    });
    
    // æ³¨å†Œæ‰€æœ‰æŒ‡æ ‡
    this.registry.registerMetric(this.httpRequestDuration);
    this.registry.registerMetric(this.httpRequestTotal);
    this.registry.registerMetric(this.activeConnections);
    this.registry.registerMetric(this.dbConnectionPool);
    this.registry.registerMetric(this.recommendationLatency);
    
    // æ”¶é›†é»˜è®¤æŒ‡æ ‡
    prometheus.collectDefaultMetrics({ register: this.registry });
  }
  
  private setupLogger(): void {
    this.logger = winston.createLogger({
      level: process.env.LOG_LEVEL || 'info',
      format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.errors({ stack: true }),
        winston.format.json()
      ),
      defaultMeta: { service: 'zhilink-platform' },
      transports: [
        new winston.transports.File({ 
          filename: 'logs/error.log', 
          level: 'error',
          maxsize: 10485760, // 10MB
          maxFiles: 5
        }),
        new winston.transports.File({ 
          filename: 'logs/combined.log',
          maxsize: 10485760,
          maxFiles: 10
        }),
        new winston.transports.Console({
          format: winston.format.combine(
            winston.format.colorize(),
            winston.format.simple()
          )
        })
      ]
    });
  }
  
  // HTTPè¯·æ±‚ç›‘æ§ä¸­é—´ä»¶
  httpMetricsMiddleware() {
    return (req: Request, res: Response, next: NextFunction) => {
      const startTime = Date.now();
      
      res.on('finish', () => {
        const duration = (Date.now() - startTime) / 1000;
        const route = req.route?.path || req.path;
        
        this.httpRequestDuration.observe(
          { method: req.method, route, status_code: res.statusCode.toString() },
          duration
        );
        
        this.httpRequestTotal.inc({
          method: req.method,
          route,
          status_code: res.statusCode.toString()
        });
      });
      
      next();
    };
  }
  
  // è®°å½•æ¨èç”ŸæˆæŒ‡æ ‡
  recordRecommendationLatency(algorithmType: string, durationSeconds: number): void {
    this.recommendationLatency.observe({ algorithm_type: algorithmType }, durationSeconds);
  }
  
  // æ›´æ–°æ•°æ®åº“è¿æ¥æ± æŒ‡æ ‡
  updateDbConnectionPool(idle: number, active: number, waiting: number): void {
    this.dbConnectionPool.set({ state: 'idle' }, idle);
    this.dbConnectionPool.set({ state: 'active' }, active);
    this.dbConnectionPool.set({ state: 'waiting' }, waiting);
  }
  
  // è·å–æŒ‡æ ‡æ•°æ®
  async getMetrics(): Promise<string> {
    return this.registry.metrics();
  }
  
  // æ—¥å¿—è®°å½•æ–¹æ³•
  log(level: string, message: string, meta?: any): void {
    this.logger.log(level, message, meta);
  }
  
  info(message: string, meta?: any): void {
    this.logger.info(message, meta);
  }
  
  error(message: string, error?: Error, meta?: any): void {
    this.logger.error(message, { error: error?.stack, ...meta });
  }
  
  warn(message: string, meta?: any): void {
    this.logger.warn(message, meta);
  }
}

// å¥åº·æ£€æŸ¥ç«¯ç‚¹
export class HealthCheckService {
  async getHealthStatus(): Promise<{
    status: 'healthy' | 'unhealthy' | 'degraded';
    timestamp: string;
    services: Record<string, any>;
  }> {
    const checks = await Promise.allSettled([
      this.checkDatabase(),
      this.checkRedis(),
      this.checkMessageQueue(),
      this.checkExternalServices()
    ]);
    
    const results = {
      database: checks[0],
      redis: checks[1],
      messageQueue: checks[2],
      externalServices: checks[3]
    };
    
    const failures = checks.filter(check => check.status === 'rejected').length;
    let overallStatus: 'healthy' | 'unhealthy' | 'degraded';
    
    if (failures === 0) {
      overallStatus = 'healthy';
    } else if (failures <= 1) {
      overallStatus = 'degraded';
    } else {
      overallStatus = 'unhealthy';
    }
    
    return {
      status: overallStatus,
      timestamp: new Date().toISOString(),
      services: results
    };
  }
  
  private async checkDatabase(): Promise<{ status: string; responseTime: number }> {
    const start = Date.now();
    try {
      // æ‰§è¡Œç®€å•çš„æ•°æ®åº“æŸ¥è¯¢
      await pool.query('SELECT 1');
      return {
        status: 'healthy',
        responseTime: Date.now() - start
      };
    } catch (error) {
      throw {
        status: 'unhealthy',
        error: error.message,
        responseTime: Date.now() - start
      };
    }
  }
  
  private async checkRedis(): Promise<{ status: string; responseTime: number }> {
    const start = Date.now();
    try {
      const redis = new Redis();
      await redis.ping();
      return {
        status: 'healthy',
        responseTime: Date.now() - start
      };
    } catch (error) {
      throw {
        status: 'unhealthy',
        error: error.message,
        responseTime: Date.now() - start
      };
    }
  }
  
  private async checkMessageQueue(): Promise<{ status: string; responseTime: number }> {
    // RabbitMQå¥åº·æ£€æŸ¥å®ç°
    return { status: 'healthy', responseTime: 10 };
  }
  
  private async checkExternalServices(): Promise<{ status: string; responseTime: number }> {
    // å¤–éƒ¨æœåŠ¡å¥åº·æ£€æŸ¥å®ç°
    return { status: 'healthy', responseTime: 50 };
  }
}
```

---

## ğŸš€ CI/CDéƒ¨ç½²æµæ°´çº¿

### GitHub Actionsé…ç½®
```yaml
# .github/workflows/deploy.yml
name: Deploy to Kubernetes

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  DOCKER_REGISTRY: ghcr.io
  IMAGE_NAME: zhilink/platform

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: zhilink_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Run type checking
      run: npm run type-check
    
    - name: Run linting
      run: npm run lint
    
    - name: Run unit tests
      run: npm run test:unit
      env:
        DATABASE_URL: postgresql://postgres:test@localhost:5432/zhilink_test
        REDIS_URL: redis://localhost:6379
    
    - name: Run integration tests
      run: npm run test:integration
      env:
        DATABASE_URL: postgresql://postgres:test@localhost:5432/zhilink_test
        REDIS_URL: redis://localhost:6379
    
    - name: Generate test coverage
      run: npm run test:coverage
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage/lcov.info

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Login to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.DOCKER_REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix=commit-
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: [test, build]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
    
    - name: Configure kubectl
      run: |
        echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
        export KUBECONFIG=kubeconfig
    
    - name: Deploy to Kubernetes
      run: |
        export KUBECONFIG=kubeconfig
        export IMAGE_TAG=commit-$(git rev-parse --short HEAD)
        
        # æ›´æ–°éƒ¨ç½²é…ç½®ä¸­çš„é•œåƒæ ‡ç­¾
        sed -i "s|image: .*|image: ${{ env.DOCKER_REGISTRY }}/${{ env.IMAGE_NAME }}:${IMAGE_TAG}|" k8s/deployment.yaml
        
        # åº”ç”¨Kubernetesé…ç½®
        kubectl apply -f k8s/namespace.yaml
        kubectl apply -f k8s/configmap.yaml
        kubectl apply -f k8s/secrets.yaml
        kubectl apply -f k8s/deployment.yaml
        kubectl apply -f k8s/service.yaml
        kubectl apply -f k8s/ingress.yaml
        
        # ç­‰å¾…éƒ¨ç½²å®Œæˆ
        kubectl rollout status deployment/zhilink-platform -n zhilink
        
    - name: Run deployment tests
      run: |
        export KUBECONFIG=kubeconfig
        
        # ç­‰å¾…Podå°±ç»ª
        kubectl wait --for=condition=ready pod -l app=zhilink-platform -n zhilink --timeout=300s
        
        # è¿è¡Œéƒ¨ç½²åæµ‹è¯•
        kubectl run test-pod --image=curlimages/curl --rm -i --restart=Never -n zhilink -- \
          curl -f http://zhilink-platform/health || exit 1
    
    - name: Notify deployment status
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
      if: always()
```

---

**æ–‡æ¡£ç»´æŠ¤**: æŠ€æœ¯æ¶æ„å›¢é˜Ÿ  
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ12æ—¥  
**ç‰ˆæœ¬æ§åˆ¶**: v2.0.0 - ç¬¬4è½®æŠ€æœ¯æ¶æ„å®æ–½ç»†èŠ‚ä¼˜åŒ–ç‰ˆ  
**æ ¸å¿ƒä»·å€¼**: åŸºäº2025å¹´æœ€ä½³å®è·µæ„å»ºå¾®æœåŠ¡æ¶æ„ã€å®¹å™¨åŒ–éƒ¨ç½²ã€è‡ªåŠ¨åŒ–è¿ç»´çš„ç°ä»£åŒ–æŠ€æœ¯æ ˆï¼Œç¡®ä¿æ™ºé“¾å¹³å°å…·å¤‡é«˜å¯æ‰©å±•æ€§ã€é«˜å¯ç”¨æ€§å’Œé«˜æ€§èƒ½