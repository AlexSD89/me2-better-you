# ç¬¬3è½®ç»†èŠ‚ä¼˜åŒ–ï¼šæ™ºé“¾å¹³å°v2æ•°æ®æ¸…æ´—ã€æ ‡æ³¨ä¸å¤„ç†æµç¨‹

**ç‰ˆæœ¬**: v2.0 | **æ—¥æœŸ**: 2025-08-12 | **çŠ¶æ€**: ç¬¬3è½®ç»†èŠ‚ä¼˜åŒ–  
**ç›®æ ‡**: æ„å»ºè‡ªåŠ¨åŒ–ã€é«˜è´¨é‡çš„æ•°æ®å¤„ç†æµæ°´çº¿ï¼Œç¡®ä¿AIæ¨èç³»ç»Ÿçš„æ•°æ®åŸºç¡€æ‰å®å¯é 

---

## ğŸ§¹ æ•°æ®æ¸…æ´—è‡ªåŠ¨åŒ–æµæ°´çº¿

### æ ¸å¿ƒæ•°æ®æ¸…æ´—æ¡†æ¶
```python
import pandas as pd
import numpy as np
import re
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime
import json

class AutoDataCleaningPipeline:
    """
    æ™ºé“¾å¹³å°è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—æµæ°´çº¿
    åŸºäº2025å¹´æœ€ä½³å®è·µçš„ä¼ä¸šçº§æ•°æ®å¤„ç†ç³»ç»Ÿ
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.cleaning_rules = self._load_cleaning_rules()
        self.validation_schemas = self._load_validation_schemas()
        self.processed_stats = {}
        
    def _load_cleaning_rules(self) -> Dict:
        """åŠ è½½æ•°æ®æ¸…æ´—è§„åˆ™é…ç½®"""
        return {
            'text_cleaning': {
                'remove_html': True,
                'remove_special_chars': True,
                'normalize_whitespace': True,
                'remove_emails': True,
                'remove_phones': True,
                'convert_traditional_chinese': True
            },
            'numeric_cleaning': {
                'outlier_method': 'iqr',  # iqr, zscore, isolation_forest
                'outlier_threshold': 3.0,
                'handle_negatives': 'keep',  # keep, remove, abs
                'round_decimals': 2
            },
            'categorical_cleaning': {
                'standardize_case': 'lower',
                'remove_leading_trailing_spaces': True,
                'map_synonyms': True,
                'handle_rare_categories': True,
                'rare_threshold': 0.01
            }
        }
    
    def process_user_requirements_data(self, raw_df: pd.DataFrame) -> pd.DataFrame:
        """
        ç”¨æˆ·éœ€æ±‚æ•°æ®æ¸…æ´—ä¸»æµç¨‹
        """
        print(f"å¼€å§‹å¤„ç†ç”¨æˆ·éœ€æ±‚æ•°æ®ï¼ŒåŸå§‹æ•°æ®: {len(raw_df)} æ¡")
        
        cleaned_df = raw_df.copy()
        cleaning_stats = {'original_count': len(raw_df)}
        
        # 1. åŸºç¡€æ•°æ®è´¨é‡æ£€æŸ¥
        cleaned_df, quality_stats = self._basic_quality_check(cleaned_df)
        cleaning_stats.update(quality_stats)
        
        # 2. æ–‡æœ¬å­—æ®µæ¸…æ´—
        text_columns = ['requirement_description', 'business_context', 'technical_requirements']
        for col in text_columns:
            if col in cleaned_df.columns:
                cleaned_df[col] = cleaned_df[col].apply(self._clean_text_content)
        
        # 3. æ•°å€¼å­—æ®µæ¸…æ´—
        numeric_columns = ['budget', 'timeline_days', 'team_size', 'expected_roi']
        for col in numeric_columns:
            if col in cleaned_df.columns:
                cleaned_df[col] = self._clean_numeric_data(cleaned_df[col])
        
        # 4. åˆ†ç±»å­—æ®µæ¸…æ´—å’Œæ ‡å‡†åŒ–
        categorical_columns = ['industry', 'company_size', 'urgency_level', 'complexity_level']
        for col in categorical_columns:
            if col in cleaned_df.columns:
                cleaned_df[col] = self._clean_categorical_data(cleaned_df[col])
        
        # 5. æ•°æ®å®Œæ•´æ€§éªŒè¯
        cleaned_df = self._validate_data_completeness(cleaned_df)
        
        # 6. å¼‚å¸¸å€¼å¤„ç†
        cleaned_df = self._handle_outliers(cleaned_df)
        
        # 7. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        cleaned_df = self._ensure_data_consistency(cleaned_df)
        
        cleaning_stats['final_count'] = len(cleaned_df)
        cleaning_stats['data_loss_rate'] = (1 - cleaning_stats['final_count'] / cleaning_stats['original_count']) * 100
        
        self.processed_stats['user_requirements'] = cleaning_stats
        print(f"ç”¨æˆ·éœ€æ±‚æ•°æ®æ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆæ•°æ®: {len(cleaned_df)} æ¡ï¼Œæ•°æ®æŸå¤±ç‡: {cleaning_stats['data_loss_rate']:.2f}%")
        
        return cleaned_df
    
    def _clean_text_content(self, text: str) -> str:
        """
        æ·±åº¦æ–‡æœ¬å†…å®¹æ¸…æ´—
        """
        if pd.isna(text) or not isinstance(text, str):
            return ""
        
        original_text = text
        
        # 1. ç§»é™¤HTMLæ ‡ç­¾
        if self.cleaning_rules['text_cleaning']['remove_html']:
            text = re.sub(r'<[^>]+>', '', text)
        
        # 2. ç§»é™¤é‚®ç®±åœ°å€ï¼ˆéšç§ä¿æŠ¤ï¼‰
        if self.cleaning_rules['text_cleaning']['remove_emails']:
            text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[é‚®ç®±å·²éšè—]', text)
        
        # 3. ç§»é™¤æ‰‹æœºå·ç ï¼ˆéšç§ä¿æŠ¤ï¼‰
        if self.cleaning_rules['text_cleaning']['remove_phones']:
            text = re.sub(r'1[3-9]\d{9}', '[æ‰‹æœºå·å·²éšè—]', text)
            text = re.sub(r'\d{3}-\d{4}-\d{4}', '[ç”µè¯å·²éšè—]', text)
        
        # 4. ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€å¸¸ç”¨æ ‡ç‚¹ç¬¦å·
        if self.cleaning_rules['text_cleaning']['remove_special_chars']:
            text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘\-\.]', ' ', text)
        
        # 5. è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        if self.cleaning_rules['text_cleaning']['normalize_whitespace']:
            text = ' '.join(text.split())
        
        # 6. ç¹ä½“è½¬ç®€ä½“ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.cleaning_rules['text_cleaning']['convert_traditional_chinese']:
            text = self._convert_traditional_to_simplified(text)
        
        # 7. é•¿åº¦æ£€æŸ¥ï¼ˆé¿å…è¿‡çŸ­æˆ–è¿‡é•¿çš„æ–‡æœ¬ï¼‰
        if len(text.strip()) < 5:
            return ""  # è¿‡çŸ­çš„æ–‡æœ¬å¯èƒ½æ²¡æœ‰æ„ä¹‰
        if len(text) > 10000:
            text = text[:10000] + "..."  # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
        
        return text.strip()
    
    def _clean_numeric_data(self, series: pd.Series) -> pd.Series:
        """
        æ•°å€¼æ•°æ®æ¸…æ´—
        """
        cleaned_series = series.copy()
        
        # 1. å¤„ç†å­—ç¬¦ä¸²å½¢å¼çš„æ•°å­—
        cleaned_series = pd.to_numeric(cleaned_series, errors='coerce')
        
        # 2. å¤„ç†å¼‚å¸¸å€¼
        if self.cleaning_rules['numeric_cleaning']['outlier_method'] == 'iqr':
            Q1 = cleaned_series.quantile(0.25)
            Q3 = cleaned_series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # å°†å¼‚å¸¸å€¼è®¾ä¸ºNaNï¼Œåç»­ç”¨æ’å€¼å¤„ç†
            cleaned_series = cleaned_series.where(
                (cleaned_series >= lower_bound) & (cleaned_series <= upper_bound)
            )
        
        # 3. å¤„ç†è´Ÿå€¼ï¼ˆæ ¹æ®ä¸šåŠ¡é€»è¾‘ï¼‰
        if series.name in ['budget', 'timeline_days', 'team_size']:
            # é¢„ç®—ã€æ—¶é—´ã€å›¢é˜Ÿè§„æ¨¡ä¸åº”ä¸ºè´Ÿ
            cleaned_series = cleaned_series.where(cleaned_series > 0)
        
        # 4. å››èˆäº”å…¥
        if not pd.api.types.is_integer_dtype(cleaned_series):
            cleaned_series = cleaned_series.round(self.cleaning_rules['numeric_cleaning']['round_decimals'])
        
        return cleaned_series
    
    def _clean_categorical_data(self, series: pd.Series) -> pd.Series:
        """
        åˆ†ç±»æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
        """
        cleaned_series = series.copy()
        
        # 1. è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å¤„ç†ç©ºå€¼
        cleaned_series = cleaned_series.astype(str)
        cleaned_series = cleaned_series.replace(['nan', 'None', 'null', ''], np.nan)
        
        # 2. å»é™¤é¦–å°¾ç©ºæ ¼
        if self.cleaning_rules['categorical_cleaning']['remove_leading_trailing_spaces']:
            cleaned_series = cleaned_series.str.strip()
        
        # 3. ç»Ÿä¸€å¤§å°å†™
        case_method = self.cleaning_rules['categorical_cleaning']['standardize_case']
        if case_method == 'lower':
            cleaned_series = cleaned_series.str.lower()
        elif case_method == 'upper':
            cleaned_series = cleaned_series.str.upper()
        
        # 4. åŒä¹‰è¯æ˜ å°„
        if self.cleaning_rules['categorical_cleaning']['map_synonyms']:
            synonym_mappings = self._get_synonym_mappings(series.name)
            if synonym_mappings:
                cleaned_series = cleaned_series.replace(synonym_mappings)
        
        # 5. å¤„ç†ç¨€æœ‰ç±»åˆ«
        if self.cleaning_rules['categorical_cleaning']['handle_rare_categories']:
            value_counts = cleaned_series.value_counts(normalize=True)
            rare_threshold = self.cleaning_rules['categorical_cleaning']['rare_threshold']
            rare_values = value_counts[value_counts < rare_threshold].index
            cleaned_series = cleaned_series.replace(rare_values, 'other')
        
        return cleaned_series
    
    def _get_synonym_mappings(self, column_name: str) -> Dict:
        """è·å–åŒä¹‰è¯æ˜ å°„è¡¨"""
        mappings = {
            'industry': {
                'åˆ¶é€ ': 'manufacturing',
                'åˆ¶é€ ä¸š': 'manufacturing',
                'manufacturing': 'manufacturing',
                'ç”Ÿäº§': 'manufacturing',
                'å·¥å‚': 'manufacturing',
                
                'é‡‘è': 'finance',
                'é“¶è¡Œ': 'finance',
                'ä¿é™©': 'finance',
                'è¯åˆ¸': 'finance',
                'finance': 'finance',
                'banking': 'finance',
                
                'é›¶å”®': 'retail',
                'ç”µå•†': 'retail',
                'é”€å”®': 'retail',
                'retail': 'retail',
                'e-commerce': 'retail',
                'ecommerce': 'retail',
                
                'åŒ»ç–—': 'healthcare',
                'å¥åº·': 'healthcare',
                'åŒ»é™¢': 'healthcare',
                'healthcare': 'healthcare',
                'medical': 'healthcare',
                
                'æ•™è‚²': 'education',
                'åŸ¹è®­': 'education',
                'å­¦æ ¡': 'education',
                'education': 'education',
                'training': 'education',
            },
            'company_size': {
                'åˆåˆ›': 'startup',
                'åˆåˆ›å…¬å¸': 'startup',
                'startup': 'startup',
                'åˆ›ä¸š': 'startup',
                
                'å°å¾®': 'small',
                'å°å‹': 'small',
                'small': 'small',
                'å¾®å‹': 'small',
                
                'ä¸­å‹': 'medium',
                'ä¸­ç­‰': 'medium',
                'medium': 'medium',
                'ä¸­å¤§å‹': 'medium',
                
                'å¤§å‹': 'large',
                'large': 'large',
                'å·¨å¤´': 'large',
                'å¤§ä¼ä¸š': 'large',
            },
            'urgency_level': {
                'ç´§æ€¥': 'high',
                'æ€¥': 'high',
                'å¾ˆæ€¥': 'high',
                'urgent': 'high',
                'high': 'high',
                
                'æ™®é€š': 'medium',
                'ä¸­ç­‰': 'medium',
                'ä¸€èˆ¬': 'medium',
                'medium': 'medium',
                'normal': 'medium',
                
                'ä¸æ€¥': 'low',
                'æ…¢': 'low',
                'ä½': 'low',
                'low': 'low',
                'slow': 'low',
            }
        }
        
        return mappings.get(column_name, {})
    
    def _validate_data_completeness(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®å®Œæ•´æ€§éªŒè¯"""
        # å®šä¹‰å¿…é¡»å­—æ®µ
        required_fields = ['requirement_description', 'industry', 'budget']
        
        # æ£€æŸ¥å¿…é¡»å­—æ®µçš„å®Œæ•´æ€§
        for field in required_fields:
            if field in df.columns:
                before_count = len(df)
                df = df.dropna(subset=[field])
                after_count = len(df)
                if before_count != after_count:
                    print(f"å› {field}å­—æ®µç¼ºå¤±ï¼Œåˆ é™¤äº† {before_count - after_count} æ¡è®°å½•")
        
        # è®¡ç®—å„å­—æ®µçš„å®Œæ•´åº¦
        completeness_stats = {}
        for col in df.columns:
            completeness = (1 - df[col].isna().sum() / len(df)) * 100
            completeness_stats[col] = completeness
        
        self.processed_stats['completeness'] = completeness_stats
        
        return df
    
    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¼‚å¸¸å€¼å¤„ç†"""
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if col in ['budget', 'timeline_days', 'expected_roi']:
                # ä½¿ç”¨ä¸šåŠ¡é€»è¾‘å¤„ç†å¼‚å¸¸å€¼
                if col == 'budget':
                    # é¢„ç®—èŒƒå›´ï¼š1000 - 10,000,000
                    df[col] = df[col].clip(lower=1000, upper=10000000)
                elif col == 'timeline_days':
                    # æ—¶é—´èŒƒå›´ï¼š1 - 365å¤©
                    df[col] = df[col].clip(lower=1, upper=365)
                elif col == 'expected_roi':
                    # ROIèŒƒå›´ï¼š0.1 - 10å€
                    df[col] = df[col].clip(lower=0.1, upper=10.0)
        
        return df

# ä¾›åº”å•†èƒ½åŠ›æ•°æ®æ¸…æ´—
class SupplierCapabilityDataCleaner:
    """ä¾›åº”å•†AIèƒ½åŠ›æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        self.skill_standardizer = SkillStandardizer()
        self.quality_scorer = QualityScorer()
        
    def clean_supplier_data(self, raw_supplier_data: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ä¾›åº”å•†èƒ½åŠ›æ•°æ®"""
        cleaned_data = raw_supplier_data.copy()
        
        # 1. åŸºç¡€ä¿¡æ¯æ¸…æ´—
        cleaned_data = self._clean_basic_info(cleaned_data)
        
        # 2. æŠ€èƒ½æ ‡ç­¾æ ‡å‡†åŒ–
        if 'skills' in cleaned_data.columns:
            cleaned_data['skills'] = cleaned_data['skills'].apply(
                self.skill_standardizer.standardize_skills
            )
        
        # 3. ä»·æ ¼æ•°æ®æ¸…æ´—
        if 'pricing_info' in cleaned_data.columns:
            cleaned_data['pricing_info'] = cleaned_data['pricing_info'].apply(
                self._clean_pricing_data
            )
        
        # 4. æ¡ˆä¾‹æ•°æ®éªŒè¯
        if 'case_studies' in cleaned_data.columns:
            cleaned_data['case_studies'] = cleaned_data['case_studies'].apply(
                self._validate_case_studies
            )
        
        # 5. ä¾›åº”å•†è´¨é‡è¯„åˆ†
        cleaned_data['quality_score'] = cleaned_data.apply(
            self.quality_scorer.calculate_quality_score, axis=1
        )
        
        return cleaned_data
    
    def _clean_pricing_data(self, pricing_str: str) -> Dict:
        """æ¸…æ´—ä»·æ ¼æ•°æ®"""
        if pd.isna(pricing_str):
            return {}
            
        try:
            if isinstance(pricing_str, str):
                pricing_data = json.loads(pricing_str)
            else:
                pricing_data = pricing_str
            
            # ä»·æ ¼æ ¼å¼æ ‡å‡†åŒ–
            cleaned_pricing = {}
            
            # åŸºç¡€ä»·æ ¼
            if 'base_price' in pricing_data:
                base_price = float(re.sub(r'[^\d.]', '', str(pricing_data['base_price'])))
                cleaned_pricing['base_price'] = max(base_price, 0)  # ç¡®ä¿éè´Ÿ
            
            # æŒ‰é‡è®¡è´¹
            if 'per_unit_price' in pricing_data:
                unit_price = float(re.sub(r'[^\d.]', '', str(pricing_data['per_unit_price'])))
                cleaned_pricing['per_unit_price'] = max(unit_price, 0)
            
            # åˆ†æˆæ¯”ä¾‹
            if 'commission_rate' in pricing_data:
                commission = float(re.sub(r'[^\d.]', '', str(pricing_data['commission_rate'])))
                cleaned_pricing['commission_rate'] = max(0, min(commission, 100))  # 0-100%èŒƒå›´
            
            return cleaned_pricing
            
        except (json.JSONDecodeError, ValueError, TypeError):
            return {}

class SkillStandardizer:
    """æŠ€èƒ½æ ‡ç­¾æ ‡å‡†åŒ–å™¨"""
    
    def __init__(self):
        self.skill_mappings = self._load_skill_mappings()
        
    def _load_skill_mappings(self) -> Dict:
        """åŠ è½½æŠ€èƒ½æ ‡ç­¾æ˜ å°„è¡¨"""
        return {
            # AI/MLç›¸å…³
            'machine learning': 'ml',
            'æœºå™¨å­¦ä¹ ': 'ml',
            'deep learning': 'dl',
            'æ·±åº¦å­¦ä¹ ': 'dl',
            'artificial intelligence': 'ai',
            'äººå·¥æ™ºèƒ½': 'ai',
            'natural language processing': 'nlp',
            'è‡ªç„¶è¯­è¨€å¤„ç†': 'nlp',
            'computer vision': 'cv',
            'è®¡ç®—æœºè§†è§‰': 'cv',
            'reinforcement learning': 'rl',
            'å¼ºåŒ–å­¦ä¹ ': 'rl',
            
            # ç¼–ç¨‹è¯­è¨€
            'python': 'python',
            'java': 'java',
            'javascript': 'javascript',
            'typescript': 'typescript',
            'go': 'golang',
            'golang': 'golang',
            'c++': 'cpp',
            'c#': 'csharp',
            'r': 'r',
            'scala': 'scala',
            
            # æ¡†æ¶å’Œå·¥å…·
            'tensorflow': 'tensorflow',
            'pytorch': 'pytorch',
            'keras': 'keras',
            'scikit-learn': 'sklearn',
            'pandas': 'pandas',
            'numpy': 'numpy',
            'opencv': 'opencv',
            'huggingface': 'huggingface',
            'transformers': 'transformers',
            
            # äº‘å¹³å°
            'aws': 'aws',
            'azure': 'azure',
            'google cloud': 'gcp',
            'gcp': 'gcp',
            'é˜¿é‡Œäº‘': 'aliyun',
            'è…¾è®¯äº‘': 'tencent_cloud',
            
            # æ•°æ®åº“
            'mysql': 'mysql',
            'postgresql': 'postgresql',
            'mongodb': 'mongodb',
            'redis': 'redis',
            'elasticsearch': 'elasticsearch',
        }
    
    def standardize_skills(self, skills_input) -> List[str]:
        """æ ‡å‡†åŒ–æŠ€èƒ½æ ‡ç­¾åˆ—è¡¨"""
        if pd.isna(skills_input):
            return []
        
        # å¤„ç†ä¸åŒè¾“å…¥æ ¼å¼
        if isinstance(skills_input, str):
            if skills_input.startswith('[') and skills_input.endswith(']'):
                try:
                    skills_list = json.loads(skills_input)
                except json.JSONDecodeError:
                    skills_list = skills_input.strip('[]').split(',')
            else:
                skills_list = [s.strip() for s in skills_input.split(',')]
        elif isinstance(skills_input, list):
            skills_list = skills_input
        else:
            return []
        
        # æ ‡å‡†åŒ–æ¯ä¸ªæŠ€èƒ½
        standardized_skills = []
        for skill in skills_list:
            if not skill or pd.isna(skill):
                continue
                
            skill_clean = skill.lower().strip()
            
            # æŸ¥æ‰¾æ˜ å°„
            standardized_skill = self.skill_mappings.get(skill_clean, skill_clean)
            
            # å»é‡æ·»åŠ 
            if standardized_skill not in standardized_skills:
                standardized_skills.append(standardized_skill)
        
        return standardized_skills
```

---

## ğŸ·ï¸ æ™ºèƒ½æ•°æ®æ ‡æ³¨ç³»ç»Ÿ

### è‡ªåŠ¨åŒ–æ ‡æ³¨æµæ°´çº¿
```python
class AutoDataAnnotationSystem:
    """
    æ™ºèƒ½æ•°æ®æ ‡æ³¨ç³»ç»Ÿ
    ç»“åˆäººå·¥æ ‡æ³¨å’ŒAIè¾…åŠ©æ ‡æ³¨ï¼Œæä¾›é«˜è´¨é‡æ•°æ®æ ‡æ³¨æœåŠ¡
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.pre_labeling_models = self._load_pre_labeling_models()
        self.quality_checkers = self._initialize_quality_checkers()
        self.annotation_stats = {}
        
    def _load_pre_labeling_models(self) -> Dict:
        """åŠ è½½AIé¢„æ ‡æ³¨æ¨¡å‹"""
        return {
            'intent_classifier': self._load_intent_classification_model(),
            'entity_extractor': self._load_entity_extraction_model(),
            'sentiment_analyzer': self._load_sentiment_analysis_model(),
            'complexity_scorer': self._load_complexity_scoring_model()
        }
    
    async def annotate_user_requirements(self, requirements_data: pd.DataFrame) -> pd.DataFrame:
        """
        ç”¨æˆ·éœ€æ±‚æ•°æ®æ‰¹é‡æ ‡æ³¨
        """
        annotated_data = requirements_data.copy()
        
        # 1. AIé¢„æ ‡æ³¨
        print("å¼€å§‹AIé¢„æ ‡æ³¨...")
        annotated_data = await self._ai_pre_labeling(annotated_data)
        
        # 2. äººå·¥å®¡æ ¸å’Œä¿®æ­£
        print("å¼€å§‹äººå·¥å®¡æ ¸...")
        annotated_data = await self._human_review_process(annotated_data)
        
        # 3. è´¨é‡æ§åˆ¶æ£€æŸ¥
        print("å¼€å§‹è´¨é‡æ£€æŸ¥...")
        quality_metrics = self._quality_control_check(annotated_data)
        
        # 4. ä¸€è‡´æ€§éªŒè¯
        print("å¼€å§‹ä¸€è‡´æ€§éªŒè¯...")
        annotated_data = self._consistency_validation(annotated_data)
        
        # 5. æ ‡æ³¨ç»Ÿè®¡
        self._generate_annotation_stats(annotated_data, quality_metrics)
        
        return annotated_data
    
    async def _ai_pre_labeling(self, data: pd.DataFrame) -> pd.DataFrame:
        """AIé¢„æ ‡æ³¨è¿‡ç¨‹"""
        pre_labeled_data = data.copy()
        
        # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªé¢„æ ‡æ³¨ä»»åŠ¡
        tasks = []
        
        # æ„å›¾åˆ†ç±»
        if 'requirement_description' in data.columns:
            tasks.append(self._classify_intents(data['requirement_description']))
        
        # å®ä½“æŠ½å–
        if 'requirement_description' in data.columns:
            tasks.append(self._extract_entities(data['requirement_description']))
        
        # æƒ…æ„Ÿåˆ†æ
        if 'requirement_description' in data.columns:
            tasks.append(self._analyze_sentiment(data['requirement_description']))
        
        # å¤æ‚åº¦è¯„åˆ†
        tasks.append(self._score_complexity(data))
        
        # æ‰§è¡Œæ‰€æœ‰æ ‡æ³¨ä»»åŠ¡
        results = await asyncio.gather(*tasks)
        
        # æ•´åˆæ ‡æ³¨ç»“æœ
        if len(results) >= 4:
            pre_labeled_data['ai_predicted_intent'] = results[0]
            pre_labeled_data['ai_extracted_entities'] = results[1]
            pre_labeled_data['ai_sentiment_score'] = results[2]
            pre_labeled_data['ai_complexity_score'] = results[3]
        
        return pre_labeled_data
    
    async def _classify_intents(self, descriptions: pd.Series) -> List[str]:
        """æ„å›¾åˆ†ç±»é¢„æ ‡æ³¨"""
        intent_predictions = []
        
        # å®šä¹‰æ„å›¾å…³é”®è¯æ¨¡å¼
        intent_patterns = {
            'development': [
                'å¼€å‘', 'æ„å»º', 'æ­å»º', 'å»ºè®¾', 'åˆ›å»º',
                'develop', 'build', 'create', 'construct'
            ],
            'consultation': [
                'å’¨è¯¢', 'äº†è§£', 'è¯¢é—®', 'è¯·æ•™', 'è®¨è®º',
                'consult', 'advice', 'discuss', 'understand'
            ],
            'integration': [
                'é›†æˆ', 'å¯¹æ¥', 'èåˆ', 'è¿æ¥', 'æ•´åˆ',
                'integrate', 'connect', 'merge', 'combine'
            ],
            'optimization': [
                'ä¼˜åŒ–', 'æ”¹è¿›', 'æå‡', 'å¢å¼º', 'å®Œå–„',
                'optimize', 'improve', 'enhance', 'upgrade'
            ],
            'automation': [
                'è‡ªåŠ¨åŒ–', 'æ™ºèƒ½åŒ–', 'æ— äººåŒ–', 'è‡ªåŠ©',
                'automate', 'intelligent', 'autonomous'
            ]
        }
        
        for desc in descriptions:
            if pd.isna(desc):
                intent_predictions.append('unknown')
                continue
            
            desc_lower = desc.lower()
            intent_scores = {}
            
            # è®¡ç®—æ¯ä¸ªæ„å›¾çš„åŒ¹é…åˆ†æ•°
            for intent, keywords in intent_patterns.items():
                score = sum(1 for keyword in keywords if keyword in desc_lower)
                intent_scores[intent] = score
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ„å›¾
            if max(intent_scores.values()) > 0:
                predicted_intent = max(intent_scores, key=intent_scores.get)
            else:
                predicted_intent = 'unknown'
            
            intent_predictions.append(predicted_intent)
        
        return intent_predictions
    
    async def _extract_entities(self, descriptions: pd.Series) -> List[Dict]:
        """å®ä½“æŠ½å–é¢„æ ‡æ³¨"""
        entity_predictions = []
        
        # å®šä¹‰å®ä½“æ¨¡å¼
        entity_patterns = {
            'technology': {
                'ai_ml': ['AI', 'äººå·¥æ™ºèƒ½', 'ML', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'DL'],
                'nlp': ['NLP', 'è‡ªç„¶è¯­è¨€', 'æ–‡æœ¬åˆ†æ', 'è¯­éŸ³è¯†åˆ«', 'èŠå¤©æœºå™¨äºº'],
                'cv': ['è®¡ç®—æœºè§†è§‰', 'CV', 'å›¾åƒè¯†åˆ«', 'äººè„¸è¯†åˆ«', 'ç‰©ä½“æ£€æµ‹'],
                'big_data': ['å¤§æ•°æ®', 'æ•°æ®åˆ†æ', 'æ•°æ®æŒ–æ˜', 'å•†ä¸šæ™ºèƒ½', 'BI']
            },
            'industry': {
                'finance': ['é‡‘è', 'é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸', 'æ”¯ä»˜', 'é£æ§'],
                'manufacturing': ['åˆ¶é€ ', 'å·¥å‚', 'ç”Ÿäº§', 'ä¾›åº”é“¾', 'è´¨é‡æ§åˆ¶'],
                'retail': ['é›¶å”®', 'ç”µå•†', 'é”€å”®', 'å®¢æˆ·æœåŠ¡', 'æ¨èç³»ç»Ÿ'],
                'healthcare': ['åŒ»ç–—', 'å¥åº·', 'è¯Šæ–­', 'è¯ç‰©', 'åŒ»é™¢', 'æ‚£è€…'],
                'education': ['æ•™è‚²', 'åŸ¹è®­', 'å­¦ä¹ ', 'è¯¾ç¨‹', 'å­¦ç”Ÿ', 'æ•™å¸ˆ']
            },
            'business_function': {
                'customer_service': ['å®¢æœ', 'å®¢æˆ·æœåŠ¡', 'åœ¨çº¿å’¨è¯¢', 'é—®ç­”ç³»ç»Ÿ'],
                'marketing': ['è¥é”€', 'æ¨å¹¿', 'å¹¿å‘Š', 'ç”¨æˆ·ç”»åƒ', 'ç²¾å‡†è¥é”€'],
                'operations': ['è¿è¥', 'æµç¨‹', 'è‡ªåŠ¨åŒ–', 'æ•ˆç‡', 'ç®¡ç†'],
                'analytics': ['åˆ†æ', 'æŠ¥å‘Š', 'é¢„æµ‹', 'ç›‘æ§', 'ç»Ÿè®¡']
            }
        }
        
        for desc in descriptions:
            extracted_entities = {
                'technology': [],
                'industry': [],
                'business_function': []
            }
            
            if pd.isna(desc):
                entity_predictions.append(extracted_entities)
                continue
            
            desc_lower = desc.lower()
            
            # åœ¨æ¯ä¸ªç±»åˆ«ä¸­æŸ¥æ‰¾åŒ¹é…çš„å®ä½“
            for entity_type, subcategories in entity_patterns.items():
                for subcat, keywords in subcategories.items():
                    for keyword in keywords:
                        if keyword.lower() in desc_lower:
                            if subcat not in extracted_entities[entity_type]:
                                extracted_entities[entity_type].append(subcat)
            
            entity_predictions.append(extracted_entities)
        
        return entity_predictions
    
    async def _score_complexity(self, data: pd.DataFrame) -> List[float]:
        """å¤æ‚åº¦è¯„åˆ†é¢„æ ‡æ³¨"""
        complexity_scores = []
        
        for _, row in data.iterrows():
            score = 0.0
            
            # åŸºäºéœ€æ±‚æè¿°é•¿åº¦
            if 'requirement_description' in row:
                desc_length = len(str(row['requirement_description'])) if pd.notna(row['requirement_description']) else 0
                length_score = min(desc_length / 1000, 1.0) * 0.3
                score += length_score
            
            # åŸºäºé¢„ç®—è§„æ¨¡
            if 'budget' in row and pd.notna(row['budget']):
                budget = float(row['budget'])
                if budget > 1000000:  # 100ä¸‡ä»¥ä¸Š
                    budget_score = 0.8
                elif budget > 100000:  # 10ä¸‡ä»¥ä¸Š
                    budget_score = 0.6
                elif budget > 10000:   # 1ä¸‡ä»¥ä¸Š
                    budget_score = 0.4
                else:
                    budget_score = 0.2
                score += budget_score * 0.3
            
            # åŸºäºæ—¶é—´è¦æ±‚
            if 'timeline_days' in row and pd.notna(row['timeline_days']):
                timeline = float(row['timeline_days'])
                if timeline < 30:      # 1ä¸ªæœˆå†…
                    timeline_score = 0.8
                elif timeline < 90:    # 3ä¸ªæœˆå†…
                    timeline_score = 0.6
                elif timeline < 180:   # 6ä¸ªæœˆå†…
                    timeline_score = 0.4
                else:
                    timeline_score = 0.2
                score += timeline_score * 0.2
            
            # åŸºäºå›¢é˜Ÿè§„æ¨¡è¦æ±‚
            if 'team_size' in row and pd.notna(row['team_size']):
                team_size = float(row['team_size'])
                if team_size > 10:
                    team_score = 0.8
                elif team_size > 5:
                    team_score = 0.6
                elif team_size > 2:
                    team_score = 0.4
                else:
                    team_score = 0.2
                score += team_score * 0.2
            
            # æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
            final_score = min(max(score, 0.0), 1.0)
            complexity_scores.append(final_score)
        
        return complexity_scores
    
    async def _human_review_process(self, pre_labeled_data: pd.DataFrame) -> pd.DataFrame:
        """
        äººå·¥å®¡æ ¸æµç¨‹
        """
        reviewed_data = pre_labeled_data.copy()
        
        # 1. ç½®ä¿¡åº¦æ£€æŸ¥ - ä½ç½®ä¿¡åº¦æ ·æœ¬éœ€è¦äººå·¥å®¡æ ¸
        low_confidence_mask = self._identify_low_confidence_samples(reviewed_data)
        
        # 2. éšæœºæŠ½æ ·å®¡æ ¸ - ä¿è¯è´¨é‡
        random_sample_mask = self._random_sample_for_review(reviewed_data, sample_rate=0.1)
        
        # 3. éœ€è¦äººå·¥å®¡æ ¸çš„æ ·æœ¬
        need_review_mask = low_confidence_mask | random_sample_mask
        
        print(f"éœ€è¦äººå·¥å®¡æ ¸çš„æ ·æœ¬æ•°é‡: {need_review_mask.sum()}")
        
        # 4. æ¨¡æ‹Ÿäººå·¥å®¡æ ¸è¿‡ç¨‹ï¼ˆå®é™…é¡¹ç›®ä¸­è¿™é‡Œä¼šæ˜¯äººå·¥ç•Œé¢ï¼‰
        reviewed_data = self._simulate_human_review(reviewed_data, need_review_mask)
        
        return reviewed_data
    
    def _identify_low_confidence_samples(self, data: pd.DataFrame) -> pd.Series:
        """è¯†åˆ«ä½ç½®ä¿¡åº¦æ ·æœ¬"""
        low_confidence_mask = pd.Series([False] * len(data), index=data.index)
        
        # åŸºäºå¤æ‚åº¦åˆ†æ•°è¯†åˆ«è¾¹ç•Œæ ·æœ¬
        if 'ai_complexity_score' in data.columns:
            complexity_scores = data['ai_complexity_score']
            # å¤æ‚åº¦åœ¨0.4-0.6ä¹‹é—´çš„æ ·æœ¬è¢«è®¤ä¸ºæ˜¯è¾¹ç•Œæ ·æœ¬ï¼Œéœ€è¦äººå·¥å®¡æ ¸
            boundary_mask = (complexity_scores >= 0.4) & (complexity_scores <= 0.6)
            low_confidence_mask |= boundary_mask
        
        # åŸºäºæ„å›¾åˆ†ç±»çš„ä¸ç¡®å®šæ€§
        if 'ai_predicted_intent' in data.columns:
            uncertain_intent_mask = data['ai_predicted_intent'] == 'unknown'
            low_confidence_mask |= uncertain_intent_mask
        
        return low_confidence_mask
    
    def _quality_control_check(self, annotated_data: pd.DataFrame) -> Dict:
        """è´¨é‡æ§åˆ¶æ£€æŸ¥"""
        quality_metrics = {}
        
        # 1. æ ‡æ³¨å®Œæ•´æ€§æ£€æŸ¥
        required_labels = ['ai_predicted_intent', 'ai_complexity_score']
        completeness_rates = {}
        
        for label in required_labels:
            if label in annotated_data.columns:
                non_null_rate = (1 - annotated_data[label].isna().sum() / len(annotated_data))
                completeness_rates[label] = non_null_rate
        
        quality_metrics['completeness_rates'] = completeness_rates
        
        # 2. æ ‡æ³¨åˆ†å¸ƒåˆç†æ€§æ£€æŸ¥
        if 'ai_predicted_intent' in annotated_data.columns:
            intent_distribution = annotated_data['ai_predicted_intent'].value_counts(normalize=True)
            quality_metrics['intent_distribution'] = intent_distribution.to_dict()
        
        # 3. å¤æ‚åº¦åˆ†æ•°åˆ†å¸ƒæ£€æŸ¥
        if 'ai_complexity_score' in annotated_data.columns:
            complexity_stats = annotated_data['ai_complexity_score'].describe()
            quality_metrics['complexity_distribution'] = complexity_stats.to_dict()
        
        return quality_metrics

# æ ‡æ³¨è´¨é‡æ§åˆ¶ç³»ç»Ÿ
class AnnotationQualityController:
    """æ ‡æ³¨è´¨é‡æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.quality_thresholds = {
            'min_agreement_rate': 0.85,  # æ ‡æ³¨å‘˜é—´ä¸€è‡´æ€§æœ€ä½85%
            'max_error_rate': 0.1,       # é”™è¯¯ç‡æœ€é«˜10%
            'min_completeness': 0.95      # å®Œæ•´æ€§æœ€ä½95%
        }
        
    def multi_annotator_consistency_check(self, annotations: List[Dict]) -> Dict:
        """å¤šæ ‡æ³¨å‘˜ä¸€è‡´æ€§æ£€æŸ¥"""
        if len(annotations) < 2:
            return {'error': 'éœ€è¦è‡³å°‘2ä¸ªæ ‡æ³¨å‘˜çš„æ ‡æ³¨ç»“æœ'}
        
        consistency_metrics = {}
        
        # æ£€æŸ¥æ¯ä¸ªæ ‡æ³¨å­—æ®µçš„ä¸€è‡´æ€§
        for field in annotations[0].keys():
            field_values = [ann.get(field) for ann in annotations]
            
            # è®¡ç®—ä¸€è‡´æ€§ç‡
            if all(v is not None for v in field_values):
                unique_values = set(field_values)
                if len(unique_values) == 1:
                    agreement_rate = 1.0
                else:
                    # è®¡ç®—å¤šæ•°ä¸€è‡´æ€§
                    from collections import Counter
                    value_counts = Counter(field_values)
                    most_common_count = value_counts.most_common(1)[0][1]
                    agreement_rate = most_common_count / len(field_values)
            else:
                agreement_rate = 0.0
            
            consistency_metrics[field] = agreement_rate
        
        # æ•´ä½“ä¸€è‡´æ€§è¯„åˆ†
        overall_consistency = np.mean(list(consistency_metrics.values()))
        
        return {
            'field_consistency': consistency_metrics,
            'overall_consistency': overall_consistency,
            'quality_pass': overall_consistency >= self.quality_thresholds['min_agreement_rate']
        }
    
    def detect_annotation_errors(self, annotated_data: pd.DataFrame) -> Dict:
        """æ£€æµ‹æ ‡æ³¨é”™è¯¯"""
        error_report = {
            'missing_values': {},
            'invalid_formats': {},
            'logical_inconsistencies': []
        }
        
        # 1. æ£€æŸ¥ç¼ºå¤±å€¼
        for col in annotated_data.columns:
            missing_count = annotated_data[col].isna().sum()
            if missing_count > 0:
                error_report['missing_values'][col] = {
                    'count': int(missing_count),
                    'rate': float(missing_count / len(annotated_data))
                }
        
        # 2. æ£€æŸ¥æ ¼å¼é”™è¯¯
        if 'ai_complexity_score' in annotated_data.columns:
            # å¤æ‚åº¦åˆ†æ•°åº”è¯¥åœ¨0-1ä¹‹é—´
            invalid_complexity = annotated_data[
                (annotated_data['ai_complexity_score'] < 0) | 
                (annotated_data['ai_complexity_score'] > 1)
            ]
            if len(invalid_complexity) > 0:
                error_report['invalid_formats']['complexity_score'] = len(invalid_complexity)
        
        # 3. æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§
        if 'budget' in annotated_data.columns and 'ai_complexity_score' in annotated_data.columns:
            # é«˜é¢„ç®—é¡¹ç›®çš„å¤æ‚åº¦ä¸åº”è¯¥å¤ªä½
            high_budget_low_complexity = annotated_data[
                (annotated_data['budget'] > 500000) & 
                (annotated_data['ai_complexity_score'] < 0.3)
            ]
            if len(high_budget_low_complexity) > 0:
                error_report['logical_inconsistencies'].append({
                    'type': 'high_budget_low_complexity',
                    'count': len(high_budget_low_complexity),
                    'description': 'é«˜é¢„ç®—é¡¹ç›®å¤æ‚åº¦è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨æ ‡æ³¨é”™è¯¯'
                })
        
        return error_report
```

---

## ğŸ“ˆ æ•°æ®è´¨é‡ç›‘æ§ä¸åé¦ˆç³»ç»Ÿ

### å®æ—¶æ•°æ®è´¨é‡ç›‘æ§
```python
class DataQualityMonitor:
    """æ•°æ®è´¨é‡å®æ—¶ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.quality_metrics = {
            'completeness': [],
            'accuracy': [],
            'consistency': [],
            'timeliness': [],
            'validity': []
        }
        self.alert_thresholds = {
            'completeness': 0.95,
            'accuracy': 0.90,
            'consistency': 0.85,
            'timeliness': 0.90,
            'validity': 0.95
        }
        
    def real_time_quality_check(self, new_data: pd.DataFrame) -> Dict:
        """å®æ—¶æ•°æ®è´¨é‡æ£€æŸ¥"""
        quality_report = {}
        
        # 1. å®Œæ•´æ€§æ£€æŸ¥
        completeness_score = self._check_completeness(new_data)
        quality_report['completeness'] = completeness_score
        
        # 2. å‡†ç¡®æ€§æ£€æŸ¥
        accuracy_score = self._check_accuracy(new_data)
        quality_report['accuracy'] = accuracy_score
        
        # 3. ä¸€è‡´æ€§æ£€æŸ¥
        consistency_score = self._check_consistency(new_data)
        quality_report['consistency'] = consistency_score
        
        # 4. æ—¶æ•ˆæ€§æ£€æŸ¥
        timeliness_score = self._check_timeliness(new_data)
        quality_report['timeliness'] = timeliness_score
        
        # 5. æœ‰æ•ˆæ€§æ£€æŸ¥
        validity_score = self._check_validity(new_data)
        quality_report['validity'] = validity_score
        
        # 6. ç»¼åˆè´¨é‡åˆ†æ•°
        overall_quality = np.mean(list(quality_report.values()))
        quality_report['overall_quality'] = overall_quality
        
        # 7. å‘Šè­¦æ£€æŸ¥
        alerts = self._check_quality_alerts(quality_report)
        quality_report['alerts'] = alerts
        
        # 8. è®°å½•è´¨é‡æŒ‡æ ‡å†å²
        self._record_quality_metrics(quality_report)
        
        return quality_report
    
    def _check_completeness(self, data: pd.DataFrame) -> float:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        total_cells = data.shape[0] * data.shape[1]
        missing_cells = data.isna().sum().sum()
        completeness = (total_cells - missing_cells) / total_cells
        return float(completeness)
    
    def _check_accuracy(self, data: pd.DataFrame) -> float:
        """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§ï¼ˆåŸºäºä¸šåŠ¡è§„åˆ™ï¼‰"""
        accuracy_scores = []
        
        # é¢„ç®—å‡†ç¡®æ€§æ£€æŸ¥
        if 'budget' in data.columns:
            valid_budget = data['budget'].between(1000, 10000000, inclusive='both')
            budget_accuracy = valid_budget.sum() / len(data)
            accuracy_scores.append(budget_accuracy)
        
        # æ—¶é—´çº¿å‡†ç¡®æ€§æ£€æŸ¥
        if 'timeline_days' in data.columns:
            valid_timeline = data['timeline_days'].between(1, 365, inclusive='both')
            timeline_accuracy = valid_timeline.sum() / len(data)
            accuracy_scores.append(timeline_accuracy)
        
        # å›¢é˜Ÿè§„æ¨¡å‡†ç¡®æ€§æ£€æŸ¥
        if 'team_size' in data.columns:
            valid_team_size = data['team_size'].between(1, 100, inclusive='both')
            team_size_accuracy = valid_team_size.sum() / len(data)
            accuracy_scores.append(team_size_accuracy)
        
        return float(np.mean(accuracy_scores)) if accuracy_scores else 1.0
    
    def _check_consistency(self, data: pd.DataFrame) -> float:
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        consistency_scores = []
        
        # é¢„ç®—ä¸å¤æ‚åº¦ä¸€è‡´æ€§
        if 'budget' in data.columns and 'ai_complexity_score' in data.columns:
            high_budget = data['budget'] > 500000
            high_complexity = data['ai_complexity_score'] > 0.7
            consistency = (high_budget == high_complexity).sum() / len(data)
            consistency_scores.append(consistency)
        
        # æ—¶é—´çº¿ä¸ç´§æ€¥ç¨‹åº¦ä¸€è‡´æ€§
        if 'timeline_days' in data.columns and 'urgency_level' in data.columns:
            urgent_timeline = data['timeline_days'] < 30
            urgent_level = data['urgency_level'] == 'high'
            consistency = (urgent_timeline == urgent_level).sum() / len(data)
            consistency_scores.append(consistency)
        
        return float(np.mean(consistency_scores)) if consistency_scores else 1.0
    
    def _check_quality_alerts(self, quality_report: Dict) -> List[Dict]:
        """æ£€æŸ¥è´¨é‡å‘Šè­¦"""
        alerts = []
        
        for metric, score in quality_report.items():
            if metric in self.alert_thresholds:
                threshold = self.alert_thresholds[metric]
                if score < threshold:
                    alerts.append({
                        'metric': metric,
                        'current_score': score,
                        'threshold': threshold,
                        'severity': 'high' if score < threshold * 0.8 else 'medium',
                        'message': f'{metric}è´¨é‡åˆ†æ•°{score:.3f}ä½äºé˜ˆå€¼{threshold}'
                    })
        
        return alerts
    
    def generate_quality_dashboard(self) -> Dict:
        """ç”Ÿæˆæ•°æ®è´¨é‡ä»ªè¡¨æ¿æ•°æ®"""
        if not any(self.quality_metrics.values()):
            return {'error': 'æš‚æ— è´¨é‡ç›‘æ§æ•°æ®'}
        
        dashboard_data = {}
        
        for metric, values in self.quality_metrics.items():
            if values:
                dashboard_data[metric] = {
                    'current': values[-1],
                    'average': np.mean(values),
                    'trend': 'improving' if len(values) > 1 and values[-1] > values[-2] else 'declining',
                    'history': values[-30:]  # æœ€è¿‘30ä¸ªæ•°æ®ç‚¹
                }
        
        return dashboard_data

# æ•°æ®åé¦ˆå¾ªç¯ç³»ç»Ÿ
class DataFeedbackLoop:
    """æ•°æ®åé¦ˆå¾ªç¯ç³»ç»Ÿ"""
    
    def __init__(self):
        self.feedback_buffer = []
        self.improvement_suggestions = []
        
    def collect_user_feedback(self, user_id: str, data_item_id: str, 
                            feedback_type: str, feedback_content: Dict):
        """æ”¶é›†ç”¨æˆ·å¯¹æ•°æ®è´¨é‡çš„åé¦ˆ"""
        feedback_record = {
            'timestamp': datetime.now(),
            'user_id': user_id,
            'data_item_id': data_item_id,
            'feedback_type': feedback_type,  # 'quality_issue', 'suggestion', 'correction'
            'feedback_content': feedback_content
        }
        
        self.feedback_buffer.append(feedback_record)
        
        # å®æ—¶å¤„ç†ç´§æ€¥åé¦ˆ
        if feedback_type == 'quality_issue':
            self._handle_quality_issue(feedback_record)
    
    def _handle_quality_issue(self, feedback_record: Dict):
        """å¤„ç†æ•°æ®è´¨é‡é—®é¢˜"""
        issue_type = feedback_record['feedback_content'].get('issue_type')
        
        if issue_type == 'data_accuracy':
            # æ•°æ®å‡†ç¡®æ€§é—®é¢˜
            self._flag_data_for_review(feedback_record['data_item_id'])
        elif issue_type == 'missing_data':
            # æ•°æ®ç¼ºå¤±é—®é¢˜
            self._request_data_completion(feedback_record['data_item_id'])
        elif issue_type == 'annotation_error':
            # æ ‡æ³¨é”™è¯¯é—®é¢˜
            self._schedule_re_annotation(feedback_record['data_item_id'])
    
    def generate_improvement_plan(self) -> Dict:
        """åŸºäºåé¦ˆç”Ÿæˆæ”¹è¿›è®¡åˆ’"""
        if not self.feedback_buffer:
            return {'message': 'æš‚æ— åé¦ˆæ•°æ®ç”¨äºç”Ÿæˆæ”¹è¿›è®¡åˆ’'}
        
        # ç»Ÿè®¡åé¦ˆç±»å‹
        feedback_stats = {}
        for feedback in self.feedback_buffer:
            ftype = feedback['feedback_type']
            feedback_stats[ftype] = feedback_stats.get(ftype, 0) + 1
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        improvement_plan = {
            'priority_issues': [],
            'suggested_actions': [],
            'resource_requirements': {}
        }
        
        # åŸºäºåé¦ˆé¢‘ç‡ç¡®å®šä¼˜å…ˆçº§é—®é¢˜
        sorted_issues = sorted(feedback_stats.items(), key=lambda x: x[1], reverse=True)
        improvement_plan['priority_issues'] = sorted_issues[:3]
        
        # ç”Ÿæˆå…·ä½“è¡ŒåŠ¨å»ºè®®
        for issue_type, count in sorted_issues:
            if issue_type == 'quality_issue':
                improvement_plan['suggested_actions'].append({
                    'action': 'åŠ å¼ºæ•°æ®è´¨é‡æ£€æŸ¥æµç¨‹',
                    'description': f'é’ˆå¯¹{count}ä¸ªè´¨é‡é—®é¢˜ï¼Œå»ºè®®å¢åŠ è‡ªåŠ¨åŒ–æ£€æŸ¥è§„åˆ™',
                    'estimated_effort': 'medium'
                })
            elif issue_type == 'annotation_error':
                improvement_plan['suggested_actions'].append({
                    'action': 'ä¼˜åŒ–æ ‡æ³¨å‘˜åŸ¹è®­',
                    'description': f'é’ˆå¯¹{count}ä¸ªæ ‡æ³¨é”™è¯¯ï¼Œå»ºè®®å¢åŠ æ ‡æ³¨å‘˜åŸ¹è®­å’Œè´¨é‡æ£€æŸ¥',
                    'estimated_effort': 'high'
                })
        
        return improvement_plan
```

---

**æ–‡æ¡£ç»´æŠ¤**: æ•°æ®å·¥ç¨‹å›¢é˜Ÿ  
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ12æ—¥  
**ç‰ˆæœ¬æ§åˆ¶**: v2.0.0 - ç¬¬3è½®æ•°æ®æ¸…æ´—æ ‡æ³¨æµç¨‹ç»†èŠ‚ä¼˜åŒ–ç‰ˆ  
**æ ¸å¿ƒä»·å€¼**: æ„å»ºè‡ªåŠ¨åŒ–ã€é«˜è´¨é‡çš„æ•°æ®å¤„ç†å’Œæ ‡æ³¨æµæ°´çº¿ï¼Œç¡®ä¿AIæ¨èç³»ç»Ÿæ‹¥æœ‰æ¸…æ´ã€å‡†ç¡®ã€ä¸€è‡´çš„æ•°æ®åŸºç¡€ï¼Œæ”¯æŒé«˜ç²¾åº¦çš„æ™ºèƒ½æ¨èæœåŠ¡