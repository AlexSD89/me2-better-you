# 第3轮细节优化：智链平台v2数据清洗、标注与处理流程

**版本**: v2.0 | **日期**: 2025-08-12 | **状态**: 第3轮细节优化  
**目标**: 构建自动化、高质量的数据处理流水线，确保AI推荐系统的数据基础扎实可靠

---

## 🧹 数据清洗自动化流水线

### 核心数据清洗框架
```python
import pandas as pd
import numpy as np
import re
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from typing import Dict, List, Tuple, Optional
import logging
from datetime import datetime
import json

class AutoDataCleaningPipeline:
    """
    智链平台自动化数据清洗流水线
    基于2025年最佳实践的企业级数据处理系统
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.cleaning_rules = self._load_cleaning_rules()
        self.validation_schemas = self._load_validation_schemas()
        self.processed_stats = {}
        
    def _load_cleaning_rules(self) -> Dict:
        """加载数据清洗规则配置"""
        return {
            'text_cleaning': {
                'remove_html': True,
                'remove_special_chars': True,
                'normalize_whitespace': True,
                'remove_emails': True,
                'remove_phones': True,
                'convert_traditional_chinese': True
            },
            'numeric_cleaning': {
                'outlier_method': 'iqr',  # iqr, zscore, isolation_forest
                'outlier_threshold': 3.0,
                'handle_negatives': 'keep',  # keep, remove, abs
                'round_decimals': 2
            },
            'categorical_cleaning': {
                'standardize_case': 'lower',
                'remove_leading_trailing_spaces': True,
                'map_synonyms': True,
                'handle_rare_categories': True,
                'rare_threshold': 0.01
            }
        }
    
    def process_user_requirements_data(self, raw_df: pd.DataFrame) -> pd.DataFrame:
        """
        用户需求数据清洗主流程
        """
        print(f"开始处理用户需求数据，原始数据: {len(raw_df)} 条")
        
        cleaned_df = raw_df.copy()
        cleaning_stats = {'original_count': len(raw_df)}
        
        # 1. 基础数据质量检查
        cleaned_df, quality_stats = self._basic_quality_check(cleaned_df)
        cleaning_stats.update(quality_stats)
        
        # 2. 文本字段清洗
        text_columns = ['requirement_description', 'business_context', 'technical_requirements']
        for col in text_columns:
            if col in cleaned_df.columns:
                cleaned_df[col] = cleaned_df[col].apply(self._clean_text_content)
        
        # 3. 数值字段清洗
        numeric_columns = ['budget', 'timeline_days', 'team_size', 'expected_roi']
        for col in numeric_columns:
            if col in cleaned_df.columns:
                cleaned_df[col] = self._clean_numeric_data(cleaned_df[col])
        
        # 4. 分类字段清洗和标准化
        categorical_columns = ['industry', 'company_size', 'urgency_level', 'complexity_level']
        for col in categorical_columns:
            if col in cleaned_df.columns:
                cleaned_df[col] = self._clean_categorical_data(cleaned_df[col])
        
        # 5. 数据完整性验证
        cleaned_df = self._validate_data_completeness(cleaned_df)
        
        # 6. 异常值处理
        cleaned_df = self._handle_outliers(cleaned_df)
        
        # 7. 数据一致性检查
        cleaned_df = self._ensure_data_consistency(cleaned_df)
        
        cleaning_stats['final_count'] = len(cleaned_df)
        cleaning_stats['data_loss_rate'] = (1 - cleaning_stats['final_count'] / cleaning_stats['original_count']) * 100
        
        self.processed_stats['user_requirements'] = cleaning_stats
        print(f"用户需求数据清洗完成，最终数据: {len(cleaned_df)} 条，数据损失率: {cleaning_stats['data_loss_rate']:.2f}%")
        
        return cleaned_df
    
    def _clean_text_content(self, text: str) -> str:
        """
        深度文本内容清洗
        """
        if pd.isna(text) or not isinstance(text, str):
            return ""
        
        original_text = text
        
        # 1. 移除HTML标签
        if self.cleaning_rules['text_cleaning']['remove_html']:
            text = re.sub(r'<[^>]+>', '', text)
        
        # 2. 移除邮箱地址（隐私保护）
        if self.cleaning_rules['text_cleaning']['remove_emails']:
            text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[邮箱已隐藏]', text)
        
        # 3. 移除手机号码（隐私保护）
        if self.cleaning_rules['text_cleaning']['remove_phones']:
            text = re.sub(r'1[3-9]\d{9}', '[手机号已隐藏]', text)
            text = re.sub(r'\d{3}-\d{4}-\d{4}', '[电话已隐藏]', text)
        
        # 4. 保留中文、英文、数字、常用标点符号
        if self.cleaning_rules['text_cleaning']['remove_special_chars']:
            text = re.sub(r'[^\w\s\u4e00-\u9fff，。！？；：""''（）【】\-\.]', ' ', text)
        
        # 5. 规范化空白字符
        if self.cleaning_rules['text_cleaning']['normalize_whitespace']:
            text = ' '.join(text.split())
        
        # 6. 繁体转简体（如果需要）
        if self.cleaning_rules['text_cleaning']['convert_traditional_chinese']:
            text = self._convert_traditional_to_simplified(text)
        
        # 7. 长度检查（避免过短或过长的文本）
        if len(text.strip()) < 5:
            return ""  # 过短的文本可能没有意义
        if len(text) > 10000:
            text = text[:10000] + "..."  # 截断过长的文本
        
        return text.strip()
    
    def _clean_numeric_data(self, series: pd.Series) -> pd.Series:
        """
        数值数据清洗
        """
        cleaned_series = series.copy()
        
        # 1. 处理字符串形式的数字
        cleaned_series = pd.to_numeric(cleaned_series, errors='coerce')
        
        # 2. 处理异常值
        if self.cleaning_rules['numeric_cleaning']['outlier_method'] == 'iqr':
            Q1 = cleaned_series.quantile(0.25)
            Q3 = cleaned_series.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # 将异常值设为NaN，后续用插值处理
            cleaned_series = cleaned_series.where(
                (cleaned_series >= lower_bound) & (cleaned_series <= upper_bound)
            )
        
        # 3. 处理负值（根据业务逻辑）
        if series.name in ['budget', 'timeline_days', 'team_size']:
            # 预算、时间、团队规模不应为负
            cleaned_series = cleaned_series.where(cleaned_series > 0)
        
        # 4. 四舍五入
        if not pd.api.types.is_integer_dtype(cleaned_series):
            cleaned_series = cleaned_series.round(self.cleaning_rules['numeric_cleaning']['round_decimals'])
        
        return cleaned_series
    
    def _clean_categorical_data(self, series: pd.Series) -> pd.Series:
        """
        分类数据清洗和标准化
        """
        cleaned_series = series.copy()
        
        # 1. 转换为字符串并处理空值
        cleaned_series = cleaned_series.astype(str)
        cleaned_series = cleaned_series.replace(['nan', 'None', 'null', ''], np.nan)
        
        # 2. 去除首尾空格
        if self.cleaning_rules['categorical_cleaning']['remove_leading_trailing_spaces']:
            cleaned_series = cleaned_series.str.strip()
        
        # 3. 统一大小写
        case_method = self.cleaning_rules['categorical_cleaning']['standardize_case']
        if case_method == 'lower':
            cleaned_series = cleaned_series.str.lower()
        elif case_method == 'upper':
            cleaned_series = cleaned_series.str.upper()
        
        # 4. 同义词映射
        if self.cleaning_rules['categorical_cleaning']['map_synonyms']:
            synonym_mappings = self._get_synonym_mappings(series.name)
            if synonym_mappings:
                cleaned_series = cleaned_series.replace(synonym_mappings)
        
        # 5. 处理稀有类别
        if self.cleaning_rules['categorical_cleaning']['handle_rare_categories']:
            value_counts = cleaned_series.value_counts(normalize=True)
            rare_threshold = self.cleaning_rules['categorical_cleaning']['rare_threshold']
            rare_values = value_counts[value_counts < rare_threshold].index
            cleaned_series = cleaned_series.replace(rare_values, 'other')
        
        return cleaned_series
    
    def _get_synonym_mappings(self, column_name: str) -> Dict:
        """获取同义词映射表"""
        mappings = {
            'industry': {
                '制造': 'manufacturing',
                '制造业': 'manufacturing',
                'manufacturing': 'manufacturing',
                '生产': 'manufacturing',
                '工厂': 'manufacturing',
                
                '金融': 'finance',
                '银行': 'finance',
                '保险': 'finance',
                '证券': 'finance',
                'finance': 'finance',
                'banking': 'finance',
                
                '零售': 'retail',
                '电商': 'retail',
                '销售': 'retail',
                'retail': 'retail',
                'e-commerce': 'retail',
                'ecommerce': 'retail',
                
                '医疗': 'healthcare',
                '健康': 'healthcare',
                '医院': 'healthcare',
                'healthcare': 'healthcare',
                'medical': 'healthcare',
                
                '教育': 'education',
                '培训': 'education',
                '学校': 'education',
                'education': 'education',
                'training': 'education',
            },
            'company_size': {
                '初创': 'startup',
                '初创公司': 'startup',
                'startup': 'startup',
                '创业': 'startup',
                
                '小微': 'small',
                '小型': 'small',
                'small': 'small',
                '微型': 'small',
                
                '中型': 'medium',
                '中等': 'medium',
                'medium': 'medium',
                '中大型': 'medium',
                
                '大型': 'large',
                'large': 'large',
                '巨头': 'large',
                '大企业': 'large',
            },
            'urgency_level': {
                '紧急': 'high',
                '急': 'high',
                '很急': 'high',
                'urgent': 'high',
                'high': 'high',
                
                '普通': 'medium',
                '中等': 'medium',
                '一般': 'medium',
                'medium': 'medium',
                'normal': 'medium',
                
                '不急': 'low',
                '慢': 'low',
                '低': 'low',
                'low': 'low',
                'slow': 'low',
            }
        }
        
        return mappings.get(column_name, {})
    
    def _validate_data_completeness(self, df: pd.DataFrame) -> pd.DataFrame:
        """数据完整性验证"""
        # 定义必须字段
        required_fields = ['requirement_description', 'industry', 'budget']
        
        # 检查必须字段的完整性
        for field in required_fields:
            if field in df.columns:
                before_count = len(df)
                df = df.dropna(subset=[field])
                after_count = len(df)
                if before_count != after_count:
                    print(f"因{field}字段缺失，删除了 {before_count - after_count} 条记录")
        
        # 计算各字段的完整度
        completeness_stats = {}
        for col in df.columns:
            completeness = (1 - df[col].isna().sum() / len(df)) * 100
            completeness_stats[col] = completeness
        
        self.processed_stats['completeness'] = completeness_stats
        
        return df
    
    def _handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """异常值处理"""
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if col in ['budget', 'timeline_days', 'expected_roi']:
                # 使用业务逻辑处理异常值
                if col == 'budget':
                    # 预算范围：1000 - 10,000,000
                    df[col] = df[col].clip(lower=1000, upper=10000000)
                elif col == 'timeline_days':
                    # 时间范围：1 - 365天
                    df[col] = df[col].clip(lower=1, upper=365)
                elif col == 'expected_roi':
                    # ROI范围：0.1 - 10倍
                    df[col] = df[col].clip(lower=0.1, upper=10.0)
        
        return df

# 供应商能力数据清洗
class SupplierCapabilityDataCleaner:
    """供应商AI能力数据清洗器"""
    
    def __init__(self):
        self.skill_standardizer = SkillStandardizer()
        self.quality_scorer = QualityScorer()
        
    def clean_supplier_data(self, raw_supplier_data: pd.DataFrame) -> pd.DataFrame:
        """清洗供应商能力数据"""
        cleaned_data = raw_supplier_data.copy()
        
        # 1. 基础信息清洗
        cleaned_data = self._clean_basic_info(cleaned_data)
        
        # 2. 技能标签标准化
        if 'skills' in cleaned_data.columns:
            cleaned_data['skills'] = cleaned_data['skills'].apply(
                self.skill_standardizer.standardize_skills
            )
        
        # 3. 价格数据清洗
        if 'pricing_info' in cleaned_data.columns:
            cleaned_data['pricing_info'] = cleaned_data['pricing_info'].apply(
                self._clean_pricing_data
            )
        
        # 4. 案例数据验证
        if 'case_studies' in cleaned_data.columns:
            cleaned_data['case_studies'] = cleaned_data['case_studies'].apply(
                self._validate_case_studies
            )
        
        # 5. 供应商质量评分
        cleaned_data['quality_score'] = cleaned_data.apply(
            self.quality_scorer.calculate_quality_score, axis=1
        )
        
        return cleaned_data
    
    def _clean_pricing_data(self, pricing_str: str) -> Dict:
        """清洗价格数据"""
        if pd.isna(pricing_str):
            return {}
            
        try:
            if isinstance(pricing_str, str):
                pricing_data = json.loads(pricing_str)
            else:
                pricing_data = pricing_str
            
            # 价格格式标准化
            cleaned_pricing = {}
            
            # 基础价格
            if 'base_price' in pricing_data:
                base_price = float(re.sub(r'[^\d.]', '', str(pricing_data['base_price'])))
                cleaned_pricing['base_price'] = max(base_price, 0)  # 确保非负
            
            # 按量计费
            if 'per_unit_price' in pricing_data:
                unit_price = float(re.sub(r'[^\d.]', '', str(pricing_data['per_unit_price'])))
                cleaned_pricing['per_unit_price'] = max(unit_price, 0)
            
            # 分成比例
            if 'commission_rate' in pricing_data:
                commission = float(re.sub(r'[^\d.]', '', str(pricing_data['commission_rate'])))
                cleaned_pricing['commission_rate'] = max(0, min(commission, 100))  # 0-100%范围
            
            return cleaned_pricing
            
        except (json.JSONDecodeError, ValueError, TypeError):
            return {}

class SkillStandardizer:
    """技能标签标准化器"""
    
    def __init__(self):
        self.skill_mappings = self._load_skill_mappings()
        
    def _load_skill_mappings(self) -> Dict:
        """加载技能标签映射表"""
        return {
            # AI/ML相关
            'machine learning': 'ml',
            '机器学习': 'ml',
            'deep learning': 'dl',
            '深度学习': 'dl',
            'artificial intelligence': 'ai',
            '人工智能': 'ai',
            'natural language processing': 'nlp',
            '自然语言处理': 'nlp',
            'computer vision': 'cv',
            '计算机视觉': 'cv',
            'reinforcement learning': 'rl',
            '强化学习': 'rl',
            
            # 编程语言
            'python': 'python',
            'java': 'java',
            'javascript': 'javascript',
            'typescript': 'typescript',
            'go': 'golang',
            'golang': 'golang',
            'c++': 'cpp',
            'c#': 'csharp',
            'r': 'r',
            'scala': 'scala',
            
            # 框架和工具
            'tensorflow': 'tensorflow',
            'pytorch': 'pytorch',
            'keras': 'keras',
            'scikit-learn': 'sklearn',
            'pandas': 'pandas',
            'numpy': 'numpy',
            'opencv': 'opencv',
            'huggingface': 'huggingface',
            'transformers': 'transformers',
            
            # 云平台
            'aws': 'aws',
            'azure': 'azure',
            'google cloud': 'gcp',
            'gcp': 'gcp',
            '阿里云': 'aliyun',
            '腾讯云': 'tencent_cloud',
            
            # 数据库
            'mysql': 'mysql',
            'postgresql': 'postgresql',
            'mongodb': 'mongodb',
            'redis': 'redis',
            'elasticsearch': 'elasticsearch',
        }
    
    def standardize_skills(self, skills_input) -> List[str]:
        """标准化技能标签列表"""
        if pd.isna(skills_input):
            return []
        
        # 处理不同输入格式
        if isinstance(skills_input, str):
            if skills_input.startswith('[') and skills_input.endswith(']'):
                try:
                    skills_list = json.loads(skills_input)
                except json.JSONDecodeError:
                    skills_list = skills_input.strip('[]').split(',')
            else:
                skills_list = [s.strip() for s in skills_input.split(',')]
        elif isinstance(skills_input, list):
            skills_list = skills_input
        else:
            return []
        
        # 标准化每个技能
        standardized_skills = []
        for skill in skills_list:
            if not skill or pd.isna(skill):
                continue
                
            skill_clean = skill.lower().strip()
            
            # 查找映射
            standardized_skill = self.skill_mappings.get(skill_clean, skill_clean)
            
            # 去重添加
            if standardized_skill not in standardized_skills:
                standardized_skills.append(standardized_skill)
        
        return standardized_skills
```

---

## 🏷️ 智能数据标注系统

### 自动化标注流水线
```python
class AutoDataAnnotationSystem:
    """
    智能数据标注系统
    结合人工标注和AI辅助标注，提供高质量数据标注服务
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.pre_labeling_models = self._load_pre_labeling_models()
        self.quality_checkers = self._initialize_quality_checkers()
        self.annotation_stats = {}
        
    def _load_pre_labeling_models(self) -> Dict:
        """加载AI预标注模型"""
        return {
            'intent_classifier': self._load_intent_classification_model(),
            'entity_extractor': self._load_entity_extraction_model(),
            'sentiment_analyzer': self._load_sentiment_analysis_model(),
            'complexity_scorer': self._load_complexity_scoring_model()
        }
    
    async def annotate_user_requirements(self, requirements_data: pd.DataFrame) -> pd.DataFrame:
        """
        用户需求数据批量标注
        """
        annotated_data = requirements_data.copy()
        
        # 1. AI预标注
        print("开始AI预标注...")
        annotated_data = await self._ai_pre_labeling(annotated_data)
        
        # 2. 人工审核和修正
        print("开始人工审核...")
        annotated_data = await self._human_review_process(annotated_data)
        
        # 3. 质量控制检查
        print("开始质量检查...")
        quality_metrics = self._quality_control_check(annotated_data)
        
        # 4. 一致性验证
        print("开始一致性验证...")
        annotated_data = self._consistency_validation(annotated_data)
        
        # 5. 标注统计
        self._generate_annotation_stats(annotated_data, quality_metrics)
        
        return annotated_data
    
    async def _ai_pre_labeling(self, data: pd.DataFrame) -> pd.DataFrame:
        """AI预标注过程"""
        pre_labeled_data = data.copy()
        
        # 并行执行多个预标注任务
        tasks = []
        
        # 意图分类
        if 'requirement_description' in data.columns:
            tasks.append(self._classify_intents(data['requirement_description']))
        
        # 实体抽取
        if 'requirement_description' in data.columns:
            tasks.append(self._extract_entities(data['requirement_description']))
        
        # 情感分析
        if 'requirement_description' in data.columns:
            tasks.append(self._analyze_sentiment(data['requirement_description']))
        
        # 复杂度评分
        tasks.append(self._score_complexity(data))
        
        # 执行所有标注任务
        results = await asyncio.gather(*tasks)
        
        # 整合标注结果
        if len(results) >= 4:
            pre_labeled_data['ai_predicted_intent'] = results[0]
            pre_labeled_data['ai_extracted_entities'] = results[1]
            pre_labeled_data['ai_sentiment_score'] = results[2]
            pre_labeled_data['ai_complexity_score'] = results[3]
        
        return pre_labeled_data
    
    async def _classify_intents(self, descriptions: pd.Series) -> List[str]:
        """意图分类预标注"""
        intent_predictions = []
        
        # 定义意图关键词模式
        intent_patterns = {
            'development': [
                '开发', '构建', '搭建', '建设', '创建',
                'develop', 'build', 'create', 'construct'
            ],
            'consultation': [
                '咨询', '了解', '询问', '请教', '讨论',
                'consult', 'advice', 'discuss', 'understand'
            ],
            'integration': [
                '集成', '对接', '融合', '连接', '整合',
                'integrate', 'connect', 'merge', 'combine'
            ],
            'optimization': [
                '优化', '改进', '提升', '增强', '完善',
                'optimize', 'improve', 'enhance', 'upgrade'
            ],
            'automation': [
                '自动化', '智能化', '无人化', '自助',
                'automate', 'intelligent', 'autonomous'
            ]
        }
        
        for desc in descriptions:
            if pd.isna(desc):
                intent_predictions.append('unknown')
                continue
            
            desc_lower = desc.lower()
            intent_scores = {}
            
            # 计算每个意图的匹配分数
            for intent, keywords in intent_patterns.items():
                score = sum(1 for keyword in keywords if keyword in desc_lower)
                intent_scores[intent] = score
            
            # 选择得分最高的意图
            if max(intent_scores.values()) > 0:
                predicted_intent = max(intent_scores, key=intent_scores.get)
            else:
                predicted_intent = 'unknown'
            
            intent_predictions.append(predicted_intent)
        
        return intent_predictions
    
    async def _extract_entities(self, descriptions: pd.Series) -> List[Dict]:
        """实体抽取预标注"""
        entity_predictions = []
        
        # 定义实体模式
        entity_patterns = {
            'technology': {
                'ai_ml': ['AI', '人工智能', 'ML', '机器学习', '深度学习', 'DL'],
                'nlp': ['NLP', '自然语言', '文本分析', '语音识别', '聊天机器人'],
                'cv': ['计算机视觉', 'CV', '图像识别', '人脸识别', '物体检测'],
                'big_data': ['大数据', '数据分析', '数据挖掘', '商业智能', 'BI']
            },
            'industry': {
                'finance': ['金融', '银行', '保险', '证券', '支付', '风控'],
                'manufacturing': ['制造', '工厂', '生产', '供应链', '质量控制'],
                'retail': ['零售', '电商', '销售', '客户服务', '推荐系统'],
                'healthcare': ['医疗', '健康', '诊断', '药物', '医院', '患者'],
                'education': ['教育', '培训', '学习', '课程', '学生', '教师']
            },
            'business_function': {
                'customer_service': ['客服', '客户服务', '在线咨询', '问答系统'],
                'marketing': ['营销', '推广', '广告', '用户画像', '精准营销'],
                'operations': ['运营', '流程', '自动化', '效率', '管理'],
                'analytics': ['分析', '报告', '预测', '监控', '统计']
            }
        }
        
        for desc in descriptions:
            extracted_entities = {
                'technology': [],
                'industry': [],
                'business_function': []
            }
            
            if pd.isna(desc):
                entity_predictions.append(extracted_entities)
                continue
            
            desc_lower = desc.lower()
            
            # 在每个类别中查找匹配的实体
            for entity_type, subcategories in entity_patterns.items():
                for subcat, keywords in subcategories.items():
                    for keyword in keywords:
                        if keyword.lower() in desc_lower:
                            if subcat not in extracted_entities[entity_type]:
                                extracted_entities[entity_type].append(subcat)
            
            entity_predictions.append(extracted_entities)
        
        return entity_predictions
    
    async def _score_complexity(self, data: pd.DataFrame) -> List[float]:
        """复杂度评分预标注"""
        complexity_scores = []
        
        for _, row in data.iterrows():
            score = 0.0
            
            # 基于需求描述长度
            if 'requirement_description' in row:
                desc_length = len(str(row['requirement_description'])) if pd.notna(row['requirement_description']) else 0
                length_score = min(desc_length / 1000, 1.0) * 0.3
                score += length_score
            
            # 基于预算规模
            if 'budget' in row and pd.notna(row['budget']):
                budget = float(row['budget'])
                if budget > 1000000:  # 100万以上
                    budget_score = 0.8
                elif budget > 100000:  # 10万以上
                    budget_score = 0.6
                elif budget > 10000:   # 1万以上
                    budget_score = 0.4
                else:
                    budget_score = 0.2
                score += budget_score * 0.3
            
            # 基于时间要求
            if 'timeline_days' in row and pd.notna(row['timeline_days']):
                timeline = float(row['timeline_days'])
                if timeline < 30:      # 1个月内
                    timeline_score = 0.8
                elif timeline < 90:    # 3个月内
                    timeline_score = 0.6
                elif timeline < 180:   # 6个月内
                    timeline_score = 0.4
                else:
                    timeline_score = 0.2
                score += timeline_score * 0.2
            
            # 基于团队规模要求
            if 'team_size' in row and pd.notna(row['team_size']):
                team_size = float(row['team_size'])
                if team_size > 10:
                    team_score = 0.8
                elif team_size > 5:
                    team_score = 0.6
                elif team_size > 2:
                    team_score = 0.4
                else:
                    team_score = 0.2
                score += team_score * 0.2
            
            # 标准化到0-1范围
            final_score = min(max(score, 0.0), 1.0)
            complexity_scores.append(final_score)
        
        return complexity_scores
    
    async def _human_review_process(self, pre_labeled_data: pd.DataFrame) -> pd.DataFrame:
        """
        人工审核流程
        """
        reviewed_data = pre_labeled_data.copy()
        
        # 1. 置信度检查 - 低置信度样本需要人工审核
        low_confidence_mask = self._identify_low_confidence_samples(reviewed_data)
        
        # 2. 随机抽样审核 - 保证质量
        random_sample_mask = self._random_sample_for_review(reviewed_data, sample_rate=0.1)
        
        # 3. 需要人工审核的样本
        need_review_mask = low_confidence_mask | random_sample_mask
        
        print(f"需要人工审核的样本数量: {need_review_mask.sum()}")
        
        # 4. 模拟人工审核过程（实际项目中这里会是人工界面）
        reviewed_data = self._simulate_human_review(reviewed_data, need_review_mask)
        
        return reviewed_data
    
    def _identify_low_confidence_samples(self, data: pd.DataFrame) -> pd.Series:
        """识别低置信度样本"""
        low_confidence_mask = pd.Series([False] * len(data), index=data.index)
        
        # 基于复杂度分数识别边界样本
        if 'ai_complexity_score' in data.columns:
            complexity_scores = data['ai_complexity_score']
            # 复杂度在0.4-0.6之间的样本被认为是边界样本，需要人工审核
            boundary_mask = (complexity_scores >= 0.4) & (complexity_scores <= 0.6)
            low_confidence_mask |= boundary_mask
        
        # 基于意图分类的不确定性
        if 'ai_predicted_intent' in data.columns:
            uncertain_intent_mask = data['ai_predicted_intent'] == 'unknown'
            low_confidence_mask |= uncertain_intent_mask
        
        return low_confidence_mask
    
    def _quality_control_check(self, annotated_data: pd.DataFrame) -> Dict:
        """质量控制检查"""
        quality_metrics = {}
        
        # 1. 标注完整性检查
        required_labels = ['ai_predicted_intent', 'ai_complexity_score']
        completeness_rates = {}
        
        for label in required_labels:
            if label in annotated_data.columns:
                non_null_rate = (1 - annotated_data[label].isna().sum() / len(annotated_data))
                completeness_rates[label] = non_null_rate
        
        quality_metrics['completeness_rates'] = completeness_rates
        
        # 2. 标注分布合理性检查
        if 'ai_predicted_intent' in annotated_data.columns:
            intent_distribution = annotated_data['ai_predicted_intent'].value_counts(normalize=True)
            quality_metrics['intent_distribution'] = intent_distribution.to_dict()
        
        # 3. 复杂度分数分布检查
        if 'ai_complexity_score' in annotated_data.columns:
            complexity_stats = annotated_data['ai_complexity_score'].describe()
            quality_metrics['complexity_distribution'] = complexity_stats.to_dict()
        
        return quality_metrics

# 标注质量控制系统
class AnnotationQualityController:
    """标注质量控制器"""
    
    def __init__(self):
        self.quality_thresholds = {
            'min_agreement_rate': 0.85,  # 标注员间一致性最低85%
            'max_error_rate': 0.1,       # 错误率最高10%
            'min_completeness': 0.95      # 完整性最低95%
        }
        
    def multi_annotator_consistency_check(self, annotations: List[Dict]) -> Dict:
        """多标注员一致性检查"""
        if len(annotations) < 2:
            return {'error': '需要至少2个标注员的标注结果'}
        
        consistency_metrics = {}
        
        # 检查每个标注字段的一致性
        for field in annotations[0].keys():
            field_values = [ann.get(field) for ann in annotations]
            
            # 计算一致性率
            if all(v is not None for v in field_values):
                unique_values = set(field_values)
                if len(unique_values) == 1:
                    agreement_rate = 1.0
                else:
                    # 计算多数一致性
                    from collections import Counter
                    value_counts = Counter(field_values)
                    most_common_count = value_counts.most_common(1)[0][1]
                    agreement_rate = most_common_count / len(field_values)
            else:
                agreement_rate = 0.0
            
            consistency_metrics[field] = agreement_rate
        
        # 整体一致性评分
        overall_consistency = np.mean(list(consistency_metrics.values()))
        
        return {
            'field_consistency': consistency_metrics,
            'overall_consistency': overall_consistency,
            'quality_pass': overall_consistency >= self.quality_thresholds['min_agreement_rate']
        }
    
    def detect_annotation_errors(self, annotated_data: pd.DataFrame) -> Dict:
        """检测标注错误"""
        error_report = {
            'missing_values': {},
            'invalid_formats': {},
            'logical_inconsistencies': []
        }
        
        # 1. 检查缺失值
        for col in annotated_data.columns:
            missing_count = annotated_data[col].isna().sum()
            if missing_count > 0:
                error_report['missing_values'][col] = {
                    'count': int(missing_count),
                    'rate': float(missing_count / len(annotated_data))
                }
        
        # 2. 检查格式错误
        if 'ai_complexity_score' in annotated_data.columns:
            # 复杂度分数应该在0-1之间
            invalid_complexity = annotated_data[
                (annotated_data['ai_complexity_score'] < 0) | 
                (annotated_data['ai_complexity_score'] > 1)
            ]
            if len(invalid_complexity) > 0:
                error_report['invalid_formats']['complexity_score'] = len(invalid_complexity)
        
        # 3. 检查逻辑一致性
        if 'budget' in annotated_data.columns and 'ai_complexity_score' in annotated_data.columns:
            # 高预算项目的复杂度不应该太低
            high_budget_low_complexity = annotated_data[
                (annotated_data['budget'] > 500000) & 
                (annotated_data['ai_complexity_score'] < 0.3)
            ]
            if len(high_budget_low_complexity) > 0:
                error_report['logical_inconsistencies'].append({
                    'type': 'high_budget_low_complexity',
                    'count': len(high_budget_low_complexity),
                    'description': '高预算项目复杂度过低，可能存在标注错误'
                })
        
        return error_report
```

---

## 📈 数据质量监控与反馈系统

### 实时数据质量监控
```python
class DataQualityMonitor:
    """数据质量实时监控系统"""
    
    def __init__(self):
        self.quality_metrics = {
            'completeness': [],
            'accuracy': [],
            'consistency': [],
            'timeliness': [],
            'validity': []
        }
        self.alert_thresholds = {
            'completeness': 0.95,
            'accuracy': 0.90,
            'consistency': 0.85,
            'timeliness': 0.90,
            'validity': 0.95
        }
        
    def real_time_quality_check(self, new_data: pd.DataFrame) -> Dict:
        """实时数据质量检查"""
        quality_report = {}
        
        # 1. 完整性检查
        completeness_score = self._check_completeness(new_data)
        quality_report['completeness'] = completeness_score
        
        # 2. 准确性检查
        accuracy_score = self._check_accuracy(new_data)
        quality_report['accuracy'] = accuracy_score
        
        # 3. 一致性检查
        consistency_score = self._check_consistency(new_data)
        quality_report['consistency'] = consistency_score
        
        # 4. 时效性检查
        timeliness_score = self._check_timeliness(new_data)
        quality_report['timeliness'] = timeliness_score
        
        # 5. 有效性检查
        validity_score = self._check_validity(new_data)
        quality_report['validity'] = validity_score
        
        # 6. 综合质量分数
        overall_quality = np.mean(list(quality_report.values()))
        quality_report['overall_quality'] = overall_quality
        
        # 7. 告警检查
        alerts = self._check_quality_alerts(quality_report)
        quality_report['alerts'] = alerts
        
        # 8. 记录质量指标历史
        self._record_quality_metrics(quality_report)
        
        return quality_report
    
    def _check_completeness(self, data: pd.DataFrame) -> float:
        """检查数据完整性"""
        total_cells = data.shape[0] * data.shape[1]
        missing_cells = data.isna().sum().sum()
        completeness = (total_cells - missing_cells) / total_cells
        return float(completeness)
    
    def _check_accuracy(self, data: pd.DataFrame) -> float:
        """检查数据准确性（基于业务规则）"""
        accuracy_scores = []
        
        # 预算准确性检查
        if 'budget' in data.columns:
            valid_budget = data['budget'].between(1000, 10000000, inclusive='both')
            budget_accuracy = valid_budget.sum() / len(data)
            accuracy_scores.append(budget_accuracy)
        
        # 时间线准确性检查
        if 'timeline_days' in data.columns:
            valid_timeline = data['timeline_days'].between(1, 365, inclusive='both')
            timeline_accuracy = valid_timeline.sum() / len(data)
            accuracy_scores.append(timeline_accuracy)
        
        # 团队规模准确性检查
        if 'team_size' in data.columns:
            valid_team_size = data['team_size'].between(1, 100, inclusive='both')
            team_size_accuracy = valid_team_size.sum() / len(data)
            accuracy_scores.append(team_size_accuracy)
        
        return float(np.mean(accuracy_scores)) if accuracy_scores else 1.0
    
    def _check_consistency(self, data: pd.DataFrame) -> float:
        """检查数据一致性"""
        consistency_scores = []
        
        # 预算与复杂度一致性
        if 'budget' in data.columns and 'ai_complexity_score' in data.columns:
            high_budget = data['budget'] > 500000
            high_complexity = data['ai_complexity_score'] > 0.7
            consistency = (high_budget == high_complexity).sum() / len(data)
            consistency_scores.append(consistency)
        
        # 时间线与紧急程度一致性
        if 'timeline_days' in data.columns and 'urgency_level' in data.columns:
            urgent_timeline = data['timeline_days'] < 30
            urgent_level = data['urgency_level'] == 'high'
            consistency = (urgent_timeline == urgent_level).sum() / len(data)
            consistency_scores.append(consistency)
        
        return float(np.mean(consistency_scores)) if consistency_scores else 1.0
    
    def _check_quality_alerts(self, quality_report: Dict) -> List[Dict]:
        """检查质量告警"""
        alerts = []
        
        for metric, score in quality_report.items():
            if metric in self.alert_thresholds:
                threshold = self.alert_thresholds[metric]
                if score < threshold:
                    alerts.append({
                        'metric': metric,
                        'current_score': score,
                        'threshold': threshold,
                        'severity': 'high' if score < threshold * 0.8 else 'medium',
                        'message': f'{metric}质量分数{score:.3f}低于阈值{threshold}'
                    })
        
        return alerts
    
    def generate_quality_dashboard(self) -> Dict:
        """生成数据质量仪表板数据"""
        if not any(self.quality_metrics.values()):
            return {'error': '暂无质量监控数据'}
        
        dashboard_data = {}
        
        for metric, values in self.quality_metrics.items():
            if values:
                dashboard_data[metric] = {
                    'current': values[-1],
                    'average': np.mean(values),
                    'trend': 'improving' if len(values) > 1 and values[-1] > values[-2] else 'declining',
                    'history': values[-30:]  # 最近30个数据点
                }
        
        return dashboard_data

# 数据反馈循环系统
class DataFeedbackLoop:
    """数据反馈循环系统"""
    
    def __init__(self):
        self.feedback_buffer = []
        self.improvement_suggestions = []
        
    def collect_user_feedback(self, user_id: str, data_item_id: str, 
                            feedback_type: str, feedback_content: Dict):
        """收集用户对数据质量的反馈"""
        feedback_record = {
            'timestamp': datetime.now(),
            'user_id': user_id,
            'data_item_id': data_item_id,
            'feedback_type': feedback_type,  # 'quality_issue', 'suggestion', 'correction'
            'feedback_content': feedback_content
        }
        
        self.feedback_buffer.append(feedback_record)
        
        # 实时处理紧急反馈
        if feedback_type == 'quality_issue':
            self._handle_quality_issue(feedback_record)
    
    def _handle_quality_issue(self, feedback_record: Dict):
        """处理数据质量问题"""
        issue_type = feedback_record['feedback_content'].get('issue_type')
        
        if issue_type == 'data_accuracy':
            # 数据准确性问题
            self._flag_data_for_review(feedback_record['data_item_id'])
        elif issue_type == 'missing_data':
            # 数据缺失问题
            self._request_data_completion(feedback_record['data_item_id'])
        elif issue_type == 'annotation_error':
            # 标注错误问题
            self._schedule_re_annotation(feedback_record['data_item_id'])
    
    def generate_improvement_plan(self) -> Dict:
        """基于反馈生成改进计划"""
        if not self.feedback_buffer:
            return {'message': '暂无反馈数据用于生成改进计划'}
        
        # 统计反馈类型
        feedback_stats = {}
        for feedback in self.feedback_buffer:
            ftype = feedback['feedback_type']
            feedback_stats[ftype] = feedback_stats.get(ftype, 0) + 1
        
        # 生成改进建议
        improvement_plan = {
            'priority_issues': [],
            'suggested_actions': [],
            'resource_requirements': {}
        }
        
        # 基于反馈频率确定优先级问题
        sorted_issues = sorted(feedback_stats.items(), key=lambda x: x[1], reverse=True)
        improvement_plan['priority_issues'] = sorted_issues[:3]
        
        # 生成具体行动建议
        for issue_type, count in sorted_issues:
            if issue_type == 'quality_issue':
                improvement_plan['suggested_actions'].append({
                    'action': '加强数据质量检查流程',
                    'description': f'针对{count}个质量问题，建议增加自动化检查规则',
                    'estimated_effort': 'medium'
                })
            elif issue_type == 'annotation_error':
                improvement_plan['suggested_actions'].append({
                    'action': '优化标注员培训',
                    'description': f'针对{count}个标注错误，建议增加标注员培训和质量检查',
                    'estimated_effort': 'high'
                })
        
        return improvement_plan
```

---

**文档维护**: 数据工程团队  
**最后更新**: 2025年8月12日  
**版本控制**: v2.0.0 - 第3轮数据清洗标注流程细节优化版  
**核心价值**: 构建自动化、高质量的数据处理和标注流水线，确保AI推荐系统拥有清洁、准确、一致的数据基础，支持高精度的智能推荐服务