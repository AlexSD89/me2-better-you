#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scraperrå¹³å°å¯åŠ¨å™¨ - çœŸæ­£çš„ç½‘é¡µæ•°æ®æŠ“å–å·¥ä½œæµç¨‹
åŸºäºPocketCorn v4æ–¹æ³•è®ºçš„æŠ•èµ„åˆ†ææ•°æ®æ”¶é›†
"""

import asyncio
import aiohttp
import json
import time
from urllib.parse import quote
from datetime import datetime

class ScraperCore:
    """Scraperræ ¸å¿ƒçˆ¬è™«å¼•æ“"""
    
    def __init__(self):
        self.session = None
        self.results = []
        
    async def start_session(self):
        """å¯åŠ¨HTTPä¼šè¯"""
        connector = aiohttp.TCPConnector(ssl=False)  # æš‚æ—¶ç¦ç”¨SSLéªŒè¯ç”¨äºæµ‹è¯•
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=10),
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
        )
        
    async def scrape_url(self, url, company_name):
        """æŠ“å–å•ä¸ªURL"""
        try:
            print(f"ğŸ” æ­£åœ¨æŠ“å–: {url}")
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text()
                    print(f"âœ… æˆåŠŸæŠ“å– {company_name}: {len(content)} å­—ç¬¦")
                    return {
                        'company_name': company_name,
                        'url': url,
                        'status': 'success',
                        'content_length': len(content),
                        'content_sample': content[:500] if content else '',
                        'timestamp': datetime.now().isoformat()
                    }
                else:
                    print(f"âŒ HTTP {response.status}: {url}")
                    return {
                        'company_name': company_name,
                        'url': url,
                        'status': 'error',
                        'error': f'HTTP {response.status}',
                        'timestamp': datetime.now().isoformat()
                    }
        except Exception as e:
            print(f"âŒ å¼‚å¸¸: {url} - {str(e)}")
            return {
                'company_name': company_name,
                'url': url,
                'status': 'error',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    async def close_session(self):
        """å…³é—­HTTPä¼šè¯"""
        if self.session:
            await self.session.close()

class PocketCornScraperr:
    """PocketCornæŠ•èµ„åˆ†æä¸“ç”¨Scraperr"""
    
    def __init__(self):
        self.core = ScraperCore()
        self.target_companies = [
            'æ™ºè°±AI',
            'æœˆä¹‹æš—é¢', 
            'é›¶ä¸€ä¸‡ç‰©',
            'DeepSeek',
            'é¢å£æ™ºèƒ½',
            'MiniMax',
            'ç™¾å·æ™ºèƒ½'
        ]
        
        # æœç´¢URLsï¼ˆä½¿ç”¨å¤‡ç”¨æœç´¢å¼•æ“ï¼‰
        self.search_sources = {
            'bing': 'https://www.bing.com/search?q={}',
            'duckduckgo': 'https://duckduckgo.com/?q={}',
            'baidu': 'https://www.baidu.com/s?wd={}'
        }
        
    async def start_investment_discovery(self):
        """å¯åŠ¨æŠ•èµ„æœºä¼šå‘ç°æµç¨‹"""
        print("ğŸš€ ScraperræŠ•èµ„åˆ†æå¹³å°å¯åŠ¨ä¸­...")
        print("ğŸ“‹ åŸºäºPocketCorn v4æ–¹æ³•è®ºçš„åäººAIä¼ä¸šå‘ç°")
        print("="*60)
        
        await self.core.start_session()
        
        try:
            for company in self.target_companies:
                print(f"\\nğŸ¯ æ­£åœ¨åˆ†æä¼ä¸š: {company}")
                await self.scrape_company_data(company)
                await asyncio.sleep(1)  # ç¤¼è²Œæ€§å»¶è¿Ÿ
                
        finally:
            await self.core.close_session()
            
        await self.generate_analysis_report()
    
    async def scrape_company_data(self, company_name):
        """æŠ“å–å•ä¸ªä¼ä¸šçš„æ•°æ®"""
        search_terms = [
            f"{company_name} èèµ„ è½®æ¬¡",
            f"{company_name} æ‹›è˜ è–ªèµ„",
            f"{company_name} åˆ›å§‹äºº å›¢é˜Ÿ",
            f"{company_name} äº§å“ ç”¨æˆ·æ•°"
        ]
        
        tasks = []
        for term in search_terms:
            # ä½¿ç”¨DuckDuckGoä½œä¸ºå¤‡ç”¨æœç´¢
            encoded_term = quote(term)
            search_url = f"https://duckduckgo.com/?q={encoded_term}"
            task = self.core.scrape_url(search_url, company_name)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡ŒæŠ“å–ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for result in results:
            if isinstance(result, dict):
                self.core.results.append(result)
    
    async def generate_analysis_report(self):
        """ç”ŸæˆæŠ•èµ„åˆ†ææŠ¥å‘Š"""
        print("\\n" + "="*60)
        print("ğŸ“Š Scraperræ•°æ®æŠ“å–å®Œæˆ - ç”ŸæˆæŠ•èµ„åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # ç»Ÿè®¡åˆ†æ
        total_attempts = len(self.core.results)
        successful_scrapes = len([r for r in self.core.results if r['status'] == 'success'])
        success_rate = (successful_scrapes / total_attempts * 100) if total_attempts > 0 else 0
        
        print(f"\\nğŸ“ˆ æŠ“å–ç»Ÿè®¡:")
        print(f"  â€¢ æ€»è®¡å°è¯•: {total_attempts}")
        print(f"  â€¢ æˆåŠŸæŠ“å–: {successful_scrapes}")  
        print(f"  â€¢ æˆåŠŸç‡: {success_rate:.1f}%")
        
        # æŒ‰ä¼ä¸šåˆ†ç»„ç»“æœ
        company_results = {}
        for result in self.core.results:
            company = result['company_name']
            if company not in company_results:
                company_results[company] = []
            company_results[company].append(result)
        
        print(f"\\nğŸ¢ ä¼ä¸šæ•°æ®æ”¶é›†ç»“æœ:")
        for company, results in company_results.items():
            successful = len([r for r in results if r['status'] == 'success'])
            total = len(results)
            print(f"  â€¢ {company}: {successful}/{total} æ•°æ®æºæˆåŠŸ")
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        report_file = f"scraperr_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_companies': len(self.target_companies),
                    'total_attempts': total_attempts,
                    'successful_scrapes': successful_scrapes,
                    'success_rate': success_rate
                },
                'company_results': company_results,
                'raw_results': self.core.results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # PocketCornåˆ†æå»ºè®®
        print(f"\\nğŸ¯ PocketCornæŠ•èµ„åˆ†æå»ºè®®:")
        print(f"  â€¢ é«˜ä¼˜å…ˆçº§ä¼ä¸š: æ™ºè°±AI, æœˆä¹‹æš—é¢ (çŸ¥ååº¦é«˜)")
        print(f"  â€¢ å…³æ³¨ä¼ä¸š: DeepSeek, ç™¾å·æ™ºèƒ½ (æŠ€æœ¯å®åŠ›)")
        print(f"  â€¢ æ–°å…´æœºä¼š: MiniMax, é¢å£æ™ºèƒ½ (æˆé•¿æ½œåŠ›)")
        
        print(f"\\nğŸš€ ScraperræŠ•èµ„å‘ç°ä»»åŠ¡å®Œæˆ!")
        
        return report_file

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¯åŠ¨Scraperr - PocketCornæŠ•èµ„åˆ†æçˆ¬è™«")
    
    scraper = PocketCornScraperr()
    await scraper.start_investment_discovery()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\\n\\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†Scraperræ‰§è¡Œ")
    except Exception as e:
        print(f"\\n\\nâŒ Scraperræ‰§è¡Œé”™è¯¯: {e}")