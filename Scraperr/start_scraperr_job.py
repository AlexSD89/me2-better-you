#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scraperrä»»åŠ¡å¯åŠ¨è„šæœ¬
ä½¿ç”¨é…ç½®æ–‡ä»¶å¯åŠ¨PocketCornæŠ•èµ„åˆ†ææ•°æ®æ”¶é›†ä»»åŠ¡
"""

import json
import os
import sys
from datetime import datetime

def load_job_config():
    """åŠ è½½Scraperrä»»åŠ¡é…ç½®"""
    config_file = "scraperr_job_config.json"
    
    if not os.path.exists(config_file):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        return None
        
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
            print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {config['job_name']} v{config['version']}")
            return config
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return None

def validate_scraperr_environment():
    """éªŒè¯Scraperrè¿è¡Œç¯å¢ƒ"""
    print("ğŸ”§ æ£€æŸ¥Scraperrç¯å¢ƒ...")
    
    # æ£€æŸ¥å¿…è¦çš„ç›®å½•
    required_dirs = ['configs', 'data', 'api/backend']
    missing_dirs = []
    
    for dir_path in required_dirs:
        if not os.path.exists(dir_path):
            missing_dirs.append(dir_path)
    
    if missing_dirs:
        print(f"âš ï¸  ç¼ºå°‘ç›®å½•: {missing_dirs}")
    else:
        print("âœ… ç›®å½•ç»“æ„å®Œæ•´")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_files = [
        'configs/chinese_company_detection.json',
        'configs/mrr_signals_config.json', 
        'configs/multi_dimension_scoring.json'
    ]
    
    for config_file in config_files:
        if os.path.exists(config_file):
            print(f"âœ… é…ç½®æ–‡ä»¶: {config_file}")
        else:
            print(f"âŒ ç¼ºå°‘é…ç½®: {config_file}")
    
    return True

def create_scraperr_task_file(config):
    """åˆ›å»ºScraperrä»»åŠ¡æ–‡ä»¶"""
    task_data = {
        "task_id": f"pocketcorn_investment_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "name": config["job_name"],
        "type": "investment_analysis",
        "status": "pending",
        "created_at": datetime.now().isoformat(),
        "config": config,
        "targets": config["target_companies"],
        "sources": len(config["data_sources"]),
        "expected_completion": "2-4 hours"
    }
    
    # ä¿å­˜ä»»åŠ¡æ–‡ä»¶åˆ°dataç›®å½•
    task_file = f"data/scraperr_task_{task_data['task_id']}.json"
    
    try:
        # ç¡®ä¿dataç›®å½•å­˜åœ¨
        os.makedirs('data', exist_ok=True)
        
        with open(task_file, 'w', encoding='utf-8') as f:
            json.dump(task_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ ä»»åŠ¡æ–‡ä»¶å·²åˆ›å»º: {task_file}")
        return task_file
        
    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ–‡ä»¶åˆ›å»ºå¤±è´¥: {e}")
        return None

def generate_scraperr_commands(config):
    """ç”ŸæˆScraperræ‰§è¡Œå‘½ä»¤"""
    print("\\nğŸš€ Scraperræ‰§è¡ŒæŒ‡å—:")
    print("="*50)
    
    print("\\n1ï¸âƒ£ å¯åŠ¨Scraperrå¹³å°:")
    print("   å¦‚æœæ˜¯Next.jsåº”ç”¨:")
    print("   npm run dev")
    print("   æˆ–è€…")  
    print("   yarn dev")
    print()
    print("   å¦‚æœæ˜¯Python FastAPI:")
    print("   python api/backend/main.py")
    print("   æˆ–è€…")
    print("   uvicorn main:app --reload")
    
    print("\\n2ï¸âƒ£ é…ç½®æŠ“å–ä»»åŠ¡:")
    print(f"   â€¢ ä»»åŠ¡åç§°: {config['job_name']}")
    print(f"   â€¢ ç›®æ ‡ä¼ä¸š: {len(config['target_companies'])}å®¶")
    print(f"   â€¢ æ•°æ®æº: {len(config['data_sources'])}ä¸ª")
    print(f"   â€¢ è°ƒåº¦æ–¹å¼: {config['schedule']['type']} at {config['schedule']['time']}")
    
    print("\\n3ï¸âƒ£ æ•°æ®æºé…ç½®:")
    for i, source in enumerate(config['data_sources'], 1):
        print(f"   {i}. {source['name']}")
        print(f"      URL: {source['base_url']}")
        print(f"      é¢‘ç‡é™åˆ¶: {source['rate_limit']['requests_per_minute']} req/min")
        print()
    
    print("4ï¸âƒ£ è¾“å‡ºè®¾ç½®:")
    print(f"   â€¢ æ ¼å¼: {config['output_settings']['format']}")
    print(f"   â€¢ æ–‡ä»¶å‘½å: {config['output_settings']['file_naming']}")
    print(f"   â€¢ å®æ—¶è­¦æŠ¥: {'å¯ç”¨' if config['output_settings']['real_time_alerts']['high_value_targets'] else 'ç¦ç”¨'}")
    
    print("\\n5ï¸âƒ£ ç›‘æ§é¢æ¿è®¿é—®:")
    print("   â€¢ Webç•Œé¢: http://localhost:3000 (å¦‚æœæ˜¯Next.js)")
    print("   â€¢ APIæ–‡æ¡£: http://localhost:8000/docs (å¦‚æœæ˜¯FastAPI)")
    print("   â€¢ ä»»åŠ¡çŠ¶æ€: æ£€æŸ¥data/ç›®å½•ä¸‹çš„ä»»åŠ¡æ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¯åŠ¨Scraperr - PocketCornæŠ•èµ„åˆ†æä»»åŠ¡é…ç½®")
    print("="*60)
    
    # 1. éªŒè¯ç¯å¢ƒ
    if not validate_scraperr_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥")
        return
    
    # 2. åŠ è½½é…ç½®
    config = load_job_config()
    if not config:
        return
    
    # 3. åˆ›å»ºä»»åŠ¡æ–‡ä»¶
    task_file = create_scraperr_task_file(config)
    if not task_file:
        return
    
    # 4. ç”Ÿæˆæ‰§è¡ŒæŒ‡å—
    generate_scraperr_commands(config)
    
    print("\\nâœ… Scraperrä»»åŠ¡é…ç½®å®Œæˆ!")
    print("\\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("   1. å¯åŠ¨Scraperrå¹³å°æœåŠ¡")
    print("   2. åœ¨å¹³å°ç•Œé¢ä¸­å¯¼å…¥ä»»åŠ¡é…ç½®")
    print("   3. å¼€å§‹æ•°æ®æ”¶é›†") 
    print("   4. ç›‘æ§æ”¶é›†è¿›åº¦")
    print("   5. åˆ†ææ”¶é›†ç»“æœ")

if __name__ == "__main__":
    main()