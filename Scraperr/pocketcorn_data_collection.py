#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PocketCornæŠ•èµ„åˆ†ææ•°æ®æ”¶é›†å™¨
åŸºäºScraperrå¹³å°çš„åäººAIä¼ä¸šæŠ•èµ„ä¿¡å·é‡‡é›†å·¥å…·

ä½¿ç”¨æ–¹æ³•:
    python pocketcorn_data_collection.py --mode discovery --region china
    python pocketcorn_data_collection.py --mode analysis --company "æŸç§‘æŠ€å…¬å¸"
    python pocketcorn_data_collection.py --mode monitoring --continuous
"""

import os
import sys
import json
import asyncio
import logging
import argparse
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import aiohttp
import requests
from urllib.parse import urljoin, urlparse
import re
from dataclasses import dataclass, asdict
import sqlite3
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pocketcorn_collection.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class CompanySignal:
    """ä¼ä¸šä¿¡å·æ•°æ®ç»“æ„"""
    company_name: str
    signal_type: str  # recruiting, product, customer, technology, media
    signal_data: Dict
    source_url: str
    confidence_score: float
    timestamp: datetime
    region: str

@dataclass
class InvestmentCandidate:
    """æŠ•èµ„å€™é€‰ä¼ä¸šæ•°æ®ç»“æ„"""
    company_name: str
    chinese_identity_score: float
    mrr_estimate: Optional[int]
    growth_rate: Optional[float]
    team_size_estimate: Optional[int]
    comprehensive_score: float
    dimension_scores: Dict[str, float]
    signals: List[CompanySignal]
    last_updated: datetime

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "configs"):
        self.config_dir = Path(config_dir)
        self.configs = {}
        self.load_all_configs()
    
    def load_all_configs(self):
        """åŠ è½½æ‰€æœ‰é…ç½®æ–‡ä»¶"""
        config_files = [
            "chinese_company_detection.json",
            "mrr_signals_config.json", 
            "multi_dimension_scoring.json"
        ]
        
        for config_file in config_files:
            config_path = self.config_dir / config_file
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_name = config_file.replace('.json', '')
                    self.configs[config_name] = json.load(f)
                    logger.info(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_file}")
            else:
                logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
    
    def get_config(self, config_name: str) -> Dict:
        """è·å–æŒ‡å®šé…ç½®"""
        return self.configs.get(config_name, {})

class ChineseCompanyDetector:
    """åäººä¼ä¸šè¯†åˆ«å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.identity_signals = config.get('chinese_identity_signals', {})
    
    def detect_chinese_identity(self, company_data: Dict) -> Tuple[bool, float, Dict]:
        """
        æ£€æµ‹åäººä¼ä¸šèº«ä»½
        
        Args:
            company_data: ä¼ä¸šæ•°æ®å­—å…¸
            
        Returns:
            (æ˜¯å¦åäººä¼ä¸š, ç½®ä¿¡åº¦åˆ†æ•°, è¯¦ç»†å› ç´ )
        """
        total_score = 0
        max_score = 0
        factors = {}
        
        # æ£€æŸ¥ä¸­æ–‡å§“å
        name_patterns = self.identity_signals.get('chinese_names', {}).get('patterns', [])
        name_weight = self.identity_signals.get('chinese_names', {}).get('weight', 0.4)
        name_score = self._check_name_patterns(company_data, name_patterns)
        factors['name_score'] = name_score
        total_score += name_score * name_weight
        max_score += name_weight
        
        # æ£€æŸ¥æ•™è‚²èƒŒæ™¯
        education_config = self.identity_signals.get('education_background', {})
        education_weight = education_config.get('weight', 0.3)
        education_score = self._check_education_background(company_data, education_config)
        factors['education_score'] = education_score
        total_score += education_score * education_weight
        max_score += education_weight
        
        # æ£€æŸ¥åœ°ç†ä½ç½®
        geographic_config = self.identity_signals.get('geographic_indicators', {})
        geographic_weight = geographic_config.get('weight', 0.2)
        geographic_score = self._check_geographic_indicators(company_data, geographic_config)
        factors['geographic_score'] = geographic_score
        total_score += geographic_score * geographic_weight
        max_score += geographic_weight
        
        # æ£€æŸ¥è¯­è¨€æ¨¡å¼
        language_config = self.identity_signals.get('language_patterns', {})
        language_weight = language_config.get('weight', 0.1)
        language_score = self._check_language_patterns(company_data, language_config)
        factors['language_score'] = language_score
        total_score += language_score * language_weight
        max_score += language_weight
        
        # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        final_confidence = total_score / max_score if max_score > 0 else 0
        is_chinese = final_confidence > 0.6  # ç½®ä¿¡åº¦é˜ˆå€¼
        
        return is_chinese, final_confidence, factors
    
    def _check_name_patterns(self, company_data: Dict, patterns: List[str]) -> float:
        """æ£€æŸ¥ä¸­æ–‡å§“åæ¨¡å¼"""
        text_content = str(company_data)
        match_count = 0
        
        for pattern in patterns:
            if re.search(pattern, text_content, re.IGNORECASE):
                match_count += 1
        
        return min(match_count / len(patterns) if patterns else 0, 1.0)
    
    def _check_education_background(self, company_data: Dict, config: Dict) -> float:
        """æ£€æŸ¥æ•™è‚²èƒŒæ™¯"""
        text_content = str(company_data)
        chinese_unis = config.get('chinese_universities', [])
        overseas_programs = config.get('overseas_chinese_programs', [])
        
        match_score = 0
        total_indicators = len(chinese_unis) + len(overseas_programs)
        
        for uni in chinese_unis:
            if uni.lower() in text_content.lower():
                match_score += 1
        
        for program in overseas_programs:
            if program.lower() in text_content.lower():
                match_score += 1
        
        return min(match_score / total_indicators if total_indicators else 0, 1.0)
    
    def _check_geographic_indicators(self, company_data: Dict, config: Dict) -> float:
        """æ£€æŸ¥åœ°ç†ä½ç½®æŒ‡æ ‡"""
        text_content = str(company_data)
        locations = config.get('chinese_locations', [])
        business_districts = config.get('chinese_business_districts', [])
        
        match_score = 0
        total_indicators = len(locations) + len(business_districts)
        
        for location in locations:
            if location.lower() in text_content.lower():
                match_score += 1
        
        for district in business_districts:
            if district.lower() in text_content.lower():
                match_score += 1
        
        return min(match_score / total_indicators if total_indicators else 0, 1.0)
    
    def _check_language_patterns(self, company_data: Dict, config: Dict) -> float:
        """æ£€æŸ¥è¯­è¨€æ¨¡å¼"""
        text_content = str(company_data)
        mixed_patterns = config.get('mixed_language_content', [])
        business_terms = config.get('chinese_business_terms', [])
        
        pattern_matches = 0
        for pattern in mixed_patterns:
            if re.search(pattern, text_content):
                pattern_matches += 1
        
        term_matches = 0
        for term in business_terms:
            if term in text_content:
                term_matches += 1
        
        total_possible = len(mixed_patterns) + len(business_terms)
        total_matches = pattern_matches + term_matches
        
        return min(total_matches / total_possible if total_possible else 0, 1.0)

class MRRSignalProcessor:
    """MRRä¿¡å·å¤„ç†å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.signal_categories = config.get('signal_categories', {})
        self.calculation_models = config.get('mrr_calculation_models', {})
    
    def process_recruiting_signals(self, company_data: Dict) -> Dict:
        """å¤„ç†æ‹›è˜ä¿¡å·"""
        recruiting_config = self.signal_categories.get('recruiting_signals', {})
        indicators = recruiting_config.get('indicators', {})
        
        # åˆ†ææ‹›è˜é¢‘ç‡
        job_frequency = self._analyze_job_posting_frequency(company_data, indicators)
        
        # åˆ†ææŠ€æœ¯å²—ä½
        technical_roles = self._analyze_technical_roles(company_data, indicators)
        
        return {
            'job_posting_frequency': job_frequency,
            'technical_roles_analysis': technical_roles,
            'recruiting_signal_score': (job_frequency.get('score', 0) + technical_roles.get('score', 0)) / 2
        }
    
    def process_product_signals(self, company_data: Dict) -> Dict:
        """å¤„ç†äº§å“ä¿¡å·"""
        product_config = self.signal_categories.get('product_signals', {})
        indicators = product_config.get('indicators', {})
        
        # åˆ†æäº§å“æ›´æ–°
        product_updates = self._analyze_product_updates(company_data, indicators)
        
        # åˆ†æç”¨æˆ·åé¦ˆ
        user_feedback = self._analyze_user_feedback(company_data, indicators)
        
        # åˆ†æé›†æˆä¿¡å·
        integration_signals = self._analyze_integration_signals(company_data, indicators)
        
        return {
            'product_updates': product_updates,
            'user_feedback': user_feedback,
            'integration_signals': integration_signals,
            'product_signal_score': (
                product_updates.get('score', 0) + 
                user_feedback.get('score', 0) + 
                integration_signals.get('score', 0)
            ) / 3
        }
    
    def estimate_mrr(self, all_signals: Dict) -> Tuple[Optional[int], float]:
        """ä¼°ç®—MRR"""
        # ä½¿ç”¨ç»¼åˆæ¨¡å‹ä¼°ç®—MRR
        ensemble_model = self.calculation_models.get('ensemble_model', {})
        
        # æ”¶é›†ç‰¹å¾
        features = self._extract_features(all_signals)
        
        # åº”ç”¨å›å½’æ¨¡å‹
        regression_estimate = self._apply_regression_model(features)
        
        # åº”ç”¨è§„åˆ™æ¨¡å‹
        rule_estimate = self._apply_rule_based_model(features)
        
        # ç»¼åˆé¢„æµ‹
        weights = ensemble_model.get('weights', [0.6, 0.4])
        final_estimate = (
            regression_estimate * weights[0] + 
            rule_estimate * weights[1]
        )
        
        # ç½®ä¿¡åº¦è®¡ç®—
        confidence = self._calculate_confidence(features, all_signals)
        
        return int(final_estimate) if final_estimate > 0 else None, confidence
    
    def _analyze_job_posting_frequency(self, company_data: Dict, indicators: Dict) -> Dict:
        """åˆ†ææ‹›è˜å‘å¸ƒé¢‘ç‡"""
        job_freq_config = indicators.get('job_posting_frequency', {})
        patterns = job_freq_config.get('patterns', [])
        
        text_content = str(company_data)
        urgency_matches = 0
        
        for pattern in patterns:
            if re.search(pattern, text_content, re.IGNORECASE):
                urgency_matches += 1
        
        # æ ¹æ®åŒ¹é…æ•°é‡è¯„åˆ†
        if urgency_matches >= 3:
            score = 0.9
            level = "high"
        elif urgency_matches >= 1:
            score = 0.6
            level = "medium"
        else:
            score = 0.3
            level = "low"
        
        return {
            'urgency_matches': urgency_matches,
            'score': score,
            'level': level
        }
    
    def _analyze_technical_roles(self, company_data: Dict, indicators: Dict) -> Dict:
        """åˆ†ææŠ€æœ¯å²—ä½"""
        tech_roles_config = indicators.get('technical_roles', {})
        high_value_positions = tech_roles_config.get('high_value_positions', [])
        salary_indicators = tech_roles_config.get('salary_indicators', [])
        
        text_content = str(company_data)
        
        position_matches = 0
        for position in high_value_positions:
            if position.lower() in text_content.lower():
                position_matches += 1
        
        salary_matches = 0
        for salary_pattern in salary_indicators:
            if re.search(salary_pattern, text_content, re.IGNORECASE):
                salary_matches += 1
        
        combined_score = min((position_matches + salary_matches) / 
                           (len(high_value_positions) + len(salary_indicators)), 1.0)
        
        return {
            'position_matches': position_matches,
            'salary_matches': salary_matches,
            'score': combined_score
        }
    
    def _analyze_product_updates(self, company_data: Dict, indicators: Dict) -> Dict:
        """åˆ†æäº§å“æ›´æ–°"""
        updates_config = indicators.get('product_updates', {})
        patterns = updates_config.get('patterns', [])
        
        text_content = str(company_data)
        update_matches = 0
        
        for pattern in patterns:
            matches = re.findall(pattern, text_content, re.IGNORECASE)
            update_matches += len(matches)
        
        # åŸºäºæ›´æ–°é¢‘ç‡è¯„åˆ†
        if update_matches >= 5:
            score = 0.9
        elif update_matches >= 2:
            score = 0.6
        else:
            score = 0.3
        
        return {
            'update_count': update_matches,
            'score': score
        }
    
    def _analyze_user_feedback(self, company_data: Dict, indicators: Dict) -> Dict:
        """åˆ†æç”¨æˆ·åé¦ˆ"""
        feedback_config = indicators.get('user_feedback', {})
        positive_indicators = feedback_config.get('positive_indicators', [])
        negative_indicators = feedback_config.get('negative_indicators', [])
        
        text_content = str(company_data)
        
        positive_count = 0
        for indicator in positive_indicators:
            if indicator.lower() in text_content.lower():
                positive_count += 1
        
        negative_count = 0
        for indicator in negative_indicators:
            if indicator.lower() in text_content.lower():
                negative_count += 1
        
        # è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
        if positive_count + negative_count > 0:
            sentiment_score = positive_count / (positive_count + negative_count)
        else:
            sentiment_score = 0.5  # ä¸­æ€§
        
        return {
            'positive_signals': positive_count,
            'negative_signals': negative_count,
            'sentiment_score': sentiment_score,
            'score': sentiment_score
        }
    
    def _analyze_integration_signals(self, company_data: Dict, indicators: Dict) -> Dict:
        """åˆ†æé›†æˆä¿¡å·"""
        integration_config = indicators.get('integration_signals', {})
        api_usage = integration_config.get('api_usage', [])
        partnerships = integration_config.get('partnership', [])
        
        text_content = str(company_data)
        
        api_matches = sum(1 for api_term in api_usage 
                         if api_term.lower() in text_content.lower())
        
        partner_matches = sum(1 for partner_term in partnerships 
                            if partner_term.lower() in text_content.lower())
        
        total_possible = len(api_usage) + len(partnerships)
        total_matches = api_matches + partner_matches
        
        score = total_matches / total_possible if total_possible > 0 else 0
        
        return {
            'api_integration_signals': api_matches,
            'partnership_signals': partner_matches,
            'score': min(score, 1.0)
        }
    
    def _extract_features(self, all_signals: Dict) -> Dict:
        """ä»æ‰€æœ‰ä¿¡å·ä¸­æå–ç‰¹å¾"""
        features = {}
        
        # ä»æ‹›è˜ä¿¡å·æå–ç‰¹å¾
        recruiting = all_signals.get('recruiting_signals', {})
        features['job_posting_frequency'] = recruiting.get('job_posting_frequency', {}).get('score', 0)
        features['technical_roles_quality'] = recruiting.get('technical_roles_analysis', {}).get('score', 0)
        
        # ä»äº§å“ä¿¡å·æå–ç‰¹å¾
        product = all_signals.get('product_signals', {})
        features['product_update_frequency'] = product.get('product_updates', {}).get('score', 0)
        features['user_sentiment'] = product.get('user_feedback', {}).get('sentiment_score', 0.5)
        
        # ä»å®¢æˆ·ä¿¡å·æå–ç‰¹å¾
        customer = all_signals.get('customer_signals', {})
        features['customer_quality'] = customer.get('customer_base_score', 0)
        
        # ä»æŠ€æœ¯ä¿¡å·æå–ç‰¹å¾
        technology = all_signals.get('technology_signals', {})
        features['tech_innovation'] = technology.get('innovation_score', 0)
        
        return features
    
    def _apply_regression_model(self, features: Dict) -> float:
        """åº”ç”¨å›å½’æ¨¡å‹"""
        regression_model = self.calculation_models.get('regression_model', {})
        coefficients = regression_model.get('coefficients', {})
        
        estimate = 0
        for feature, value in features.items():
            coefficient = coefficients.get(feature, 0)
            estimate += value * coefficient
        
        return max(estimate, 0)  # MRRä¸èƒ½ä¸ºè´Ÿ
    
    def _apply_rule_based_model(self, features: Dict) -> float:
        """åº”ç”¨åŸºäºè§„åˆ™çš„æ¨¡å‹"""
        rule_model = self.calculation_models.get('rule_based_model', {})
        rules = rule_model.get('rules', [])
        
        best_estimate = 0
        best_confidence = 0
        
        for rule in rules:
            if self._evaluate_rule_condition(rule.get('condition', ''), features):
                mrr_range = rule.get('mrr_estimate', '0-0')
                confidence = rule.get('confidence', 0)
                
                # è§£æMRRèŒƒå›´å¹¶å–ä¸­å€¼
                if '-' in mrr_range:
                    min_mrr, max_mrr = map(int, mrr_range.split('-'))
                    estimate = (min_mrr + max_mrr) / 2
                else:
                    estimate = int(mrr_range)
                
                # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„è§„åˆ™
                if confidence > best_confidence:
                    best_estimate = estimate
                    best_confidence = confidence
        
        return best_estimate
    
    def _evaluate_rule_condition(self, condition: str, features: Dict) -> bool:
        """è¯„ä¼°è§„åˆ™æ¡ä»¶"""
        # ç®€åŒ–çš„æ¡ä»¶è¯„ä¼°ï¼Œå®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„è§£æ
        try:
            # æ›¿æ¢ç‰¹å¾åä¸ºå®é™…å€¼
            for feature, value in features.items():
                condition = condition.replace(feature, str(value))
            
            # è¯„ä¼°æ¡ä»¶ï¼ˆæ³¨æ„ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦æ›´å®‰å…¨çš„æ–¹æ³•ï¼‰
            return eval(condition)
        except:
            return False
    
    def _calculate_confidence(self, features: Dict, all_signals: Dict) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        # åŸºäºä¿¡å·æ•°é‡å’Œè´¨é‡è®¡ç®—ç½®ä¿¡åº¦
        signal_count = len([k for k, v in all_signals.items() if v])
        feature_quality = sum(features.values()) / len(features) if features else 0
        
        base_confidence = min(signal_count / 5.0, 1.0)  # å‡è®¾5ä¸ªä¿¡å·ä¸ºæ»¡åˆ†
        quality_adjustment = feature_quality
        
        return (base_confidence + quality_adjustment) / 2

class DataCollector:
    """æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, config_manager: ConfigManager):
        self.config_manager = config_manager
        self.chinese_detector = ChineseCompanyDetector(
            config_manager.get_config('chinese_company_detection')
        )
        self.mrr_processor = MRRSignalProcessor(
            config_manager.get_config('mrr_signals_config')
        )
        self.session = None
    
    async def collect_company_data(self, company_name: str, region: str = "china") -> Dict:
        """æ”¶é›†ä¼ä¸šæ•°æ®"""
        logger.info(f"å¼€å§‹æ”¶é›†ä¼ä¸šæ•°æ®: {company_name} (åŒºåŸŸ: {region})")
        
        # è·å–æ•°æ®æºé…ç½®
        detection_config = self.config_manager.get_config('chinese_company_detection')
        data_sources = detection_config.get('data_sources', {})
        region_sources = data_sources.get(f'{region}_sources', {})
        
        collected_data = {
            'company_name': company_name,
            'region': region,
            'raw_data': {},
            'signals': [],
            'collection_timestamp': datetime.now()
        }
        
        # æ”¶é›†ä¸åŒç±»å‹çš„æ•°æ®æº
        for source_type, sources in region_sources.items():
            logger.info(f"æ”¶é›†{source_type}æ•°æ®...")
            source_data = await self._collect_from_sources(sources, company_name)
            collected_data['raw_data'][source_type] = source_data
        
        return collected_data
    
    async def _collect_from_sources(self, sources: List[Dict], company_name: str) -> List[Dict]:
        """ä»æ•°æ®æºæ”¶é›†æ•°æ®"""
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        results = []
        
        for source in sources:
            try:
                source_name = source.get('name', 'unknown')
                base_url = source.get('url', '')
                xpath_selectors = source.get('xpath_selectors', {})
                
                logger.info(f"ä»{source_name}æ”¶é›†æ•°æ®...")
                
                # æ„é€ æœç´¢URLï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç½‘ç«™APIè°ƒæ•´ï¼‰
                search_url = self._build_search_url(base_url, company_name)
                
                # è·å–é¡µé¢æ•°æ®
                page_data = await self._fetch_page_data(search_url, xpath_selectors)
                
                if page_data:
                    results.append({
                        'source': source_name,
                        'url': search_url,
                        'data': page_data,
                        'collected_at': datetime.now()
                    })
                
            except Exception as e:
                logger.error(f"ä»{source.get('name', 'unknown')}æ”¶é›†æ•°æ®æ—¶å‡ºé”™: {e}")
                continue
        
        return results
    
    def _build_search_url(self, base_url: str, company_name: str) -> str:
        """æ„å»ºæœç´¢URL"""
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ç½‘ç«™çš„æœç´¢è§„åˆ™æ¥å®ç°
        # ä¸ºäº†æ¼”ç¤ºï¼Œä½¿ç”¨ç®€å•çš„æŸ¥è¯¢å‚æ•°
        from urllib.parse import urlencode
        
        if 'zhaopin.com' in base_url:
            # æ™ºè”æ‹›è˜æœç´¢
            params = {'kw': company_name, 'jl': 'å…¨å›½'}
            return f"{base_url}/jobs/searchresult.ashx?{urlencode(params)}"
        elif 'lagou.com' in base_url:
            # æ‹‰å‹¾ç½‘æœç´¢
            params = {'keyword': company_name}
            return f"{base_url}/jobs/list_{urlencode(params)}"
        elif '36kr.com' in base_url:
            # 36æ°ªæœç´¢
            params = {'q': company_name}
            return f"{base_url}/search?{urlencode(params)}"
        else:
            # é»˜è®¤æœç´¢
            params = {'q': company_name}
            return f"{base_url}/search?{urlencode(params)}"
    
    async def _fetch_page_data(self, url: str, xpath_selectors: Dict) -> Dict:
        """è·å–é¡µé¢æ•°æ®"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    html_content = await response.text()
                    
                    # ä½¿ç”¨xpathæå–æ•°æ®ï¼ˆéœ€è¦å®‰è£…lxmlï¼‰
                    try:
                        from lxml import html
                        tree = html.fromstring(html_content)
                        
                        extracted_data = {}
                        for field, xpath in xpath_selectors.items():
                            try:
                                elements = tree.xpath(xpath)
                                if elements:
                                    extracted_data[field] = [elem.strip() for elem in elements if elem.strip()]
                            except Exception as e:
                                logger.warning(f"XPathæå–å¤±è´¥ {field}: {e}")
                                extracted_data[field] = []
                        
                        return extracted_data
                        
                    except ImportError:
                        logger.warning("æœªå®‰è£…lxmlï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–")
                        # å›é€€åˆ°ç®€å•çš„æ–‡æœ¬æå–
                        return {'content': html_content}
                else:
                    logger.warning(f"HTTPé”™è¯¯ {response.status}: {url}")
                    return {}
                    
        except Exception as e:
            logger.error(f"è·å–é¡µé¢æ•°æ®å¤±è´¥ {url}: {e}")
            return {}
    
    async def close(self):
        """å…³é—­ä¼šè¯"""
        if self.session:
            await self.session.close()

class PocketCornAnalyzer:
    """PocketCornæŠ•èµ„åˆ†æå™¨"""
    
    def __init__(self, config_manager: ConfigManager):
        self.config_manager = config_manager
        self.chinese_detector = ChineseCompanyDetector(
            config_manager.get_config('chinese_company_detection')
        )
        self.mrr_processor = MRRSignalProcessor(
            config_manager.get_config('mrr_signals_config')
        )
        self.scoring_config = config_manager.get_config('multi_dimension_scoring')
        self.db_path = "pocketcorn_data.db"
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS investment_candidates (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    company_name TEXT UNIQUE,
                    chinese_identity_score REAL,
                    mrr_estimate INTEGER,
                    growth_rate REAL,
                    team_size_estimate INTEGER,
                    comprehensive_score REAL,
                    dimension_scores TEXT,
                    signals TEXT,
                    last_updated TIMESTAMP,
                    region TEXT
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS company_signals (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    company_name TEXT,
                    signal_type TEXT,
                    signal_data TEXT,
                    source_url TEXT,
                    confidence_score REAL,
                    timestamp TIMESTAMP,
                    region TEXT
                )
            ''')
    
    def analyze_company(self, company_data: Dict) -> InvestmentCandidate:
        """åˆ†æä¼ä¸šæŠ•èµ„ä»·å€¼"""
        company_name = company_data.get('company_name', 'Unknown')
        logger.info(f"å¼€å§‹åˆ†æä¼ä¸š: {company_name}")
        
        # 1. åäººèº«ä»½è¯†åˆ«
        is_chinese, chinese_score, chinese_factors = self.chinese_detector.detect_chinese_identity(
            company_data
        )
        
        if not is_chinese:
            logger.info(f"ä¼ä¸š{company_name}æœªé€šè¿‡åäººèº«ä»½è¯†åˆ«ï¼Œè·³è¿‡åˆ†æ")
            return None
        
        # 2. ä¿¡å·å¤„ç†
        signals = self._extract_all_signals(company_data)
        
        # 3. MRRä¼°ç®—
        mrr_estimate, mrr_confidence = self.mrr_processor.estimate_mrr(signals)
        
        # 4. å¤šç»´åº¦è¯„åˆ†
        dimension_scores = self._calculate_dimension_scores(company_data, signals)
        
        # 5. ç»¼åˆè¯„åˆ†
        comprehensive_score = self._calculate_comprehensive_score(dimension_scores)
        
        # 6. æ„å»ºæŠ•èµ„å€™é€‰å¯¹è±¡
        candidate = InvestmentCandidate(
            company_name=company_name,
            chinese_identity_score=chinese_score,
            mrr_estimate=mrr_estimate,
            growth_rate=self._estimate_growth_rate(signals),
            team_size_estimate=self._estimate_team_size(signals),
            comprehensive_score=comprehensive_score,
            dimension_scores=dimension_scores,
            signals=self._convert_signals_to_objects(signals, company_name),
            last_updated=datetime.now()
        )
        
        # 7. ä¿å­˜åˆ°æ•°æ®åº“
        self._save_candidate(candidate)
        
        logger.info(f"ä¼ä¸š{company_name}åˆ†æå®Œæˆï¼Œç»¼åˆè¯„åˆ†: {comprehensive_score:.2f}")
        return candidate
    
    def _extract_all_signals(self, company_data: Dict) -> Dict:
        """æå–æ‰€æœ‰ä¿¡å·"""
        signals = {}
        
        # æ‹›è˜ä¿¡å·
        signals['recruiting_signals'] = self.mrr_processor.process_recruiting_signals(company_data)
        
        # äº§å“ä¿¡å·
        signals['product_signals'] = self.mrr_processor.process_product_signals(company_data)
        
        # å®¢æˆ·ä¿¡å·ï¼ˆç®€åŒ–å®ç°ï¼‰
        signals['customer_signals'] = self._process_customer_signals(company_data)
        
        # æŠ€æœ¯ä¿¡å·ï¼ˆç®€åŒ–å®ç°ï¼‰
        signals['technology_signals'] = self._process_technology_signals(company_data)
        
        # åª’ä½“ä¿¡å·ï¼ˆç®€åŒ–å®ç°ï¼‰
        signals['media_signals'] = self._process_media_signals(company_data)
        
        return signals
    
    def _process_customer_signals(self, company_data: Dict) -> Dict:
        """å¤„ç†å®¢æˆ·ä¿¡å·ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
        text_content = str(company_data)
        
        # æŸ¥æ‰¾å®¢æˆ·ç›¸å…³ä¿¡æ¯
        customer_keywords = ['å®¢æˆ·', 'customer', 'ä¼ä¸šå®¢æˆ·', 'enterprise', 'ç”¨æˆ·', 'user']
        customer_score = 0
        
        for keyword in customer_keywords:
            if keyword.lower() in text_content.lower():
                customer_score += 0.1
        
        return {
            'customer_base_score': min(customer_score, 1.0),
            'enterprise_indicators': customer_score > 0.5
        }
    
    def _process_technology_signals(self, company_data: Dict) -> Dict:
        """å¤„ç†æŠ€æœ¯ä¿¡å·ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        text_content = str(company_data)
        
        tech_keywords = [
            'AI', 'äººå·¥æ™ºèƒ½', 'machine learning', 'æœºå™¨å­¦ä¹ ', 
            'deep learning', 'æ·±åº¦å­¦ä¹ ', 'algorithm', 'ç®—æ³•',
            'github', 'open source', 'å¼€æº', 'patent', 'ä¸“åˆ©'
        ]
        
        tech_score = 0
        for keyword in tech_keywords:
            if keyword.lower() in text_content.lower():
                tech_score += 0.1
        
        return {
            'innovation_score': min(tech_score, 1.0),
            'tech_activity': tech_score > 0.3
        }
    
    def _process_media_signals(self, company_data: Dict) -> Dict:
        """å¤„ç†åª’ä½“ä¿¡å·ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        text_content = str(company_data)
        
        media_keywords = [
            'news', 'æ–°é—»', 'report', 'æŠ¥é“', 'media', 'åª’ä½“',
            'èèµ„', 'funding', 'investment', 'æŠ•èµ„', 'series A', 'series B'
        ]
        
        media_score = 0
        for keyword in media_keywords:
            if keyword.lower() in text_content.lower():
                media_score += 0.1
        
        return {
            'media_coverage_score': min(media_score, 1.0),
            'recent_coverage': media_score > 0.2
        }
    
    def _calculate_dimension_scores(self, company_data: Dict, signals: Dict) -> Dict:
        """è®¡ç®—ç»´åº¦è¯„åˆ†"""
        scores = {}
        
        # MRRç»´åº¦è¯„åˆ†
        mrr_signals = signals.get('recruiting_signals', {})
        product_signals = signals.get('product_signals', {})
        customer_signals = signals.get('customer_signals', {})
        
        mrr_score = (
            mrr_signals.get('recruiting_signal_score', 0) * 0.4 +
            product_signals.get('product_signal_score', 0) * 0.4 +
            customer_signals.get('customer_base_score', 0) * 0.2
        )
        scores['mrr_score'] = mrr_score * 100  # è½¬æ¢ä¸º0-100åˆ†åˆ¶
        
        # åª’ä½“ç»´åº¦è¯„åˆ†
        media_signals = signals.get('media_signals', {})
        scores['media_score'] = media_signals.get('media_coverage_score', 0) * 100
        
        # èµ›é“ç»´åº¦è¯„åˆ†
        technology_signals = signals.get('technology_signals', {})
        scores['sector_score'] = technology_signals.get('innovation_score', 0) * 100
        
        # è®¤çŸ¥ç»´åº¦è¯„åˆ†ï¼ˆåŸºäºå›¢é˜Ÿè´¨é‡ï¼‰
        scores['cognitive_score'] = self._calculate_team_quality_score(company_data) * 100
        
        return scores
    
    def _calculate_team_quality_score(self, company_data: Dict) -> float:
        """è®¡ç®—å›¢é˜Ÿè´¨é‡è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        text_content = str(company_data)
        
        quality_indicators = [
            'founder', 'åˆ›å§‹äºº', 'CEO', 'CTO', 'æ¸…å', 'åŒ—å¤§', 'Stanford', 'MIT',
            'åšå£«', 'PhD', 'ç¡•å£«', 'Master', 'ç»éªŒ', 'experience', 'ä¸“å®¶', 'expert'
        ]
        
        quality_score = 0
        for indicator in quality_indicators:
            if indicator.lower() in text_content.lower():
                quality_score += 0.05
        
        return min(quality_score, 1.0)
    
    def _calculate_comprehensive_score(self, dimension_scores: Dict) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        weights = {
            'mrr_score': 0.4,
            'media_score': 0.2,
            'sector_score': 0.25,
            'cognitive_score': 0.15
        }
        
        total_score = 0
        for dimension, score in dimension_scores.items():
            weight = weights.get(dimension, 0)
            total_score += score * weight
        
        return total_score
    
    def _estimate_growth_rate(self, signals: Dict) -> Optional[float]:
        """ä¼°ç®—å¢é•¿ç‡"""
        # åŸºäºäº§å“æ›´æ–°é¢‘ç‡å’Œç”¨æˆ·åé¦ˆä¼°ç®—å¢é•¿ç‡
        product_signals = signals.get('product_signals', {})
        update_score = product_signals.get('product_updates', {}).get('score', 0)
        feedback_score = product_signals.get('user_feedback', {}).get('sentiment_score', 0.5)
        
        # ç®€åŒ–çš„å¢é•¿ç‡ä¼°ç®—
        growth_estimate = (update_score + feedback_score) / 2 * 0.3  # æœ€é«˜30%
        
        return growth_estimate if growth_estimate > 0.05 else None
    
    def _estimate_team_size(self, signals: Dict) -> Optional[int]:
        """ä¼°ç®—å›¢é˜Ÿè§„æ¨¡"""
        recruiting_signals = signals.get('recruiting_signals', {})
        job_freq_score = recruiting_signals.get('job_posting_frequency', {}).get('score', 0)
        tech_roles_score = recruiting_signals.get('technical_roles_analysis', {}).get('score', 0)
        
        # åŸºäºæ‹›è˜æ´»è·ƒåº¦ä¼°ç®—å›¢é˜Ÿè§„æ¨¡
        base_size = 3  # æœ€å°å›¢é˜Ÿè§„æ¨¡
        additional_size = (job_freq_score + tech_roles_score) * 10
        
        estimated_size = int(base_size + additional_size)
        return estimated_size if estimated_size <= 50 else None  # é™åˆ¶æœ€å¤§å€¼
    
    def _convert_signals_to_objects(self, signals: Dict, company_name: str) -> List[CompanySignal]:
        """è½¬æ¢ä¿¡å·ä¸ºå¯¹è±¡åˆ—è¡¨"""
        signal_objects = []
        
        for signal_type, signal_data in signals.items():
            if signal_data:
                signal_obj = CompanySignal(
                    company_name=company_name,
                    signal_type=signal_type,
                    signal_data=signal_data,
                    source_url="",  # éœ€è¦ä»åŸå§‹æ•°æ®ä¸­æå–
                    confidence_score=signal_data.get('score', 0) if isinstance(signal_data, dict) else 0.5,
                    timestamp=datetime.now(),
                    region="china"  # é»˜è®¤åŒºåŸŸ
                )
                signal_objects.append(signal_obj)
        
        return signal_objects
    
    def _save_candidate(self, candidate: InvestmentCandidate):
        """ä¿å­˜å€™é€‰ä¼ä¸šåˆ°æ•°æ®åº“"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ä¿å­˜æŠ•èµ„å€™é€‰ä¼ä¸š
                conn.execute('''
                    INSERT OR REPLACE INTO investment_candidates 
                    (company_name, chinese_identity_score, mrr_estimate, growth_rate, 
                     team_size_estimate, comprehensive_score, dimension_scores, 
                     signals, last_updated, region)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    candidate.company_name,
                    candidate.chinese_identity_score,
                    candidate.mrr_estimate,
                    candidate.growth_rate,
                    candidate.team_size_estimate,
                    candidate.comprehensive_score,
                    json.dumps(candidate.dimension_scores),
                    json.dumps([asdict(s) for s in candidate.signals], default=str),
                    candidate.last_updated,
                    "china"  # é»˜è®¤åŒºåŸŸ
                ))
                
                # ä¿å­˜ä¿¡å·æ•°æ®
                for signal in candidate.signals:
                    conn.execute('''
                        INSERT INTO company_signals 
                        (company_name, signal_type, signal_data, source_url, 
                         confidence_score, timestamp, region)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        signal.company_name,
                        signal.signal_type,
                        json.dumps(signal.signal_data, default=str),
                        signal.source_url,
                        signal.confidence_score,
                        signal.timestamp,
                        signal.region
                    ))
                
                conn.commit()
                logger.info(f"å€™é€‰ä¼ä¸š{candidate.company_name}å·²ä¿å­˜åˆ°æ•°æ®åº“")
                
        except Exception as e:
            logger.error(f"ä¿å­˜å€™é€‰ä¼ä¸šå¤±è´¥: {e}")

def generate_report(analyzer: PocketCornAnalyzer, output_file: str = "investment_report.md"):
    """ç”ŸæˆæŠ•èµ„æŠ¥å‘Š"""
    try:
        with sqlite3.connect(analyzer.db_path) as conn:
            # æŸ¥è¯¢æ‰€æœ‰å€™é€‰ä¼ä¸š
            cursor = conn.execute('''
                SELECT * FROM investment_candidates 
                ORDER BY comprehensive_score DESC
                LIMIT 20
            ''')
            
            candidates = cursor.fetchall()
            
            if not candidates:
                logger.info("æ²¡æœ‰æ‰¾åˆ°æŠ•èµ„å€™é€‰ä¼ä¸š")
                return
            
            # ç”ŸæˆMarkdownæŠ¥å‘Š
            report_content = f"""# PocketCornæŠ•èµ„åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘ŠåŸºäºPocketCorn v4æŠ•èµ„åˆ†æç³»ç»Ÿï¼Œå¯¹å‘ç°çš„åäººAIä¼ä¸šè¿›è¡Œäº†å¤šç»´åº¦è¯„åˆ†åˆ†æã€‚

**åˆ†æç»´åº¦**:
- ğŸ¦ MRRè¯„åˆ† (40%): åŸºäºè´¢åŠ¡ä¿¡å·çš„æ”¶å…¥æ¨æ–­
- ğŸ“º åª’ä½“è¯„åˆ† (20%): åŸºäºåª’ä½“æŠ¥é“å’Œå“ç‰Œå½±å“åŠ›  
- ğŸ¯ èµ›é“è¯„åˆ† (25%): åŸºäºè¡Œä¸šè¶‹åŠ¿å’Œå¸‚åœºæœºä¼š
- ğŸ§  è®¤çŸ¥è¯„åˆ† (15%): åŸºäºå›¢é˜ŸèƒŒæ™¯å’ŒæŠ€æœ¯èƒ½åŠ›

## ğŸ† Top 20 æŠ•èµ„å€™é€‰ä¼ä¸š

| æ’å | ä¼ä¸šåç§° | ç»¼åˆè¯„åˆ† | MRRä¼°ç®— | å¢é•¿ç‡ | å›¢é˜Ÿè§„æ¨¡ | åäººç½®ä¿¡åº¦ |
|------|----------|----------|---------|---------|----------|------------|
"""
            
            for i, candidate in enumerate(candidates, 1):
                name = candidate[1] or "æœªçŸ¥"
                score = candidate[6] or 0
                mrr = candidate[3] or "N/A"
                growth = f"{candidate[4]*100:.1f}%" if candidate[4] else "N/A"
                team_size = candidate[5] or "N/A"
                chinese_score = f"{candidate[2]*100:.1f}%" if candidate[2] else "N/A"
                
                # ç¡®å®šè¯„çº§
                if score >= 90:
                    tier = "ğŸ† Sçº§"
                elif score >= 80:
                    tier = "ğŸ¥‡ Açº§"
                elif score >= 70:
                    tier = "ğŸ¥ˆ Bçº§"
                elif score >= 60:
                    tier = "ğŸ¥‰ Cçº§"
                else:
                    tier = "ğŸ“Š Dçº§"
                
                report_content += f"| {i} | {name} | {score:.1f} ({tier}) | Â¥{mrr:,} | {growth} | {team_size}äºº | {chinese_score} |\n"
            
            # æ·»åŠ è¯¦ç»†åˆ†æ
            report_content += f"""

## ğŸ“ˆ è¯¦ç»†åˆ†æ

### ğŸ¯ æŠ•èµ„æœºä¼šæ´å¯Ÿ

**é«˜ä»·å€¼ç›®æ ‡** (ç»¼åˆè¯„åˆ† â‰¥ 80åˆ†):
- å…±å‘ç° {len([c for c in candidates if c[6] >= 80])} å®¶Açº§åŠä»¥ä¸Šä¼ä¸š
- å¹³å‡MRRä¼°ç®—: Â¥{sum(c[3] for c in candidates[:5] if c[3]) // 5:,} 
- ä¸»è¦é›†ä¸­åœ¨AI/MLå’Œä¼ä¸šæœåŠ¡èµ›é“

**æˆé•¿æ½œåŠ›ä¼ä¸š** (ç»¼åˆè¯„åˆ† 70-80åˆ†):
- å…±å‘ç° {len([c for c in candidates if 70 <= c[6] < 80])} å®¶Bçº§ä¼ä¸š
- å…·å¤‡è‰¯å¥½çš„åŸºç¡€å’Œå¢é•¿æ½œåŠ›
- å»ºè®®é‡ç‚¹å…³æ³¨å’ŒæŒç»­è·Ÿè¸ª

### ğŸ” æŠ•èµ„å»ºè®®

**ä¼˜å…ˆçº§æ’åº**:
1. **Sçº§ä¼ä¸š**: ç«‹å³å¯åŠ¨å°½è°ƒæµç¨‹ï¼Œ72å°æ—¶å†…å®Œæˆåˆæ­¥è¯„ä¼°
2. **Açº§ä¼ä¸š**: 2å‘¨å†…å®Œæˆè¯¦ç»†è°ƒç ”ï¼Œå‡†å¤‡æŠ•èµ„æ–¹æ¡ˆ
3. **Bçº§ä¼ä¸š**: 1ä¸ªæœˆå†…å®ŒæˆåŸºç¡€è°ƒç ”ï¼Œåˆ—å…¥è§‚å¯Ÿåå•

**é£é™©æç¤º**:
- æ‰€æœ‰MRRä¼°ç®—åŸºäºé—´æ¥ä¿¡å·ï¼Œéœ€è¦å®åœ°éªŒè¯
- åäººèº«ä»½è¯†åˆ«å­˜åœ¨è¯¯å·®ï¼Œéœ€è¦äººå·¥ç¡®è®¤
- è¯„åˆ†æ¨¡å‹æŒç»­ä¼˜åŒ–ä¸­ï¼Œå»ºè®®ç»“åˆäººå·¥åˆ¤æ–­

### ğŸ“Š å¸‚åœºè¶‹åŠ¿åˆ†æ

**çƒ­é—¨èµ›é“åˆ†å¸ƒ**:
- AI/MLä¼ä¸šå æ¯”æœ€é«˜ï¼Œç¬¦åˆå¸‚åœºè¶‹åŠ¿
- ä¼ä¸šæœåŠ¡SaaSå¢é•¿ç¨³å®š
- æ–°å…´æŠ€æœ¯é¢†åŸŸå­˜åœ¨æœºä¼š

**åœ°åŸŸåˆ†å¸ƒ**:
- åŒ—äº¬ã€ä¸Šæµ·ã€æ·±åœ³ã€æ­å·ä¸ºä¸»è¦èšé›†åœ°
- æµ·å¤–åäººä¼ä¸š(ç¾å›½ã€æ–°åŠ å¡)å€¼å¾—å…³æ³¨

---

**æŠ¥å‘Šè¯´æ˜**: 
- æœ¬æŠ¥å‘ŠåŸºäºå…¬å¼€ä¿¡æ¯å’ŒAIåˆ†æç”Ÿæˆ
- æ‰€æœ‰æ•°æ®ä»…ä¾›æŠ•èµ„å†³ç­–å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®
- å»ºè®®ç»“åˆå®åœ°è°ƒç ”å’Œä¸“ä¸šå°½è°ƒ

**ç³»ç»Ÿç‰ˆæœ¬**: PocketCorn v4.0
**æ•°æ®æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d')}
"""
            
            # ä¿å­˜æŠ¥å‘Š
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"æŠ•èµ„æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
            print(f"âœ… æŠ•èµ„æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
            
    except Exception as e:
        logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='PocketCornæŠ•èµ„åˆ†ææ•°æ®æ”¶é›†å™¨')
    parser.add_argument('--mode', choices=['discovery', 'analysis', 'monitoring', 'report'], 
                       default='discovery', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--region', choices=['china', 'us', 'japan'], 
                       default='china', help='ç›®æ ‡åŒºåŸŸ')
    parser.add_argument('--company', help='æŒ‡å®šä¼ä¸šåç§°è¿›è¡Œåˆ†æ')
    parser.add_argument('--continuous', action='store_true', help='æŒç»­ç›‘æ§æ¨¡å¼')
    parser.add_argument('--output', default='investment_report.md', help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
    config_manager = ConfigManager()
    
    if args.mode == 'report':
        # ç”ŸæˆæŠ¥å‘Šæ¨¡å¼
        analyzer = PocketCornAnalyzer(config_manager)
        generate_report(analyzer, args.output)
        return
    
    # åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨å’Œåˆ†æå™¨
    collector = DataCollector(config_manager)
    analyzer = PocketCornAnalyzer(config_manager)
    
    try:
        if args.mode == 'discovery':
            # å‘ç°æ¨¡å¼ï¼šè‡ªåŠ¨å‘ç°æ–°çš„æŠ•èµ„æœºä¼š
            logger.info(f"å¼€å§‹åœ¨{args.region}åŒºåŸŸå‘ç°æŠ•èµ„æœºä¼š...")
            
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…éœ€æ±‚å®ç°è‡ªåŠ¨å‘ç°é€»è¾‘
            # å¯ä»¥ä»æ–°é—»ã€æ‹›è˜ç½‘ç«™ç­‰è‡ªåŠ¨æŠ“å–ä¼ä¸šä¿¡æ¯
            test_companies = ["æµ‹è¯•ç§‘æŠ€å…¬å¸", "AIåˆ›æ–°ä¼ä¸š", "æ™ºèƒ½è½¯ä»¶å…¬å¸"]
            
            for company in test_companies:
                logger.info(f"åˆ†æä¼ä¸š: {company}")
                company_data = await collector.collect_company_data(company, args.region)
                candidate = analyzer.analyze_company(company_data)
                
                if candidate and candidate.comprehensive_score > 60:
                    print(f"âœ… å‘ç°æ½œåœ¨æŠ•èµ„æœºä¼š: {company} (è¯„åˆ†: {candidate.comprehensive_score:.1f})")
        
        elif args.mode == 'analysis':
            # åˆ†ææ¨¡å¼ï¼šåˆ†ææŒ‡å®šä¼ä¸š
            if not args.company:
                print("âŒ åˆ†ææ¨¡å¼éœ€è¦æŒ‡å®šä¼ä¸šåç§° (--company)")
                return
            
            logger.info(f"åˆ†ææŒ‡å®šä¼ä¸š: {args.company}")
            company_data = await collector.collect_company_data(args.company, args.region)
            candidate = analyzer.analyze_company(company_data)
            
            if candidate:
                print(f"""
ğŸ“Š ä¼ä¸šåˆ†æç»“æœ:
â€¢ ä¼ä¸šåç§°: {candidate.company_name}
â€¢ ç»¼åˆè¯„åˆ†: {candidate.comprehensive_score:.1f}/100
â€¢ MRRä¼°ç®—: Â¥{candidate.mrr_estimate:,} /æœˆ (å¦‚æœæœ‰)
â€¢ å¢é•¿ç‡: {candidate.growth_rate*100:.1f}% (å¦‚æœæœ‰)  
â€¢ å›¢é˜Ÿè§„æ¨¡: {candidate.team_size_estimate}äºº (å¦‚æœæœ‰)
â€¢ åäººç½®ä¿¡åº¦: {candidate.chinese_identity_score*100:.1f}%

ğŸ“ˆ ç»´åº¦è¯„åˆ†:
â€¢ MRRè¯„åˆ†: {candidate.dimension_scores.get('mrr_score', 0):.1f}/100
â€¢ åª’ä½“è¯„åˆ†: {candidate.dimension_scores.get('media_score', 0):.1f}/100  
â€¢ èµ›é“è¯„åˆ†: {candidate.dimension_scores.get('sector_score', 0):.1f}/100
â€¢ è®¤çŸ¥è¯„åˆ†: {candidate.dimension_scores.get('cognitive_score', 0):.1f}/100
                """)
            else:
                print("âŒ è¯¥ä¼ä¸šæœªé€šè¿‡åäººèº«ä»½è¯†åˆ«æˆ–æ•°æ®ä¸è¶³")
        
        elif args.mode == 'monitoring':
            # ç›‘æ§æ¨¡å¼ï¼šæŒç»­ç›‘æ§æŠ•èµ„æœºä¼š
            logger.info("å¯åŠ¨æŒç»­ç›‘æ§æ¨¡å¼...")
            
            if args.continuous:
                while True:
                    # å®ç°æŒç»­ç›‘æ§é€»è¾‘
                    logger.info("æ‰§è¡Œç›‘æ§æ‰«æ...")
                    await asyncio.sleep(3600)  # æ¯å°æ—¶æ‰«æä¸€æ¬¡
            else:
                logger.info("æ‰§è¡Œå•æ¬¡ç›‘æ§æ‰«æ...")
                # æ‰§è¡Œå•æ¬¡æ‰«æ
                
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        raise
    finally:
        await collector.close()
        logger.info("æ•°æ®æ”¶é›†å™¨å·²å…³é—­")

if __name__ == "__main__":
    print("""
ğŸš€ PocketCornæŠ•èµ„åˆ†ææ•°æ®æ”¶é›†å™¨å¯åŠ¨ä¸­...
ğŸ“‹ åŸºäºPocketCorn v4æ–¹æ³•è®ºçš„åäººAIä¼ä¸šå‘ç°ç³»ç»Ÿ
    """)
    
    asyncio.run(main())